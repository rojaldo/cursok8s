= Curso Completo de Kubernetes
:author: DevOps Team
:date: 2025
:doctype: book
:toc: left
:toc-title: Temario
:toclevels: 3
:numbered:
:icons: font

== MÓDULO 1: Introducción a Kubernetes

=== 1.1 Conceptos Fundamentales

==== 1.1.1 ¿Qué es Kubernetes?

Kubernetes (también conocido como K8s) es un **orquestador de contenedores de código abierto** que automatiza la implementación, escalado y gestión de aplicaciones en contenedores. Fue desarrollado por Google y ahora es mantenido por la Cloud Native Computing Foundation (CNCF).

**Definición técnica:**
Kubernetes es una plataforma de orquestación que permite:
- **Automatizar el despliegue** de contenedores en múltiples máquinas
- **Escalar aplicaciones** automáticamente según la demanda
- **Gestionar recursos** de forma eficiente
- **Recuperarse de fallos** automáticamente
- **Actualizar aplicaciones** sin downtime

**¿Por qué Kubernetes?**

Cuando trabajas con contenedores (Docker), surgen preguntas:
- ¿Cómo despliego múltiples contenedores en varios servidores?
- ¿Qué pasa si un contenedor falla? ¿Se reinicia automáticamente?
- ¿Cómo balancea la carga entre contenedores?
- ¿Cómo actualizo mi aplicación sin cortar el servicio?
- ¿Cómo escalo mi aplicación cuando hay mucha demanda?

Kubernetes responde a todas estas preguntas.

==== 1.1.2 Comparación con Docker Swarm

[cols="1,1,1", options="header"]
|===
| Característica | Kubernetes | Docker Swarm
| Complejidad | Alta | Baja
| Curva de aprendizaje | Pronunciada | Suave
| Escalabilidad | Excelente (miles de nodos) | Buena (hasta 500 nodos)
| Adopción empresarial | Dominante | Baja
| Comunidad | Muy grande | Pequeña
| Herramientas de monitoreo | Amplias | Limitadas
| Networking | Avanzado | Básico
| Storage | Completo | Básico
| Uso en producción | Muy recomendado | Limitado
|===

**Veredicto:** Kubernetes es la opción estándar para empresas grandes y medianas, mientras que Docker Swarm es más simple pero menos potente.

==== 1.1.3 Ventajas de Kubernetes

*Orquestación Automática:*
- Despliegue y retirada automática de contenedores
- Escalado automático basado en métricas
- Actualización automática con cero downtime

*Alta Disponibilidad:*
- Reinicio automático de contenedores que fallan
- Replicación de aplicaciones
- Health checks

*Eficiencia de Recursos:*
- Bin packing (uso óptimo de recursos)
- Escalado automático de nodos
- Límites de recursos por aplicación

*Portabilidad:*
- Funciona en cualquier nube (AWS, Google Cloud, Azure, etc.)
- Funciona on-premises
- Evita vendor lock-in

*Ecosistema Maduro:*
- Herramientas de monitoreo (Prometheus, Grafana)
- Service mesh (Istio)
- Ingress controllers
- Package managers (Helm)

==== 1.1.4 Desventajas de Kubernetes

*Complejidad:*
- Curva de aprendizaje empinada
- Muchos conceptos para entender
- Configuración compleja

*Overhead de Recursos:*
- Requiere recursos para los componentes del control plane
- No es ideal para aplicaciones muy pequeñas

*Debugging Difícil:*
- Los problemas pueden ser complejos de diagnosticar
- Requiere herramientas especializadas

*Costo:*
- Puede ser caro en infraestructura
- Requiere expertise en DevOps

==== 1.1.5 Casos de Uso Reales

*Microservicios:*
Kubernetes brilla cuando tienes múltiples servicios que necesitan comunicarse entre sí.

*Aplicaciones Web a Escala:*
Si tu aplicación recibe tráfico variable, Kubernetes escala automáticamente.

*Machine Learning:*
Distribuir trabajos de entrenamiento en múltiples GPUs.

*Procesamiento de Datos:*
Ejecutar trabajos de batch distribuidos con Spark, Hadoop, etc.

*CI/CD Pipelines:*
Ejecutar builds y tests en contenedores gestionados por Kubernetes.

==== 1.1.6 Arquitectura General de Kubernetes

[source,
----
                    ┌─────────────────────────────────────┐
                    │     KUBERNETES CLUSTER              │
                    │                                     │
    ┌───────────────┤  CONTROL PLANE (Master)            │
    │               │  ├─ API Server                      │
    │               │  ├─ Scheduler                       │
    │               │  ├─ Controller Manager              │
    │               │  └─ etcd                            │
    │               └─────────────────────────────────────┘
    │
    │  ┌──────────────────────────────────────────────────┐
    │  │  NODO WORKER 1                                   │
    │  │  ├─ Kubelet                                      │
    │  │  ├─ kube-proxy                                   │
    │  │  └─ Contenedores                                │
    │  │     ├─ Pod 1 (App Container)                    │
    │  │     └─ Pod 2 (App Container)                    │
    │  └──────────────────────────────────────────────────┘
    │
    └──┬──────────────────────────────────────────────────┐
       │  NODO WORKER 2                                   │
       │  ├─ Kubelet                                      │
       │  ├─ kube-proxy                                   │
       │  └─ Contenedores                                │
       │     ├─ Pod 3 (App Container)                    │
       │     └─ Pod 4 (App Container)                    │
       └──────────────────────────────────────────────────┘
----
]

La arquitectura sigue un modelo **Master-Worker (Master-Slave)**:
- El **Control Plane** es el "cerebro" que toma decisiones
- Los **Worker Nodes** ejecutan las aplicaciones reales

---

=== 1.2 Componentes de la Arquitectura

==== 1.2.1 Control Plane (Nodo Maestro)

El Control Plane es el corazón de Kubernetes. Gestiona el estado del cluster y toma todas las decisiones importantes.

===== API Server

**¿Qué es?**
El API Server es el punto central de comunicación de Kubernetes. Es un servicio REST que expone la API de Kubernetes.

**Funciones principales:**
- Recibe solicitudes HTTP/REST de kubectl y otras herramientas
- Valida las solicitudes
- Persiste los cambios en etcd
- Transmite cambios a otros componentes

**Ejemplo de comunicación:**
[source,bash]
----
# Cuando ejecutas esto:
kubectl create deployment nginx --image=nginx

# Lo que sucede internamente:
1. kubectl envía una solicitud HTTP POST al API Server
2. API Server valida la solicitud
3. Crea un recurso Deployment en etcd
4. El Controller Manager detecta el cambio
5. El Scheduler asigna Pods a nodos
6. Kubelet en los nodos ejecuta los contenedores
----

===== etcd (Almacén de Datos)

**¿Qué es?**
etcd es una base de datos clave-valor distribuida que almacena **TODO el estado del cluster**.

**Características:**
- Almacenamiento confiable de datos críticos
- Consistencia fuerte
- Replicación automática
- Backup y recuperación

**¿Qué se almacena en etcd?**
- Definiciones de Pods, Deployments, Servicios
- Configuraciones (ConfigMaps, Secrets)
- Estado del cluster
- Información de nodos

**Advertencia Crítica:**
Si pierdes etcd, pierdes TODO tu cluster. Por eso se recomienda:
- Hacer backups regulares de etcd
- Usar múltiples réplicas de etcd para HA
- Encriptar etcd en reposo

===== Scheduler

**¿Qué es?**
El Scheduler es responsable de asignar Pods a nodos.

**¿Cómo funciona?**
[source,
----
1. Un Pod es creado (pero sin nodo asignado)
2. El Scheduler observa los Pods sin asignación
3. Evalúa cada nodo disponible
4. Elige el mejor nodo basado en:
   - Recursos disponibles (CPU, memoria)
   - Labels y tolerancias
   - Afinidad de Pods
   - Políticas de distribución
5. Asigna el Pod al nodo elegido
----
]

**Algoritmo simplificado:**
[source,bash]
----
# El Scheduler pregunta:
- ¿Este nodo tiene suficiente CPU y memoria?
- ¿Este nodo tiene los labels requeridos?
- ¿Este Pod tiene alguna afinidad/anti-afinidad con otros Pods?
- ¿Está este nodo tainted (marcado como no disponible)?
- ¿Cómo está la distribución actual de Pods en nodos?
----
]

===== Controller Manager

**¿Qué es?**
El Controller Manager es un componente que ejecuta múltiples "controladores" que vigilan el estado del cluster.

**¿Qué es un Controlador?**
Un controlador es un bucle infinito que:
1. Observa el estado actual
2. Compara con el estado deseado
3. Toma acciones para que coincidan

**Ejemplos de Controladores:**
- **Deployment Controller**: Si deseas 3 replicas pero solo hay 2, crea otro Pod
- **Node Controller**: Si un nodo falla, marca sus Pods como fallidos
- **Service Controller**: Mantiene sincronizados los endpoints con los Pods
- **StatefulSet Controller**: Gestiona Pods con identidad estable
- **Job Controller**: Ejecuta trabajos hasta completarse

**Analogía:**
Es como un termostato:
- Temperatura deseada (estado deseado) = 22°C
- Temperatura actual (estado actual) = 18°C
- El termostato enciende la calefacción (toma acción)
- Repite el ciclo constantemente

---

==== 1.2.2 Nodos Worker

Los Nodos Worker son máquinas (físicas o virtuales) que ejecutan tus aplicaciones.

===== Kubelet

**¿Qué es?**
Kubelet es el agente que corre en cada nodo worker. Es responsable de ejecutar los Pods.

**Funciones:**
- Recibe especificaciones de Pods del API Server
- Descarga las imágenes de contenedores
- Inicia contenedores usando el container runtime
- Monitorea la salud de los Pods
- Realiza health checks (readiness, liveness)
- Reporta el estado del nodo y sus Pods al API Server

**Flujo de vida de un Pod:**
[source,bash]
----
1. Scheduler asigna el Pod a este nodo
2. Kubelet observa que hay un Pod asignado
3. Kubelet descarga la imagen del contenedor
4. Kubelet inicia el contenedor
5. Kubelet ejecuta health checks
6. Si el contenedor falla, Kubelet lo reinicia (según política)
7. Kubelet reporta eventos al API Server
----
]

===== Container Runtime

**¿Qué es?**
El Container Runtime es el software que ejecuta los contenedores. Ejemplos:
- **Docker** (el más común históricamente)
- **containerd** (cada vez más común)
- **CRI-O** (usado en OpenShift)
- **runc** (runtime de bajo nivel)

**¿Por qué importa?**
Kubernetes puede trabajar con diferentes runtimes. El Kubelet se comunica con el runtime a través de la **CRI (Container Runtime Interface)**.

===== kube-proxy

**¿Qué es?**
kube-proxy es un componente de networking que:
- Mantiene reglas de red en cada nodo
- Implementa Servicios de Kubernetes
- Realiza balanceo de carga local

**¿Cómo funciona?**
[source,
----
Cliente → kube-proxy en Nodo A → iptables/rules de red → 
  → Pods en Nodo A/B/C (balanceado)
----
]

kube-proxy usa una de estas implementaciones:
- **iptables mode** (por defecto): Rápido pero puede ser inestable con muchos Servicios
- **ipvs mode**: Más escalable que iptables

---

==== 1.2.3 Componentes Adicionales

===== DNS (CoreDNS)

**¿Qué es?**
CoreDNS es el servidor DNS de Kubernetes. Proporciona resolución de nombres para Servicios y Pods.

**Ejemplos de nombres DNS:**
[source,bash]
----
# Servicio en el mismo namespace
nginx-service

# Servicio en otro namespace
nginx-service.production

# FQDN completo
nginx-service.production.svc.cluster.local

# Pod individual
10-244-1-5.default.pod.cluster.local
----
]

===== Sistema de Logging

**¿Qué es?**
Kubernetes genera muchos logs:
- Logs de contenedores (stdout/stderr)
- Logs del API Server
- Logs del Scheduler
- Logs del Kubelet

**¿Cómo se recopilan?**
Generalmente con herramientas como:
- **ELK Stack** (Elasticsearch, Logstash, Kibana)
- **Fluentd o Fluent Bit**
- **Loki** (de Grafana Labs)
- **Splunk, Datadog, etc.**

===== Monitoring

**¿Qué es?**
Monitoreo de:
- Métricas de recursos (CPU, memoria, disco)
- Métricas de aplicación
- Eventos del cluster

**Herramientas:**
- **Metrics Server**: Proporciona métricas básicas
- **Prometheus**: Scraping de métricas (estándar de facto)
- **Grafana**: Visualización de métricas

---

=== 1.3 Instalación y Configuración

==== 1.3.1 Instalación de kubectl

kubectl es la herramienta de línea de comandos para interactuar con Kubernetes.

**En Linux:**
[source,bash]
----
# Descargar kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

# Hacer ejecutable
chmod +x kubectl

# Mover a PATH
sudo mv kubectl /usr/local/bin/

# Verificar instalación
kubectl version --client
----
]

**Con Homebrew (macOS):**
[source,bash]
----
brew install kubectl
kubectl version --client
----
]

**Con Chocolatey (Windows):**
[source,powershell]
----
choco install kubernetes-cli
kubectl version --client
----
]

==== 1.3.2 Instalación de Docker

Docker es necesario para crear imágenes de contenedores.

**En Ubuntu/Debian:**
[source,bash]
----
# Actualizar repos
sudo apt-get update

# Instalar dependencias
sudo apt-get install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg \
    lsb-release

# Agregar clave GPG de Docker
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

# Agregar repositorio
echo \
  "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# Instalar Docker
sudo apt-get update
sudo apt-get install -y docker-ce docker-ce-cli containerd.io

# Permitir usar Docker sin sudo
sudo usermod -aG docker $USER
newgrp docker

# Verificar
docker run hello-world
----
]

==== 1.3.3 Opciones de Instalación de Kubernetes

===== Minikube (Desarrollo Local)

**¿Qué es?**
Minikube es una herramienta que crea un cluster Kubernetes pequeño en tu máquina local. Perfecto para desarrollo.

**Ventajas:**
- Fácil de instalar y usar
- Rápido para experimentar
- Incluye un cluster completo (aunque pequeño)

**Desventajas:**
- Un único nodo
- Limitado en recursos
- No es para producción

**Instalación:**
[source,bash]
----
# En macOS
brew install minikube

# En Linux
curl -LO https://github.com/kubernetes/minikube/releases/latest/download/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube

# En Windows
choco install minikube
----
]

**Uso:**
[source,bash]
----
# Iniciar el cluster
minikube start

# Ver el estado
minikube status

# Detener el cluster
minikube stop

# Eliminar el cluster
minikube delete

# Ver IP del cluster
minikube ip

# Acceder al dashboard
minikube dashboard
----
]

===== kubeadm

**¿Qué es?**
kubeadm es una herramienta para crear clusters Kubernetes de forma manual. Más control pero más complejidad.

**Instalación del master:**
[source,bash]
----
# Instalación de dependencias
sudo apt-get update && sudo apt-get install -y apt-transport-https ca-certificates curl

# Agregar repo de Kubernetes
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

# Instalar kubeadm, kubelet y kubectl
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl

# Inicializar el master
sudo kubeadm init --pod-network-cidr=10.244.0.0/16

# Copiar kubeconfig
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Instalar CNI (red)
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
----
]

===== Kubernetes en la Nube

**Amazon EKS (Elastic Kubernetes Service):**
[source,bash]
----
# Instalar AWS CLI y eksctl
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install

# Crear cluster
eksctl create cluster --name my-cluster --region us-east-1

# Conectarse
aws eks update-kubeconfig --name my-cluster --region us-east-1
----
]

**Google GKE (Google Kubernetes Engine):**
[source,bash]
----
# Instalar Google Cloud SDK
curl https://sdk.cloud.google.com | bash

# Crear cluster
gcloud container clusters create my-cluster --zone us-central1-a

# Conectarse
gcloud container clusters get-credentials my-cluster --zone us-central1-a
----
]

**Azure AKS (Azure Kubernetes Service):**
[source,bash]
----
# Instalar Azure CLI
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

# Crear grupo de recursos
az group create --name myResourceGroup --location eastus

# Crear cluster
az aks create --resource-group myResourceGroup --name myAKSCluster --node-count 2

# Conectarse
az aks get-credentials --resource-group myResourceGroup --name myAKSCluster
----
]

==== 1.3.4 Configuración de kubectl

kubectl se configura con un archivo `kubeconfig` que contiene conexiones a clusters.

**Ubicación por defecto:**
[source,bash]
----
~/.kube/config
----
]

**Estructura del archivo kubeconfig:**
[source,yaml]
----
apiVersion: v1
kind: Config
clusters:
  - cluster:
      certificate-authority: /path/to/ca.crt
      server: https://kubernetes.example.com:6443
    name: kubernetes-cluster
contexts:
  - context:
      cluster: kubernetes-cluster
      user: admin
    name: kubernetes-admin
current-context: kubernetes-admin
users:
  - name: admin
    user:
      client-certificate: /path/to/admin.crt
      client-key: /path/to/admin.key
----
]

**Comandos útiles:**
[source,bash]
----
# Ver configuración actual
kubectl config view

# Ver contexto actual
kubectl config current-context

# Listar contextos disponibles
kubectl config get-contexts

# Cambiar de contexto
kubectl config use-context kubernetes-admin

# Agregar nuevo cluster
kubectl config set-cluster my-cluster --server=https://my-cluster:6443

# Agregar nuevo usuario
kubectl config set-credentials my-user --client-certificate=path/to/cert
----
]

==== 1.3.5 Verificación de la Instalación

**Verificar que Kubernetes está funcionando:**
[source,bash]
----
# Verificar versión
kubectl version

# Ver nodos disponibles
kubectl get nodes

# Ver servicios del sistema
kubectl get services --namespace kube-system

# Ver pods del sistema
kubectl get pods --namespace kube-system

# Descripción detallada del cluster
kubectl cluster-info

# Ver logs del cluster
kubectl cluster-info dump --output-directory=/tmp/cluster-dump
----
]

**Ejemplo de salida esperada:**
[source,bash]
----
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27"...}
Server Version: version.Info{Major:"1", Minor:"27"...}

$ kubectl get nodes
NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   10m   v1.27.0

$ kubectl get pods --namespace kube-system
NAME                               READY   STATUS    RESTARTS   AGE
coredns-64897fb6d9-qr5v6          1/1     Running   0          10m
etcd-minikube                      1/1     Running   0          10m
kube-apiserver-minikube            1/1     Running   0          10m
kube-controller-manager-minikube   1/1     Running   0          10m
----
]

---

=== 1.4 Mi Primer Cluster

==== 1.4.1 Crear un Cluster Local con Minikube

**Paso 1: Iniciar Minikube**
[source,bash]
----
# Iniciar con configuración recomendada
minikube start --cpus 4 --memory 4096

# O con más recursos
minikube start --cpus 8 --memory 8192 --disk-size 50gb
----
]

**Paso 2: Esperar a que esté listo**
[source,bash]
----
# Ver el estado
minikube status

# Espera a ver:
# minikube: Running
# kubectl.io: Running
----
]

==== 1.4.2 Conectarse al Cluster

**kubectl ya está configurado automáticamente:**
[source,bash]
----
# Verificar conexión
kubectl cluster-info

# Debe mostrar:
# Kubernetes master is running at https://192.168.59.100:8443
# CoreDNS is running at https://192.168.59.100:8443/api/v1/namespaces/kube-system/services/kube-dns/proxy
----
]

==== 1.4.3 Explorar el Cluster

**Ver nodos:**
[source,bash]
----
kubectl get nodes

# Salida:
# NAME       STATUS   ROLES           AGE   VERSION
# minikube   Ready    control-plane   2m    v1.27.0
----
]

**Ver pods del sistema:**
[source,bash]
----
kubectl get pods --namespace kube-system

# Salida mostrará los componentes de Kubernetes
----
]

**Crear tu primer Pod:**
[source,bash]
----
# Crear un Deployment simple
kubectl create deployment hello-world --image=nginx:latest

# Ver el Deployment
kubectl get deployments

# Ver los Pods creados
kubectl get pods

# Ver descripción detallada
kubectl describe pod <pod-name>
----
]

**Acceder a tu aplicación:**
[source,bash]
----
# Port forward al Pod
kubectl port-forward pod/hello-world-xxxxx 8080:80

# Ahora accede en tu navegador a http://localhost:8080
----
]

**Ver logs:**
[source,bash]
----
kubectl logs <pod-name>

# Seguir logs en vivo
kubectl logs -f <pod-name>
----
]

==== 1.4.4 Eliminar el Cluster

**Detener Minikube:**
[source,bash]
----
# Solo detener (los datos se mantienen)
minikube stop

# Completamente eliminar
minikube delete

# Borrar todo incluyendo configuraciones
minikube delete --all
----
]

---

=== 1.5 Ejercicios Prácticos

==== Ejercicio 1: Desplegar tu Primera Aplicación

**Objetivo:** Crear y ejecutar un Deployment simple

**YAML - deployment-nginx.yaml:**
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: default
  labels:
    app: nginx
    version: v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
          name: http
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"
----
]

**Comandos para ejecutar:**
[source,bash]
----
# Aplicar el YAML
kubectl apply -f deployment-nginx.yaml

# Ver el Deployment
kubectl get deployments

# Ver los Pods
kubectl get pods

# Ver detalles
kubectl describe deployment nginx-deployment

# Ver eventos
kubectl get events

# Eliminar el Deployment
kubectl delete deployment nginx-deployment
----
]

==== Ejercicio 2: Exponer tu Aplicación

**Objetivo:** Crear un Servicio para acceder a la aplicación

**YAML - service-nginx.yaml:**
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  namespace: default
  labels:
    app: nginx
spec:
  type: ClusterIP
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    name: http
----
]

**Comandos para ejecutar:**
[source,bash]
----
# Aplicar el Servicio
kubectl apply -f service-nginx.yaml

# Ver servicios
kubectl get services

# Obtener IP del Servicio
kubectl get svc nginx-service

# Describir el Servicio
kubectl describe service nginx-service

# Acceder al servicio (desde dentro del cluster)
kubectl run -it --rm debug --image=busybox --restart=Never -- wget -O- http://nginx-service
----
]

==== Ejercicio 3: Escalar tu Aplicación

**Objetivo:** Cambiar el número de replicas

**Comando directo:**
[source,bash]
----
# Escalar a 5 replicas
kubectl scale deployment nginx-deployment --replicas=5

# Ver los nuevos Pods
kubectl get pods

# Escalar de nuevo a 2
kubectl scale deployment nginx-deployment --replicas=2
----
]

**Usando YAML:**
Modificar el archivo `deployment-nginx.yaml` y cambiar `replicas: 3` a `replicas: 5`, luego:
[source,bash]
----
kubectl apply -f deployment-nginx.yaml

# Ver cambios
kubectl get pods
----
]

==== Ejercicio 4: Actualizar tu Aplicación

**Objetivo:** Cambiar la versión de nginx

**YAML actualizado - deployment-nginx-v2.yaml:**
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: default
  labels:
    app: nginx
    version: v2
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.22  # Versión actualizada
        ports:
        - containerPort: 80
          name: http
        resources:
          requests:
            cpu: "100m"
            memory: "128Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"
----
]

**Comandos:**
[source,bash]
----
# Aplicar la actualización
kubectl apply -f deployment-nginx-v2.yaml

# Ver el progreso del update
kubectl rollout status deployment/nginx-deployment

# Ver el historial de rollouts
kubectl rollout history deployment/nginx-deployment

# Si algo sale mal, revertir a la versión anterior
kubectl rollout undo deployment/nginx-deployment

# Ver nuevamente
kubectl rollout status deployment/nginx-deployment
----
]

==== Ejercicio 5: Monitoreo Básico

**Objetivo:** Observar recursos y eventos

**Comandos:**
[source,bash]
----
# Ver uso de recursos en nodos
kubectl top nodes

# Ver uso de recursos en Pods
kubectl top pods

# Monitorear cambios en tiempo real
kubectl get pods --watch

# Ver eventos del cluster
kubectl get events --sort-by='.lastTimestamp'

# Ver eventos de un pod específico
kubectl describe pod <pod-name>

# Ver logs de un contenedor
kubectl logs <pod-name>

# Seguir logs en vivo
kubectl logs -f <pod-name>

# Ver logs de múltiples Pods
kubectl logs -l app=nginx

# Ver logs del pod anterior (si fue reiniciado)
kubectl logs <pod-name> --previous
----
]

---

=== 1.6 Solución de Problemas Iniciales

==== Problema: "The connection to the server was refused"

**Causa:** Kubernetes no está corriendo o no hay conexión

**Solución:**
[source,bash]
----
# Verificar si Minikube está corriendo
minikube status

# Si no está corriendo, iniciar
minikube start

# Verificar kubeconfig
kubectl config view

# Intentar conectarse
kubectl cluster-info
----
]

==== Problema: Pods en estado Pending

**Causa:** El Pod no puede ser programado (recursos insuficientes, nodo no disponible)

**Solución:**
[source,bash]
----
# Describir el Pod
kubectl describe pod <pod-name>

# Ver eventos del cluster
kubectl get events

# Verificar recursos disponibles
kubectl top nodes

# Si es Minikube, aumentar recursos
minikube stop
minikube start --cpus 8 --memory 8192
----
]

==== Problema: Pod en estado CrashLoopBackOff

**Causa:** El contenedor está fallando al iniciar

**Solución:**
[source,bash]
----
# Ver logs del Pod
kubectl logs <pod-name>

# Ver logs del pod anterior
kubectl logs <pod-name> --previous

# Describir el Pod
kubectl describe pod <pod-name>

# Revisar que la imagen exista
kubectl describe pod <pod-name> | grep Image

# Si es error de imagen, ver los eventos
kubectl get events --sort-by='.lastTimestamp'
----
]

==== Problema: No puedo acceder a mi aplicación

**Causa:** El Servicio no está configurado correctamente o el Port Forward falla

**Solución:**
[source,bash]
----
# Verificar que el Pod está corriendo
kubectl get pods

# Verificar que el Servicio existe
kubectl get services

# Describir el Servicio
kubectl describe service <service-name>

# Ver endpoints del Servicio
kubectl get endpoints

# Intentar conectarse desde dentro del cluster
kubectl run -it --rm debug --image=busybox --restart=Never -- /bin/sh

# Dentro del Pod de debug:
wget -O- http://service-name:port
----
]

---

=== 1.7 Resumen del Módulo 1

En este módulo aprendiste:

1. **Conceptos Fundamentales**: Qué es Kubernetes, sus ventajas, desventajas y casos de uso
2. **Arquitectura**: Control Plane y Worker Nodes con todos sus componentes
3. **Instalación**: Cómo instalar Kubernetes localmente con Minikube
4. **Configuración**: Cómo configurar kubectl para interactuar con el cluster
5. **Primeros pasos**: Crear Deployments, Servicios, escalar y actualizar aplicaciones
6. **Troubleshooting**: Resolver problemas comunes

Con estos conocimientos, estás listo para profundizar en Pods y Deployments en el Módulo 2.

---

== MÓDULO 2: Pods y Contenedores

=== 2.1 Pods Basics

==== 2.1.1 Concepto de Pod

**¿Qué es un Pod?**

Un Pod es la **unidad mínima desplegable en Kubernetes**. Es un contenedor (o conjunto de contenedores estrechamente acoplados) que comparte recursos de red y almacenamiento.

**Analogía:**
Si Docker es como un contenedor de envío (contiene tu aplicación), Kubernetes no es el contenedor sino la grúa que lo maneja. Y un Pod es como un pequeño paquete dentro del contenedor que puede contener una o varias cajas (contenedores).

**Características importantes:**
- Un Pod puede contener 1 o más contenedores
- Los contenedores en un Pod comparten la interfaz de red (misma IP)
- Los contenedores en un Pod pueden compartir almacenamiento (volúmenes)
- Los Pods son **efímeros** (temporales, pueden ser eliminados en cualquier momento)
- Los Pods son típicamente creados por Controladores (Deployments, StatefulSets, etc.)

**Ejemplo visual:**
[source,
----
┌─────────────────────────────────────┐
│         POD                         │
│                                     │
│  ┌──────────────┐  ┌──────────────┐│
│  │  Container 1 │  │  Container 2 ││
│  │  (nginx)     │  │  (logging)   ││
│  │  Puerto 80   │  │  Sidecar     ││
│  └──────────────┘  └──────────────┘│
│                                     │
│  Compartidos:                       │
│  - IP: 10.244.1.5                   │
│  - Filesystem (volúmenes)           │
│  - IPC namespace                    │
│  - Network namespace                │
└─────────────────────────────────────┘
----
]

==== 2.1.2 Pod vs Contenedor

[cols="1,1,1", options="header"]
|===
| Aspecto | Contenedor (Docker) | Pod (Kubernetes)
| Unidad | Empaquetado de aplicación | Unidad de despliegue en K8s
| Cantidad | 1 contenedor por contenedor | 1+ contenedores por Pod
| Red | Red propia isolada | Red compartida dentro del Pod
| Ciclo de vida | Independiente | Gestionado por K8s
| Reemplazo | Reemplazable | Efímero
| Escalado | Manual | Automático (via Deployments)
|===

==== 2.1.3 Ciclo de Vida del Pod

Un Pod pasa por varios estados desde su creación hasta su eliminación:

[source,
----
Pending → Running → Succeeded/Failed → Terminating → Terminated
----
]

**Estado: Pending**
- El Pod ha sido aceptado por Kubernetes
- Pero los contenedores aún no han sido iniciados
- Puede estar esperando recursos, descargas de imágenes, etc.

**Estado: Running**
- Al menos un contenedor está en ejecución
- El Pod está completamente operacional
- Puede manejar tráfico

**Estado: Succeeded**
- Todos los contenedores han completado exitosamente
- No se reinician
- Típico en Jobs

**Estado: Failed**
- Al menos un contenedor falló
- El Pod no se reinicia automáticamente
- El controlador (Deployment) puede crear un nuevo Pod

**Estado: Terminating**
- El Pod está siendo eliminado
- Grace period: espera a que se limpien los recursos (por defecto 30 segundos)
- Después se termina forzosamente

**Diagrama de estados:**
[source,yaml]
----
Pod Lifecycle:

1. CREATE
   ├─ Pod objeto creado
   ├─ Guardado en etcd
   └─ Scheduler lo observa

2. SCHEDULING
   ├─ Scheduler selecciona un nodo
   ├─ Pod asignado al nodo
   └─ Kubelet en el nodo lo observa

3. CONTAINER SETUP
   ├─ Pull imagen del contenedor
   ├─ Crear volúmenes
   ├─ Montar volúmenes
   └─ Iniciar contenedor

4. RUNNING
   ├─ Health checks activos
   ├─ Tráfico acepto
   └─ Esperando órdenes

5. TERMINATION
   ├─ Grace period (30s por defecto)
   ├─ SIGTERM a contenedores
   ├─ Kill si no terminan
   └─ Limpieza
----
]

==== 2.1.4 Tipos de Contenedores en un Pod

===== Contenedor Principal (Application Container)

Es el contenedor que ejecuta tu aplicación. **Debe estar presente**.

**Características:**
- Ejecuta el proceso principal
- Si falla, el Pod falla (generalmente)
- Es lo que típicamente desplegarás

**Ejemplo:**
```yaml
containers:
- name: web-app
  image: my-app:1.0
  ports:
  - containerPort: 8080
```

===== Sidecar Containers

Contenedores adicionales que **apoyan al contenedor principal** pero no son la aplicación en sí.

**Casos de uso comunes:**
- **Logging**: Recolectar logs del contenedor principal
- **Monitoring**: Ejecutar agentes de monitoreo
- **Networking**: Proxies, VPNs
- **Seguridad**: Firewalls, encriptación

**Ejemplo de Sidecar de Logging:**
```yaml
containers:
- name: web-app
  image: nginx:latest
  # Contenedor principal

- name: log-shipper
  image: fluent-bit:latest
  # Sidecar que envía logs a un servicio central
```

**Ventajas de Sidecars:**
- Separación de responsabilidades
- Reutilizable en múltiples Pods
- No necesita cambiar la imagen principal

===== Init Containers

Contenedores que **se ejecutan antes** que los contenedores principales.

**Características:**
- Se ejecutan secuencialmente (uno por uno)
- Deben completar exitosamente (exit code 0)
- Se usan para setup inicial
- No continúan ejecutándose después del Pod listo

**Casos de uso:**
- Descargar archivos de configuración
- Esperar a que otros servicios estén listos
- Realizar migraciones de base de datos
- Inicializar datos

**Ejemplo:**
```yaml
initContainers:
- name: wait-for-db
  image: busybox:latest
  command: ['sh', '-c', 'until nslookup postgres; do echo waiting for postgres; sleep 2; done']

- name: db-migration
  image: my-app:1.0
  command: ['./migrate.sh']

containers:
- name: web-app
  image: my-app:1.0
  # Solo se ejecuta después de que init containers completen
```

---

=== 2.2 Creación de Pods

==== 2.2.1 YAML para Pods

Un Pod se define en YAML con una estructura específica. Veamos la estructura completa:

**Estructura Mínima:**
[source,yaml]
----
apiVersion: v1                    # Versión de API de Kubernetes
kind: Pod                         # Tipo de objeto (Pod)
metadata:
  name: my-pod                    # Nombre único del Pod
spec:
  containers:
  - name: app                     # Nombre del contenedor
    image: nginx:latest           # Imagen del contenedor
----
]

**Estructura Completa con Todas las Opciones:**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: complete-pod
  namespace: default              # Namespace donde reside el Pod
  labels:                          # Labels para seleccionar el Pod
    app: myapp
    environment: production
    version: v1
  annotations:                     # Anotaciones (metadatos adicionales)
    description: "Production Pod"
    owner: "team-backend"
spec:
  # Política de reinicio
  restartPolicy: Always           # Always, Never, OnFailure

  # Duración máxima del Pod (segundos)
  activeDeadlineSeconds: 3600

  # Tolerancia de riesgos (taints)
  tolerations:
  - key: node-type
    operator: Equal
    value: gpu
    effect: NoSchedule

  # Afinidad de nodos
  nodeSelector:
    kubernetes.io/hostname: worker-1

  # Cuenta de servicio
  serviceAccountName: my-account

  # Seguridad del Pod
  securityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000

  # DNS
  dnsPolicy: ClusterFirst
  dnsConfig:
    nameservers:
    - 8.8.8.8
    searches:
    - my.dns.search.suffix

  # Volúmenes
  volumes:
  - name: config-vol
    configMap:
      name: my-config
  - name: secret-vol
    secret:
      secretName: my-secret
  - name: storage-vol
    emptyDir: {}

  # Init containers (se ejecutan primero)
  initContainers:
  - name: init
    image: busybox:latest
    command: ['echo', 'Initializing...']

  # Contenedores principales
  containers:
  - name: app
    image: myapp:1.0
    imagePullPolicy: IfNotPresent  # Always, Never, IfNotPresent
    
    # Puertos
    ports:
    - name: http
      containerPort: 8080
      protocol: TCP
    
    # Variables de entorno
    env:
    - name: APP_ENV
      value: production
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: password
    
    # Mount de volúmenes
    volumeMounts:
    - name: config-vol
      mountPath: /etc/config
    - name: storage-vol
      mountPath: /data
    
    # Recursos
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "500m"
        memory: "512Mi"
    
    # Health checks
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 10
      periodSeconds: 5
    
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 3
    
    startupProbe:
      httpGet:
        path: /started
        port: 8080
      failureThreshold: 30
      periodSeconds: 10

  # Sidecars (contenedores adicionales)
  - name: logging-sidecar
    image: fluent-bit:latest
    volumeMounts:
    - name: config-vol
      mountPath: /etc/config

  # Terminación grácil
  terminationGracePeriodSeconds: 30
----
]

==== 2.2.2 Crear Pods con kubectl apply

**Método Imperativo (rápido para testing):**
[source,bash]
----
# Crear un Pod simple directamente
kubectl run my-pod --image=nginx:latest

# Con especificaciones adicionales
kubectl run my-pod \
  --image=nginx:latest \
  --port=8080 \
  --env=APP_ENV=production

# Con límites de recursos
kubectl run my-pod \
  --image=nginx:latest \
  --limits=cpu=500m,memory=512Mi \
  --requests=cpu=100m,memory=128Mi
----
]

**Método Declarativo (recomendado para producción):**
[source,bash]
----
# Crear desde archivo YAML
kubectl apply -f pod.yaml

# Crear desde múltiples archivos
kubectl apply -f pod1.yaml -f pod2.yaml

# Crear desde directorio
kubectl apply -f ./pods/

# Crear desde URL
kubectl apply -f https://example.com/pod.yaml

# Ver qué cambiaría (dry-run)
kubectl apply -f pod.yaml --dry-run=client
kubectl apply -f pod.yaml --dry-run=server
----
]

==== 2.2.3 Campos Obligatorios y Opcionales

**Campos Obligatorios:**
[source,yaml]
----
apiVersion: v1              # Requerido
kind: Pod                   # Requerido
metadata:
  name: my-pod              # Requerido (único en el namespace)
spec:
  containers:               # Requerido (al menos 1)
  - name: app               # Requerido
    image: nginx:latest     # Requerido
----
]

**Campos Opcionales pero Recomendados:**
[source,yaml]
----
metadata:
  namespace: production      # Por defecto: default
  labels:
    app: myapp
  annotations:
    owner: team

spec:
  containers:
  - resources:
      requests:             # Recomendado para scheduling
        cpu: 100m
        memory: 128Mi
      limits:                # Recomendado para evitar OOM
        cpu: 500m
        memory: 512Mi
    
    livenessProbe:           # Recomendado para health checks
      httpGet:
        path: /health
        port: 8080
    
    readinessProbe:          # Recomendado para zero-downtime deploys
      httpGet:
        path: /ready
        port: 8080
----
]

==== 2.2.4 Etiquetas y Anotaciones

===== Etiquetas (Labels)

Las **labels** son pares clave-valor que identifican objetos. Se usan para **seleccionar y agrupar** objetos.

**Reglas:**
- Clave: debe ser válida (letras, números, guiones, puntos, máximo 63 caracteres)
- Valor: alfanumérico, guiones, puntos, máximo 63 caracteres
- Máximo 64 labels por objeto

**Ejemplo:**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: web-pod
  labels:
    app: web                        # Label simple
    environment: production
    team: backend
    version: v1
    component: api
    tier: frontend
spec:
  containers:
  - name: web
    image: nginx:latest
----
]

**Uso de Labels:**
[source,bash]
----
# Seleccionar Pods por label
kubectl get pods -l app=web

# Múltiples labels
kubectl get pods -l app=web,environment=production

# Operadores: in, notin, exists
kubectl get pods -l environment in (production,staging)
kubectl get pods -l tier notin (frontend)
kubectl get pods -l version

# Describir con labels
kubectl describe pod web-pod
kubectl get pods --show-labels
----
]

===== Anotaciones (Annotations)

Las **annotations** son similares a labels pero se usan para información que **no es para selección**.

**Usos:**
- Información de build
- Información de contacto
- URLs de documentación
- Información de debugging

**Reglas similares a labels** pero pueden ser más largas.

**Ejemplo:**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: web-pod
  annotations:
    description: "Main web server for API"
    documentation: "https://wiki.company.com/web-pod"
    contact: "team-backend@company.com"
    build-date: "2025-01-10"
    git-commit: "abc123def456"
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
    prometheus.io/path: "/metrics"
spec:
  containers:
  - name: web
    image: nginx:latest
----
]

**Ventajas de Anotaciones:**
- No tienen restricciones de formato
- Pueden contener datos complejos
- Herramientas pueden leerlas e interpretarlas

---

=== 2.3 Gestión de Pods

==== 2.3.1 Listar Pods

**Comandos básicos:**
[source,bash]
----
# Listar Pods en namespace actual (default)
kubectl get pods

# Listar con más información
kubectl get pods -o wide

# Listar en todos los namespaces
kubectl get pods -A
kubectl get pods --all-namespaces

# Listar en namespace específico
kubectl get pods -n kube-system

# Listar con labels
kubectl get pods --show-labels

# Listar filtrando por label
kubectl get pods -l app=nginx

# Formato de salida personalizado
kubectl get pods -o yaml        # YAML format
kubectl get pods -o json        # JSON format
kubectl get pods -o json | jq   # JSON con jq para filtrar
----
]

**Salida de ejemplo:**
[source,bash]
----
$ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
nginx-deployment-5d...  1/1     Running   0          10m
nginx-deployment-7c...  1/1     Running   0          10m
redis-cache             1/1     Running   2          5d
postgres-db-0           1/1     Running   0          3d

READY: contenedores listos / total de contenedores
STATUS: estado actual del Pod
RESTARTS: número de veces que se reinició
AGE: tiempo desde que se creó
----
]

==== 2.3.2 Ver Detalles de un Pod

**Comando describe (muy útil):**
[source,bash]
----
# Ver todos los detalles de un Pod
kubectl describe pod nginx-deployment-5d...

# Salida incluye:
# - Metadata (nombre, namespace, labels, etc.)
# - Spec (configuración)
# - Status (estado actual)
# - Eventos (qué pasó)
# - Mounts (volúmenes)
# - Resource limits
# - Health checks
----
]

**Ver en YAML:**
[source,bash]
----
# Ver definición YAML actual
kubectl get pod nginx-deployment-5d... -o yaml

# Salida es YAML completo que puedes editar o guardar
----
]

==== 2.3.3 Logs de Pods

**Ver logs:**
[source,bash]
----
# Ver logs del último startup
kubectl logs pod-name

# Seguir logs en vivo (like tail -f)
kubectl logs -f pod-name

# Últimas N líneas
kubectl logs pod-name --tail=50

# Últimos N segundos
kubectl logs pod-name --since=10m

# Con timestamps
kubectl logs pod-name --timestamps=true

# De contenedor específico en Pod multi-contenedor
kubectl logs pod-name -c container-name

# Pod anterior (si fue reiniciado)
kubectl logs pod-name --previous
----
]

**Logs de múltiples Pods:**
[source,bash]
----
# De múltiples Pods con label
kubectl logs -l app=nginx

# De múltiples Pods (todos con prefijo)
kubectl logs -f -l app=nginx --all-containers=true
----
]

==== 2.3.4 Ejecutar Comandos en Pods

**Ejecutar comando una vez:**
[source,bash]
----
# Ejecutar comando en Pod
kubectl exec pod-name -- ls -la

# Ejecutar en contenedor específico
kubectl exec pod-name -c container-name -- pwd

# Ejecutar shell command
kubectl exec pod-name -- sh -c "echo Hello from Pod"

# Pasar variables de entorno
kubectl exec pod-name -- env
----
]

**Sesión interactiva:**
[source,bash]
----
# Abrir bash/sh interactivo
kubectl exec -it pod-name -- /bin/bash
kubectl exec -it pod-name -- /bin/sh

# Una vez dentro, puedes ejecutar comandos normales
$ ls -la
$ cat /etc/config/app.conf
$ ps aux
----
]

**Copiar archivos:**
[source,bash]
----
# Copiar archivo desde Pod a local
kubectl cp pod-name:/path/in/pod /path/local

# Copiar archivo desde local a Pod
kubectl cp /path/local pod-name:/path/in/pod

# Especificar contenedor
kubectl cp pod-name:/path -c container-name /path/local
----
]

==== 2.3.5 Eliminar Pods

**Eliminar Pods:**
[source,bash]
----
# Eliminar un Pod
kubectl delete pod pod-name

# Eliminar múltiples Pods
kubectl delete pod pod1 pod2 pod3

# Eliminar por label
kubectl delete pods -l app=nginx

# Eliminar todos los Pods en namespace
kubectl delete pods --all

# Eliminar con período de gracia (segundos)
kubectl delete pod pod-name --grace-period=30

# Forzar eliminación inmediata (puede causar problemas)
kubectl delete pod pod-name --force

# De un archivo YAML
kubectl delete -f pod.yaml
----
]

==== 2.3.6 Port Forwarding

**Conectar a un Pod directamente:**
[source,bash]
----
# Port forward simple
kubectl port-forward pod/pod-name 8080:8080

# Forward a puerto diferente en local
kubectl port-forward pod/pod-name 9090:8080
# Accede en http://localhost:9090

# Forward en background
kubectl port-forward pod/pod-name 8080:8080 &

# Usar dirección específica
kubectl port-forward pod/pod-name 127.0.0.1:8080:8080

# Multiple ports
kubectl port-forward pod/pod-name 8080:8080 8443:8443

# De un Servicio
kubectl port-forward svc/service-name 8080:8080
----
]

---

=== 2.4 Configuración de Pods

==== 2.4.1 Variables de Entorno

**Método 1: Valor literal**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: env-demo
spec:
  containers:
  - name: app
    image: myapp:latest
    env:
    - name: APP_ENV
      value: production
    - name: LOG_LEVEL
      value: "INFO"
    - name: DATABASE_URL
      value: postgres://db.example.com:5432/mydb
----
]

**Método 2: Desde ConfigMap**
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENV: production
  LOG_LEVEL: "INFO"

---
apiVersion: v1
kind: Pod
metadata:
  name: env-demo
spec:
  containers:
  - name: app
    image: myapp:latest
    envFrom:
    - configMapRef:
        name: app-config
    # Todas las keys del ConfigMap se importan como variables
----
]

**Método 3: Desde Secrets**
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:
  password: cGFzc3dvcmQxMjM=  # base64 encoded

---
apiVersion: v1
kind: Pod
metadata:
  name: env-demo
spec:
  containers:
  - name: app
    image: myapp:latest
    env:
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: password
----
]

**Método 4: Field references (desde metadata del Pod)**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: env-demo
spec:
  containers:
  - name: app
    image: myapp:latest
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    - name: POD_UID
      valueFrom:
        fieldRef:
          fieldPath: metadata.uid
    - name: CONTAINER_CPU_LIMIT
      valueFrom:
        resourceFieldRef:
          containerName: app
          resource: limits.cpu
    - name: CONTAINER_MEM_REQUEST
      valueFrom:
        resourceFieldRef:
          containerName: app
          resource: requests.memory
----
]

==== 2.4.2 Volúmenes

**Volumen emptyDir (almacenamiento temporal):**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: vol-demo
spec:
  containers:
  - name: writer
    image: busybox
    command: ['sh', '-c', 'for i in 1 2 3; do echo $i > /data/file$i; sleep 1; done']
    volumeMounts:
    - name: data
      mountPath: /data
  
  - name: reader
    image: busybox
    command: ['sh', '-c', 'watch "ls -la /data"']
    volumeMounts:
    - name: data
      mountPath: /data
  
  volumes:
  - name: data
    emptyDir: {}  # Se elimina cuando el Pod se elimina
----
]

**Volumen hostPath (acceso al nodo):**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-demo
spec:
  containers:
  - name: app
    image: busybox
    volumeMounts:
    - name: host-vol
      mountPath: /data
  
  volumes:
  - name: host-vol
    hostPath:
      path: /var/data          # Path en el nodo
      type: DirectoryOrCreate  # Crear si no existe
----
]

==== 2.4.3 Probes (Health Checks)

===== Liveness Probe

Determina si el Pod está vivo. Si falla, el Pod se reinicia.

**HTTP GET:**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: probe-demo
spec:
  containers:
  - name: app
    image: myapp:latest
    ports:
    - containerPort: 8080
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 10   # Esperar 10s antes de empezar
      periodSeconds: 5          # Check cada 5s
      timeoutSeconds: 2         # Timeout de 2s
      failureThreshold: 3       # Reiniciar después de 3 fallos
----
]

**TCP Socket:**
[source,yaml]
----
livenessProbe:
  tcpSocket:
    port: 3306
  initialDelaySeconds: 15
  periodSeconds: 10
----
]

**Exec (ejecutar comando):**
[source,yaml]
----
livenessProbe:
  exec:
    command:
    - /bin/sh
    - -c
    - "curl -f http://localhost:8080/health || exit 1"
  initialDelaySeconds: 10
  periodSeconds: 10
----
]

===== Readiness Probe

Determina si el Pod está listo para recibir tráfico. Si falla, se quita del Service.

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: probe-demo
spec:
  containers:
  - name: app
    image: myapp:latest
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 3
      failureThreshold: 2
----
]

===== Startup Probe

Para aplicaciones que toman mucho tiempo en iniciar. Previene que Liveness Probe las mate antes de iniciar.

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: slow-app
spec:
  containers:
  - name: app
    image: slow-startup-app:latest
    startupProbe:
      httpGet:
        path: /started
        port: 8080
      failureThreshold: 30      # 30 intentos
      periodSeconds: 10         # Cada 10s = 300s = 5 minutos máximo
    
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      periodSeconds: 10
----
]

**Ejemplo Completo con los 3 Probes:**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: healthy-app
spec:
  containers:
  - name: app
    image: myapp:latest
    ports:
    - containerPort: 8080
    
    # Esperar a que la app inicie
    startupProbe:
      httpGet:
        path: /started
        port: 8080
      failureThreshold: 30
      periodSeconds: 10
    
    # Asegurar que la app sigue viva
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 10
      periodSeconds: 5
      failureThreshold: 3
    
    # Determinar si puede recibir tráfico
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 3
      failureThreshold: 2
----
]

==== 2.4.4 Resource Limits y Requests

**Requests: Lo que el Pod necesita**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: resource-demo
spec:
  containers:
  - name: app
    image: myapp:latest
    resources:
      requests:
        cpu: "100m"        # 100 milli-cores (0.1 CPU)
        memory: "128Mi"    # 128 Mebibytes
        ephemeralStorage: "1Gi"
----
]

El Scheduler usa `requests` para decidir en qué nodo ejecutar el Pod. El Pod solo será programado en nodos con suficientes recursos libres.

**Limits: Máximo que el Pod puede usar**
[source,yaml]
----
resources:
  limits:
    cpu: "500m"        # Máximo 500 milli-cores
    memory: "512Mi"    # Máximo 512 Mebibytes
    ephemeralStorage: "2Gi"
----
]

El kubelet enforces `limits`. Si el Pod excede CPU, es throttled. Si excede memoria, es OOMKilled.

**Unidades:**
- **CPU**: `m` (milli-cores), `100m` = 0.1 CPU
- **Memoria**: `Mi` (Mebibytes), `Gi` (Gibibytes)
- **Storage**: `Gi` (Gibibytes)

**Ejemplo Recomendado:**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: well-configured
spec:
  containers:
  - name: app
    image: myapp:latest
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "500m"
        memory: "512Mi"
    
    # El ratio típico es 1:5 (requests:limits)
    # Pero depende de tu aplicación
----
]

==== 2.4.5 Security Context

**Pod Security Context (aplica a todos los contenedores):**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: secure-pod
spec:
  securityContext:
    runAsUser: 1000          # UID del usuario que ejecuta procesos
    runAsGroup: 3000         # GID
    fsGroup: 2000            # GID para permisos de archivo
    runAsNonRoot: true       # No permitir root
  containers:
  - name: app
    image: myapp:latest
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
        add:
        - NET_BIND_SERVICE
----
]

**Container Security Context (por contenedor):**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: container-security-demo
spec:
  containers:
  - name: app
    image: myapp:latest
    securityContext:
      runAsUser: 2000
      runAsNonRoot: true
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
      seLinuxOptions:
        level: "s0:c123,c456"
    volumeMounts:
    - name: tmp
      mountPath: /tmp
    - name: var-log
      mountPath: /var/log
  volumes:
  - name: tmp
    emptyDir: {}
  - name: var-log
    emptyDir: {}
----
]

---

=== 2.5 Troubleshooting de Pods

==== 2.5.1 Comando Describe

El comando `kubectl describe pod` es tu mejor amigo para debugging:

[source,bash]
----
kubectl describe pod pod-name

# Salida incluye:
# - Name, Namespace, Priority
# - Node donde está ejecutándose
# - Start Time
# - Labels y Annotations
# - Image, Image ID
# - Ports
# - Resource limits/requests
# - State (Running/Waiting/Terminated)
# - Reason y Message
# - Last State
# - Ready y RestartCount
# - Probes
# - Environment
# - Mounts
# - Eventos (MUY IMPORTANTE)
----
]

**Los eventos son lo más importante. Muestran el historial:**
[source,bash]
----
Events:
  Type    Reason     Age   From               Message
  ----    ------     ---   ----               -------
  Normal  Scheduled  2m    default-scheduler  Successfully assigned default/pod to node1
  Normal  Pulling    2m    kubelet            Pulling image "myapp:latest"
  Normal  Pulled     1m    kubelet            Successfully pulled image
  Normal  Created    1m    kubelet            Created container
  Normal  Started    1m    kubelet            Started container
----
]

==== 2.5.2 Ver Eventos

**Eventos del cluster:**
[source,bash]
----
# Todos los eventos (últimos)
kubectl get events

# Ordenar por tiempo
kubectl get events --sort-by='.lastTimestamp'

# Eventos de un Pod
kubectl describe pod pod-name | grep -A 100 "Events:"

# Eventos en tiempo real (watch)
kubectl get events -w

# Eventos en namespace específico
kubectl get events -n kube-system

# Eventos por tipo
kubectl get events --field-selector type=Warning
kubectl get events --field-selector type=Normal
----
]

==== 2.5.3 Analizar Logs

**Estrategia de debugging con logs:**
[source,bash]
----
# 1. Ver logs actuales
kubectl logs pod-name

# 2. Seguir logs en vivo mientras genera carga
kubectl logs -f pod-name

# 3. Ver logs del startup anterior
kubectl logs pod-name --previous

# 4. Ver logs de solo los últimos 5 minutos
kubectl logs pod-name --since=5m

# 5. Combinar con grep para buscar
kubectl logs pod-name | grep ERROR

# 6. Ver todas las líneas que coinciden
kubectl logs pod-name | grep -C 5 "ERROR"

# 7. Si hay múltiples contenedores
kubectl logs pod-name -c container-name

# 8. Logs de todos los Pods con label
kubectl logs -l app=nginx -f
----
]

**Ejemplo: Debugging de Pod que no inicia:**
[source,bash]
----
# 1. Ver estado
kubectl get pods
# Resultado: CrashLoopBackOff

# 2. Describir para ver el error
kubectl describe pod problem-pod
# En Eventos verás el problema

# 3. Ver logs del último startup
kubectl logs problem-pod

# 4. Ver logs del anterior
kubectl logs problem-pod --previous

# 5. Si la imagen está mal:
kubectl describe pod problem-pod | grep Image
# Verás ImagePullBackOff si hay problema con la imagen
----
]

==== 2.5.4 Problemas Comunes y Soluciones

===== ImagePullBackOff

**Problema:** No puede descargar la imagen

**Síntomas:**
```
Status: ImagePullBackOff
```

**Causas posibles:**
- Imagen no existe
- Registry privada sin credenciales
- Red no funciona
- Typo en nombre de imagen

**Solución:**
[source,bash]
----
# Ver el error específico
kubectl describe pod problem-pod

# Si es privada, crear secret
kubectl create secret docker-registry regcred \
  --docker-server=docker.io \
  --docker-username=myuser \
  --docker-password=mypass

# Usar en Pod
spec:
  imagePullSecrets:
  - name: regcred

# Verificar que la imagen existe
docker search myimage
docker pull docker.io/myuser/myimage:tag
----
]

===== CrashLoopBackOff

**Problema:** El contenedor inicia pero luego falla

**Síntomas:**
```
Status: CrashLoopBackOff
Restarts: 5 (in last 2m)
```

**Solución:**
[source,bash]
----
# Ver logs del último startup
kubectl logs pod-name

# Ver logs anteriores
kubectl logs pod-name --previous

# Describir el pod
kubectl describe pod pod-name

# Ejecutar bash en el mismo contexto para debuggear
kubectl run -it --rm debug \
  --image=myapp:latest \
  --restart=Never \
  -- /bin/bash
----
]

===== Pending

**Problema:** Pod no se inicia

**Síntomas:**
```
Status: Pending
```

**Causas posibles:**
- Recursos insuficientes
- Nodo no disponible
- PVC no vinculado

**Solución:**
[source,bash]
----
# Ver descripción (muestra por qué está pending)
kubectl describe pod pod-name

# Ver recursos disponibles
kubectl top nodes
kubectl describe nodes

# Si es por storage
kubectl get pvc
kubectl describe pvc pvc-name

# Aumentar recursos del cluster
minikube stop
minikube start --cpus 8 --memory 8192
----
]

===== Pod Stuck in Terminating

**Problema:** Pod no se elimina

**Síntomas:**
```
Status: Terminating
Age: 10m
```

**Solución:**
[source,bash]
----
# Ver qué tiene el Pod
kubectl describe pod pod-name

# Forzar eliminación (último recurso)
kubectl delete pod pod-name --grace-period=0 --force

# O con alternativa:
kubectl patch pod pod-name -p '{"metadata":{"finalizers":null}}'
----
]

===== OOMKilled

**Problema:** Pod se queda sin memoria

**Síntomas:**
```
Last State: Terminated
Reason: OOMKilled
```

**Solución:**
[source,bash]
----
# Ver memoria actual
kubectl top pods

# Aumentar límites
kubectl set resources pod pod-name --limits=memory=1Gi

# O editar YAML
kubectl edit pod pod-name
# Editar resources.limits.memory

# Depende del Pod en namespace
kubectl top pods -n kube-system
----
]

==== 2.5.5 Herramientas de Debugging Útiles

**Ejecutar un Pod de debug:**
[source,bash]
----
# Pod debug simple
kubectl run -it --rm debug --image=busybox --restart=Never -- /bin/sh

# Pod debug con tooling avanzado
kubectl run -it --rm debug \
  --image=nicolaka/netshoot \
  --restart=Never \
  -- /bin/bash
----
]

**Dentro del Pod de debug:**
[source,bash]
----
# Verificar conectividad DNS
nslookup kubernetes.default
nslookup nginx-service

# Ping a service
ping nginx-service

# Wget/curl a servicio
wget -O- http://nginx-service:80
curl http://nginx-service:80

# Ver puertos abiertos
netstat -tlnp
ss -tlnp

# Verificar variables de entorno
env | grep KUBERNETES

# Ver servicios disponibles
nslookup nginx-service.default.svc.cluster.local
----
]

---

=== 2.6 Ejemplos Prácticos Completos

==== Ejemplo 1: Pod Simple con Health Checks

**YAML - simple-pod.yaml:**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: simple-app
  labels:
    app: simple
    version: v1
spec:
  containers:
  - name: app
    image: nginx:1.21
    ports:
    - containerPort: 80
      name: http
    
    livenessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 10
      periodSeconds: 10
    
    readinessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 5
    
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
----
]

**Uso:**
[source,bash]
----
kubectl apply -f simple-pod.yaml
kubectl get pods
kubectl describe pod simple-app
kubectl logs simple-app
kubectl port-forward pod/simple-app 8080:80
----
]

==== Ejemplo 2: Pod con Init Container

**YAML - init-container-pod.yaml:**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: init-demo
spec:
  initContainers:
  - name: wait-for-network
    image: busybox:latest
    command: ['sh', '-c', 'for i in {1..30}; do ping -c 1 8.8.8.8 && break || sleep 1; done']
  
  - name: setup-config
    image: busybox:latest
    command: ['sh', '-c', 'echo "APP_READY=true" > /config/app.conf']
    volumeMounts:
    - name: config-vol
      mountPath: /config
  
  containers:
  - name: app
    image: nginx:latest
    volumeMounts:
    - name: config-vol
      mountPath: /config
      readOnly: true
  
  volumes:
  - name: config-vol
    emptyDir: {}
----
]

==== Ejemplo 3: Pod con Sidecar para Logging

**YAML - sidecar-pod.yaml:**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-logging
spec:
  containers:
  - name: app
    image: nginx:latest
    ports:
    - containerPort: 80
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx
  
  - name: log-rotator
    image: busybox:latest
    command: ['sh', '-c', 'while true; do sleep 3600; rm -f /var/log/nginx/*.log.* ; done']
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/nginx
  
  volumes:
  - name: shared-logs
    emptyDir: {}
----
]

==== Ejemplo 4: Pod con Configuración y Secretos

**YAML - config-secret-pod.yaml:**
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  app.properties: |
    APP_NAME=MyApp
    LOG_LEVEL=INFO
    DATABASE_HOST=db.default.svc.cluster.local

---
apiVersion: v1
kind: Secret
metadata:
  name: db-credentials
type: Opaque
stringData:
  username: admin
  password: "secret123"

---
apiVersion: v1
kind: Pod
metadata:
  name: app-configured
spec:
  containers:
  - name: app
    image: myapp:latest
    env:
    - name: DB_USER
      valueFrom:
        secretKeyRef:
          name: db-credentials
          key: username
    - name: DB_PASS
      valueFrom:
        secretKeyRef:
          name: db-credentials
          key: password
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    
    volumeMounts:
    - name: config
      mountPath: /etc/config
      readOnly: true
    - name: secrets
      mountPath: /etc/secrets
      readOnly: true
  
  volumes:
  - name: config
    configMap:
      name: app-config
  - name: secrets
    secret:
      secretName: db-credentials
----
]

==== Ejemplo 5: Pod Seguro

**YAML - secure-pod.yaml:**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: secure-app
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
  
  containers:
  - name: app
    image: myapp:latest
    
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      capabilities:
        drop:
        - ALL
        add:
        - NET_BIND_SERVICE
    
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi
    
    volumeMounts:
    - name: tmp
      mountPath: /tmp
    - name: app-data
      mountPath: /app/data
  
  volumes:
  - name: tmp
    emptyDir: {}
  - name: app-data
    emptyDir: {}
----
]

---

=== 2.7 Resumen del Módulo 2

En este módulo aprendiste:

1. **Conceptos de Pods**: Unidad mínima, ciclo de vida, tipos de contenedores
2. **Crear Pods**: YAML declarativo, campos obligatorios y opcionales
3. **Gestión**: Listar, describir, logs, exec, port-forward
4. **Configuración**: Variables de entorno, volúmenes, probes
5. **Seguridad**: Security context, permisos, capabilities
6. **Troubleshooting**: Describe, logs, eventos, problemas comunes

Con estos conocimientos, estás listo para aprender sobre **Controladores como Deployments** en el Módulo 3.

---

== MÓDULO 3: Controladores

Un **Controlador** en Kubernetes es un bucle infinito que observa el estado actual del cluster y toma acciones para que coincida con el estado deseado. Es el concepto fundamental de Kubernetes.

**Patrón del Controlador:**
[source,
----
┌─────────────────────────────────────────┐
│     BUCLE INFINITO DEL CONTROLADOR      │
├─────────────────────────────────────────┤
│  1. OBSERVE (observar estado actual)    │
│  2. COMPARE (con estado deseado)        │
│  3. ACT (tomar acciones para sincronizar)
│  4. REPEAT (volver al paso 1)           │
└─────────────────────────────────────────┘
----
]

---

=== 3.1 Deployments

==== 3.1.1 Concepto de Deployment

Un **Deployment** es un controlador que gestiona un conjunto de Pods idénticos replicados. Es la forma **estándar y recomendada** de desplegar aplicaciones en Kubernetes.

**¿Por qué Deployments en lugar de Pods?**

Con Pods solos:
- Si un Pod falla, debe reiniciarlo manualmente
- Para escalar, debe crear Pods manualmente
- Para actualizar, debe eliminar y recrear Pods (downtime)

Con Deployments:
- ✅ Reinicio automático de Pods fallidos
- ✅ Escalado automático (manual o automático)
- ✅ Actualizaciones con cero downtime (rolling updates)
- ✅ Rollback automático si algo sale mal
- ✅ Historial de versiones

**Analogía:**
Un Deployment es como un manager de un equipo de trabajo:
- Asegura que siempre haya el número correcto de personas trabajando
- Si alguien se enferma, contrata a otro
- Cuando hay mucho trabajo, contrata a más personas
- Cuando necesita cambiar procesos, lo hace gradualmente para no perder eficiencia

==== 3.1.2 YAML de Deployment

**Estructura Mínima:**
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
        ports:
        - containerPort: 80
----
]

**Desglose:**
- `apiVersion: apps/v1`: Versión de API para Deployments
- `kind: Deployment`: Tipo de objeto
- `metadata.name`: Nombre único del Deployment
- `spec.replicas`: Número de Pods que deseas
- `spec.selector`: Cómo identificar los Pods que gestiona este Deployment
- `spec.template`: Plantilla para crear Pods (es básicamente un Pod spec)

**Estructura Completa con Todas las Opciones:**
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: production
  labels:
    app: web-app
    version: v1
  annotations:
    description: "Production web application"

spec:
  # Número de replicas
  replicas: 3
  
  # Revisión historial (para rollback)
  revisionHistoryLimit: 10
  
  # Estrategia de actualización
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1           # Máximo Pods extra durante update (además de replicas)
      maxUnavailable: 0     # Máximo Pods no disponibles durante update
  
  # Tiempo máximo para que un Deployment esté listo
  progressDeadlineSeconds: 600
  
  # Tiempo máximo entre checks de progreso
  minReadySeconds: 0
  
  # Selector de Pods
  selector:
    matchLabels:
      app: web-app
    matchExpressions:
    - key: version
      operator: In
      values: [v1, v2]
  
  # Plantilla de Pod
  template:
    metadata:
      labels:
        app: web-app
        version: v1
      annotations:
        prometheus.io/scrape: "true"
    
    spec:
      # Duración de shutdown grácil
      terminationGracePeriodSeconds: 30
      
      # Pod disruption budget
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - web-app
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: web-app
        image: myapp:1.0
        imagePullPolicy: IfNotPresent
        
        ports:
        - containerPort: 8080
          name: http
        
        env:
        - name: ENVIRONMENT
          value: production
        - name: LOG_LEVEL
          value: info
        
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          failureThreshold: 2
        
        volumeMounts:
        - name: config
          mountPath: /etc/config
      
      volumes:
      - name: config
        configMap:
          name: app-config
----
]

==== 3.1.3 Replicas y Escalado

**Ver replicas actuales:**
[source,bash]
----
kubectl get deployment nginx-deployment
kubectl describe deployment nginx-deployment

# Salida:
# Replicas: 3 desired | 3 updated | 3 total | 3 available
----
]

**Escalado Manual:**
[source,bash]
----
# Escalar a 5 replicas
kubectl scale deployment nginx-deployment --replicas=5

# Ver cambios
kubectl get pods

# Escalar hacia abajo
kubectl scale deployment nginx-deployment --replicas=2

# Editar directamente
kubectl edit deployment nginx-deployment
# Busca "replicas" y cambia el número
----
]

**Escalado mediante YAML:**
[source,bash]
----
# Actualizar el YAML y aplicar
kubectl apply -f deployment.yaml

# O hacer un patch
kubectl patch deployment nginx-deployment -p '{"spec":{"replicas":4}}'
----
]

**Observar escalado en tiempo real:**
[source,bash]
----
# Ver Pods siendo creados
kubectl get pods --watch

# Ver eventos de escalado
kubectl get events --sort-by='.lastTimestamp'

# Describir el Deployment para ver progreso
kubectl describe deployment nginx-deployment
----
]

==== 3.1.4 Actualización de Deployments

===== Rolling Update (Estrategia Recomendada)

La estrategia **RollingUpdate** actualiza Pods gradualmente, manteniendo disponibilidad.

**Cómo funciona:**
[source,
----
Estado Inicial (3 Pods v1):
Pod1-v1  Pod2-v1  Pod3-v1

Paso 1 (maxSurge=1, maxUnavailable=0):
Pod1-v1  Pod2-v1  Pod3-v1  Pod4-v2  ← Nuevo

Paso 2 (Elimina uno antiguo):
Pod1-v1  Pod2-v1  Pod4-v2  Pod5-v2

Paso 3:
Pod1-v1  Pod3-v2  Pod4-v2  Pod5-v2

Final (3 Pods v2):
Pod3-v2  Pod4-v2  Pod5-v2
----
]

**Configuración en YAML:**
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1          # Máximo 1 Pod extra (4 en lugar de 3)
      maxUnavailable: 0    # Siempre 3 disponibles (0 indisponibles)
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.22  # Versión actualizada
----
]

**Actualizar la versión de imagen:**
[source,bash]
----
# Método 1: Editar el Deployment
kubectl edit deployment nginx-deployment
# Cambiar imagen y guardar

# Método 2: Set image
kubectl set image deployment/nginx-deployment nginx=nginx:1.22

# Método 3: Patch
kubectl patch deployment nginx-deployment -p '{"spec":{"template":{"spec":{"containers":[{"name":"nginx","image":"nginx:1.22"}]}}}}'

# Ver progreso
kubectl rollout status deployment/nginx-deployment

# Ver en tiempo real
kubectl get pods --watch

# Ver eventos
kubectl get events --sort-by='.lastTimestamp'
----
]

===== Recreate Strategy

La estrategia **Recreate** elimina todos los Pods y luego crea nuevos. Causa downtime.

**Cuándo usar:**
- Aplicaciones que no pueden correr múltiples versiones simultáneamente
- Datos compartidos en estado inconsistente

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-deployment
spec:
  replicas: 3
  strategy:
    type: Recreate
    # No hay opciones de configuración para Recreate
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: myapp:2.0
----
]

**Cómo se ve:**
[source,
----
t=0s: 3 Pods v1 ejecutándose
t=5s: Se inician todos los Pods v1 (downtime)
t=10s: Se crean 3 Pods v2
t=15s: 3 Pods v2 ejecutándose

DOWNTIME: ~10 segundos
----
]

==== 3.1.5 Rollback de Deployments

Kubernetes guarda un historial de todas las versiones del Deployment.

**Ver historial:**
[source,bash]
----
# Ver todas las revisiones
kubectl rollout history deployment/nginx-deployment

# Salida:
# REVISION  CHANGE-CAUSE
# 1         kubectl apply --filename=deployment.yaml
# 2         kubectl set image deployment/nginx-deployment nginx=nginx:1.22
# 3         kubectl set image deployment/nginx-deployment nginx=nginx:1.23-broken

# Ver detalles de una revisión
kubectl rollout history deployment/nginx-deployment --revision=2
----
]

**Revertir a versión anterior:**
[source,bash]
----
# Revertir a la versión anterior
kubectl rollout undo deployment/nginx-deployment

# Revertir a una versión específica
kubectl rollout undo deployment/nginx-deployment --to-revision=2

# Ver progreso del rollback
kubectl rollout status deployment/nginx-deployment
----
]

**Configurar límite de revisiones a guardar:**
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  revisionHistoryLimit: 10  # Guardar últimas 10 versiones
  replicas: 3
  # ... resto del spec
----
]

==== 3.1.6 Casos de Uso

**Aplicaciones Web Stateless:**
```yaml
- Nginx, Apache
- APIs REST
- Frontends
```

**Microservicios:**
```yaml
- Cada microservicio como un Deployment
- Escalado independiente
- Actualizaciones independientes
```

**Batch Processing (para aplicaciones que pueden escalar):**
```yaml
- Workers de procesamiento
- Scraping distribuido
```

---

=== 3.2 ReplicaSets

==== 3.2.1 ¿Qué es un ReplicaSet?

Un **ReplicaSet** es un controlador de bajo nivel que **garantiza que un número específico de Pods idénticos está en ejecución**.

**Responsabilidades:**
- Crear/eliminar Pods para mantener el número deseado
- Monitorear la salud de los Pods
- Reiniciar Pods fallidos

==== 3.2.2 Diferencia con Deployment

[cols="1,1,1", options="header"]
|===
| Aspecto | ReplicaSet | Deployment
| Nivel | Bajo nivel | Alto nivel
| Gestión | Directa | Indirecta (via Deployment)
| Actualización | No tiene mecanismo | Rolling/Recreate updates
| Historial | No guarda | Sí (revisionHistoryLimit)
| Rollback | Manual | Automático
| Uso recomendado | Raramente directo | Siempre
|===

**Regla de Oro:** Casi nunca creas ReplicaSets directamente. Los Deployments los crean automáticamente.

==== 3.2.3 YAML de ReplicaSet

**Estructura:**
[source,yaml]
----
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21
----
]

**Ver ReplicaSets creadas por Deployments:**
[source,bash]
----
# Ver todos los ReplicaSets
kubectl get replicasets

# Salida:
# NAME                            DESIRED   CURRENT   READY
# nginx-deployment-5d4d4d4d4d     3         3         3
# nginx-deployment-7a8a8a8a8a     0         0         0  (versión anterior)

# El Deployment crea ReplicaSets automáticamente
# y las gestiona según la estrategia de actualización

# Ver relación
kubectl get deployment nginx-deployment -o yaml | grep -A 20 "spec:"

# Ver Pods creados por ReplicaSet
kubectl get pods -l app=nginx
----
]

==== 3.2.4 Manejo de Replicas

**Escalar ReplicaSet directamente (no recomendado):**
[source,bash]
----
# Si editas el ReplicaSet directamente
kubectl scale replicaset nginx-replicaset --replicas=5

# El Deployment lo revierte (si lo controla)
# Por eso es mejor escalar el Deployment
kubectl scale deployment nginx-deployment --replicas=5
----
]

==== 3.2.5 Labels y Selectors

Los ReplicaSets usan **selectors** para identificar qué Pods deben gestionar.

**Tipos de Selectors:**
[source,yaml]
----
# matchLabels: Coincidencia exacta
selector:
  matchLabels:
    app: nginx
    version: v1

# matchExpressions: Más flexibilidad
selector:
  matchExpressions:
  - key: app
    operator: In
    values: [nginx, apache]
  - key: environment
    operator: NotIn
    values: [test]
  - key: release
    operator: Exists

# Operadores disponibles:
# In, NotIn, Exists, DoesNotExist
----
]

**Ejemplo:**
[source,yaml]
----
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: complex-rs
spec:
  replicas: 3
  selector:
    matchExpressions:
    - key: tier
      operator: In
      values:
      - frontend
      - backend
    - key: environment
      operator: NotIn
      values:
      - test
  template:
    metadata:
      labels:
        tier: frontend
        environment: production
        version: v1
    spec:
      containers:
      - name: app
        image: myapp:1.0
----
]

---

=== 3.3 StatefulSets

==== 3.3.1 Diferencias con Deployments

**Deployments** asumen que Pods son **intercambiables** (stateless).
**StatefulSets** están diseñados para aplicaciones con **estado persistente**.

[cols="1,1,1", options="header"]
|===
| Aspecto | Deployment | StatefulSet
| Identidad de Pods | Efímera, intercambiable | Estable, única
| Nombres de Pods | Aleatorio (nginx-5d...) | Ordenado (mysql-0, mysql-1, mysql-2)
| DNS | A través de Service | DNS predecible para cada Pod
| Almacenamiento | Compartido | Dedicado a cada Pod
| Orden de inicio | Paralelo | Secuencial (por defecto)
| Casos de uso | Web apps, APIs | Bases de datos, colas
|===

**¿Por qué StatefulSets?**

Imagina una base de datos con 3 nodos:
- Con Deployment: Los nombres cambian, rompe la replicación
- Con StatefulSet: `postgres-0`, `postgres-1`, `postgres-2` (siempre)

==== 3.3.2 Identidades Estables

**Nombres ordenados:**
[source,bash]
----
# Con Deployment:
kubectl get pods
# nginx-deployment-5d4d... (nombre aleatorio)
# nginx-deployment-7a8a... (nombre aleatorio)

# Con StatefulSet:
kubectl get pods
# mysql-0
# mysql-1
# mysql-2
# (Siempre los mismos nombres)
----
]

**DNS predecible:**
[source,bash]
----
# Puedes confiar en estos nombres:
# mysql-0.mysql-headless.default.svc.cluster.local
# mysql-1.mysql-headless.default.svc.cluster.local
# mysql-2.mysql-headless.default.svc.cluster.local
----
]

==== 3.3.3 Almacenamiento Persistente

**VolumeClaimTemplates:**
[source,yaml]
----
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-statefulset
spec:
  serviceName: mysql-headless
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        ports:
        - containerPort: 3306
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
  
  # Clave para StatefulSets: volumeClaimTemplates
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "fast"
      resources:
        requests:
          storage: 10Gi
----
]

**Resultado:**
```yaml
Se crean automáticamente:
- mysql-0 con PVC: data-mysql-0 (10Gi)
- mysql-1 con PVC: data-mysql-1 (10Gi)
- mysql-2 con PVC: data-mysql-2 (10Gi)

Cada uno con su almacenamiento dedicado.
```

==== 3.3.4 Orden de Escalado

**Creación (secuencial):**
[source,
----
1. Crear mysql-0 → esperar a listo
2. Crear mysql-1 → esperar a listo
3. Crear mysql-2 → esperar a listo
----
]

**Eliminación (inverso):**
[source,
----
1. Eliminar mysql-2
2. Eliminar mysql-1
3. Eliminar mysql-0
----
]

**Configurar orden de inicio:**
[source,yaml]
----
spec:
  podManagementPolicy: Parallel  # Paralelo (por defecto: OrderedReady)
  # OrderedReady: Espera a que cada Pod esté listo
  # Parallel: Crea todos en paralelo
----
]

==== 3.3.5 Casos de Uso

**Bases de Datos:**
```yaml
- MySQL con replicación
- PostgreSQL con replicación
- MongoDB con replica sets
```

**Colas de Mensajes:**
```yaml
- Kafka brokers (ordenados)
- RabbitMQ nodes
```

**Almacenamiento Distribuido:**
```yaml
- Elasticsearch nodes
- Cassandra nodes
- HDFS datanodes
```

**Aplicaciones de Juegos:**
```yaml
- Game servers que necesitan identidad estable
- Session storage ordenado
```

==== 3.3.6 YAML Completo: StatefulSet de PostgreSQL

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-config
data:
  postgresql.conf: |
    shared_buffers = 256MB
    max_connections = 100

---
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
type: Opaque
stringData:
  password: "secure-password-123"

---
apiVersion: v1
kind: Service
metadata:
  name: postgres-headless
spec:
  clusterIP: None  # Headless service
  selector:
    app: postgres
  ports:
  - name: postgres
    port: 5432
    targetPort: 5432

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres-headless
  replicas: 3
  selector:
    matchLabels:
      app: postgres
  
  template:
    metadata:
      labels:
        app: postgres
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - postgres
            topologyKey: kubernetes.io/hostname
      
      initContainers:
      - name: init-chmod
        image: busybox
        command: ['chmod', '700', '/var/lib/postgresql/data']
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
      
      containers:
      - name: postgres
        image: postgres:13
        ports:
        - containerPort: 5432
          name: postgres
        
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        
        resources:
          requests:
            cpu: 250m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
        
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U postgres
          initialDelaySeconds: 30
          periodSeconds: 10
        
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U postgres
          initialDelaySeconds: 5
          periodSeconds: 10
        
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
        - name: config
          mountPath: /etc/postgresql
      
      volumes:
      - name: config
        configMap:
          name: postgres-config
  
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "standard"
      resources:
        requests:
          storage: 20Gi
----
]

---

=== 3.4 DaemonSets

==== 3.4.1 Concepto de DaemonSet

Un **DaemonSet** asegura que **cada nodo en el cluster tenga exactamente una copia de un Pod**.

**Característica clave:** Si agregas nuevos nodos, el DaemonSet automáticamente crea Pods en ellos.

[source,
----
┌─────────────────────────────────────────┐
│  CLUSTER KUBERNETES                     │
├─────────────────┬───────────────┬───────┤
│   NODO 1        │   NODO 2      │ NODO 3│
│                 │               │       │
│  ┌───────────┐  │ ┌───────────┐ │ ┌───┐ │
│  │ Pod-v1    │  │ │ Pod-v2    │ │ │Pod││
│  └───────────┘  │ └───────────┘ │ └───┘ │
└─────────────────┴───────────────┴───────┘

1 Pod por nodo (exactamente)
----
]

==== 3.4.2 Casos de Uso

**Monitoreo:**
```yaml
- Recolector de métricas (node-exporter para Prometheus)
- Recolector de logs (Fluentd, Logstash)
- APM agents
```

**Mantenimiento:**
```yaml
- Limpieza de disco
- Sincronización de datos
- Actualizaciones de sistema
```

**Networking:**
```yaml
- CNI plugins (Flannel, Calico)
- Ingress controller local
- Network proxies
```

**Seguridad:**
```yaml
- Antivirus
- Firewall local
- RBAC enforcers
```

==== 3.4.3 YAML de DaemonSet

**Ejemplo: Node Exporter (Prometheus)**
[source,yaml]
----
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
  labels:
    app: node-exporter
spec:
  selector:
    matchLabels:
      app: node-exporter
  
  # Política de actualización
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      # Permitir ejecutarse en nodos del control plane (master)
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      
      hostNetwork: true         # Acceso a red del nodo
      hostPID: true             # Acceso a PID del nodo
      
      containers:
      - name: node-exporter
        image: prom/node-exporter:latest
        args:
        - --path.procfs=/host/proc
        - --path.rootfs=/rootfs
        - --path.sysfs=/host/sys
        - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)
        
        ports:
        - containerPort: 9100
          name: metrics
        
        resources:
          requests:
            cpu: 100m
            memory: 64Mi
          limits:
            cpu: 200m
            memory: 128Mi
        
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        - name: rootfs
          mountPath: /rootfs
          readOnly: true
      
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      - name: rootfs
        hostPath:
          path: /
----
]

==== 3.4.4 Monitoreo con DaemonSets

**Ver DaemonSets:**
[source,bash]
----
# Listar DaemonSets
kubectl get daemonsets

# Ver detalles
kubectl describe daemonset node-exporter

# Ver Pods creados por DaemonSet
kubectl get pods -l app=node-exporter

# Salida: Un Pod por nodo
# node-exporter-xxxxx (en nodo-1)
# node-exporter-yyyyy (en nodo-2)
# node-exporter-zzzzz (en nodo-3)

# Escalar (no aplica para DaemonSets)
# Los Pods se crean automáticamente según nodos disponibles
----
]

==== 3.4.5 Logging Centralizado con Fluentd

**Ejemplo: DaemonSet de Fluentd**
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: logging
data:
  fluent.conf: |
    <source>
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/containers/*.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    <match **>
      @type elasticsearch
      @id output_elasticsearch
      @log_level info
      include_tag_key true
      host elasticsearch-service
      port 9200
      path _bulk
      <buffer>
        @type file
        path /var/log/fluentd-buffers/kubernetes.system.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        retry_max_times 18
      </buffer>
    </match>

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
  labels:
    app: fluentd
spec:
  selector:
    matchLabels:
      app: fluentd
  
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch-service"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        - name: FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX
          value: "logstash"
        
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: config
          mountPath: /fluentd/etc/fluent.conf
          subPath: fluent.conf
        - name: buffer
          mountPath: /var/log/fluentd-buffers
      
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config
        configMap:
          name: fluentd-config
      - name: buffer
        emptyDir: {}
----
]

---

=== 3.5 Jobs y CronJobs

==== 3.5.1 Jobs: Ejecución Única

Un **Job** crea Pods que ejecutan una tarea hasta completarse exitosamente, luego se detienen.

**Diferencias con Deployments:**
[cols="1,1,1", options="header"]
|===
| Aspecto | Deployment | Job
| Duración | Indefinida | Hasta completarse
| Reinicio fallido | Sí, forever | Configurable
| Exitoso | No existe ese concepto | Sí (exit 0)
| Uso | Servicios siempre activos | Tareas únicas
|===

**YAML de Job:**
[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: data-processing-job
spec:
  # Número de Pods que deben completarse exitosamente
  completions: 1
  
  # Número de Pods paralelos
  parallelism: 1
  
  # Tiempo máximo para completar el Job (segundos)
  activeDeadlineSeconds: 3600
  
  # Política de reinicio si falla
  backoffLimit: 3
  
  # Tiempo de retención después de completar
  ttlSecondsAfterFinished: 3600
  
  template:
    metadata:
      labels:
        app: data-processor
    spec:
      restartPolicy: Never  # Never, OnFailure, Always
      
      containers:
      - name: processor
        image: data-processor:1.0
        args:
        - --input=/data/input.csv
        - --output=/data/output.csv
        - --format=parquet
        
        volumeMounts:
        - name: data
          mountPath: /data
      
      volumes:
      - name: data
        hostPath:
          path: /mnt/data
----
]

**Gestión de Jobs:**
[source,bash]
----
# Crear Job
kubectl apply -f job.yaml

# Ver Jobs
kubectl get jobs

# Ver detalles
kubectl describe job data-processing-job

# Ver Pods del Job
kubectl get pods --selector=job-name=data-processing-job

# Ver logs del Job
kubectl logs job/data-processing-job

# Esperar a que complete
kubectl wait --for=condition=complete job/data-processing-job

# Eliminar Job (mantiene Pods)
kubectl delete job data-processing-job

# Eliminar Job y sus Pods
kubectl delete job data-processing-job --cascade=foreground
----
]

**Parámetros importantes:**

- `completions`: Cuántos Pods deben completar exitosamente
- `parallelism`: Cuántos Pods ejecutar en paralelo
- `backoffLimit`: Cuántas veces reintentar antes de marcar como fallido
- `ttlSecondsAfterFinished`: Tiempo antes de limpiar el Job completado
- `restartPolicy`: Never (no reiniciar), OnFailure (reiniciar si falla), Always (siempre)

==== 3.5.2 Job Paralelo

**Procesar trabajo en paralelo:**
[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: parallel-job
spec:
  # Se necesitan 10 completiones exitosas
  completions: 10
  
  # Ejecutar 5 Pods en paralelo
  parallelism: 5
  
  # Reintentar máximo 3 veces
  backoffLimit: 3
  
  activeDeadlineSeconds: 7200
  
  template:
    metadata:
      labels:
        app: worker
    spec:
      restartPolicy: OnFailure
      
      containers:
      - name: worker
        image: worker:1.0
        env:
        - name: JOB_INDEX
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
----
]

**Resultado:**
```yaml
Se crean 5 Pods inicialmente:
Pod-0, Pod-1, Pod-2, Pod-3, Pod-4

Cuando 1 completa, se crea Pod-5 (mantiene 5 en paralelo)
Cuando 2 completa, se crea Pod-6
...hasta 10 completiones totales
```

==== 3.5.3 CronJob: Ejecución Programada

Un **CronJob** es un Job que se ejecuta según un cronograma.

**YAML de CronJob:**
[source,yaml]
----
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-job
spec:
  # Cronograma (formato cron estándar de Unix)
  schedule: "0 2 * * *"  # Todos los días a las 2 AM
  
  # Zona horaria (opcional)
  timeZone: "America/New_York"
  
  # Mantener historial de Jobs exitosos
  successfulJobsHistoryLimit: 3
  
  # Mantener historial de Jobs fallidos
  failedJobsHistoryLimit: 1
  
  # Política de concurrencia
  concurrencyPolicy: Forbid  # Allow, Forbid, Replace
  
  # Plazo para ejecutar si se perdió una ejecución
  startingDeadlineSeconds: 300
  
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: backup
        spec:
          restartPolicy: OnFailure
          
          containers:
          - name: backup
            image: backup-tool:1.0
            args:
            - backup
            - --database=postgres
            - --destination=s3://backups/
            - --retention=30d
            
            env:
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-secret
                  key: access-key
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-secret
                  key: secret-key
----
]

**Sintaxis de Cronograma:**
[source,text]
----
┌───────────── minuto (0-59)
│ ┌───────────── hora (0-23)
│ │ ┌───────────── día del mes (1-31)
│ │ │ ┌───────────── mes (1-12)
│ │ │ │ ┌───────────── día de la semana (0-6, 0=domingo)
│ │ │ │ │
│ │ │ │ │
* * * * *

Ejemplos:
0 0 * * *   - Medianoche todos los días
0 9 * * 1-5 - 9 AM de lunes a viernes
*/30 * * * * - Cada 30 minutos
0 0 1 * *   - Primer día del mes a medianoche
----

**Gestión de CronJobs:**
[source,bash]
----
# Crear CronJob
kubectl apply -f cronjob.yaml

# Ver CronJobs
kubectl get cronjobs

# Ver detalles
kubectl describe cronjob backup-job

# Ver Jobs generados por CronJob
kubectl get jobs --selector=cronjob=backup-job

# Ver últimas ejecuciones
kubectl get jobs --sort-by=.metadata.creationTimestamp

# Ejecutar manualmente (para testing)
kubectl create job backup-manual --from=cronjob/backup-job

# Ver logs de última ejecución
kubectl logs job/backup-job-xxxxx
----
]

==== 3.5.4 Manejo de Fallos

**Política backoffLimit:**
[source,yaml]
----
spec:
  backoffLimit: 3  # Reintentar 3 veces antes de fallar
  
  # Tiempo entre reintentos: 1s, 2s, 4s, 8s... (exponencial)
  # hasta backoff.maxDuration (15 minutos)
----
]

**Política activeDeadlineSeconds:**
[source,yaml]
----
spec:
  activeDeadlineSeconds: 3600  # Máximo 1 hora total de ejecución
  
  # Si se excede, el Job se marca como fallido
----
]

**Política de concurrencia (para CronJobs):**
[source,yaml]
----
concurrencyPolicy: Allow    # Múltiples ejecuciones simultáneas (default)
concurrencyPolicy: Forbid   # No ejecutar si anterior aún corre
concurrencyPolicy: Replace  # Cancelar anterior y ejecutar nueva
----
]

==== 3.5.5 Limpieza de Jobs Completados

**Automática con ttlSecondsAfterFinished:**
[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: cleanup-job
spec:
  ttlSecondsAfterFinished: 3600  # Limpiar después de 1 hora
  completions: 1
  parallelism: 1
  
  template:
    metadata:
      labels:
        app: cleanup
    spec:
      restartPolicy: Never
      
      containers:
      - name: cleanup
        image: busybox:latest
        command: ['sh', '-c', 'echo "Cleaning up..." && sleep 5']
----
]

**Manual:**
[source,bash]
----
# Eliminar Jobs completados
kubectl delete jobs --field-selector=status.successful=1

# Eliminar Jobs fallidos
kubectl delete jobs --field-selector=status.failed=1

# Eliminar todo
kubectl delete jobs --all
----
]

---

=== 3.6 Ejemplos Completos

==== Ejemplo 1: Deployment con Todas las Características

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: production-app
  namespace: production
  labels:
    app: production-app
    version: v1
  annotations:
    description: "Production grade application deployment"
    owner: "platform-team"

spec:
  replicas: 3
  revisionHistoryLimit: 10
  
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  
  selector:
    matchLabels:
      app: production-app
  
  template:
    metadata:
      labels:
        app: production-app
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - production-app
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: app
        image: myapp:1.0
        imagePullPolicy: IfNotPresent
        
        ports:
        - containerPort: 8080
          name: http
        
        env:
        - name: ENVIRONMENT
          value: production
        - name: LOG_LEVEL
          value: info
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: password
        
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          failureThreshold: 2
        
        volumeMounts:
        - name: config
          mountPath: /etc/config
          readOnly: true
        - name: cache
          mountPath: /tmp/cache
      
      volumes:
      - name: config
        configMap:
          name: app-config
      - name: cache
        emptyDir: {}
      
      terminationGracePeriodSeconds: 30
----
]

#### Ejemplo 2: Job de Batch Processing

[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-processor
  labels:
    app: batch-processor
spec:
  completions: 100
  parallelism: 10
  backoffLimit: 3
  activeDeadlineSeconds: 7200
  ttlSecondsAfterFinished: 3600
  
  template:
    metadata:
      labels:
        app: batch-processor
    spec:
      restartPolicy: OnFailure
      
      initContainers:
      - name: download-data
        image: curl:latest
        command:
        - /bin/sh
        - -c
        - curl https://data.example.com/data.zip -o /data/data.zip
        volumeMounts:
        - name: data
          mountPath: /data
      
      containers:
      - name: processor
        image: batch-processor:1.0
        args:
        - --input=/data/data.zip
        - --index=$(JOB_COMPLETION_INDEX)
        - --output=/results/result-$(JOB_COMPLETION_INDEX).json
        
        env:
        - name: JOB_COMPLETION_INDEX
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
        
        volumeMounts:
        - name: data
          mountPath: /data
        - name: results
          mountPath: /results
      
      volumes:
      - name: data
        emptyDir: {}
      - name: results
        hostPath:
          path: /mnt/results
----
]

==== Ejemplo 3: CronJob de Backup

[source,yaml]
----
apiVersion: batch/v1
kind: CronJob
metadata:
  name: daily-backup
  labels:
    app: backup-system
spec:
  schedule: "0 2 * * *"  # 2 AM todos los días
  timeZone: "UTC"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 7
  failedJobsHistoryLimit: 3
  
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          serviceAccountName: backup-sa
          
          containers:
          - name: backup
            image: backup-tool:latest
            args:
            - backup
            - --database=postgres
            - --destination=s3://backups/
            - --encryption=true
            - --retention=30d
            - --notify=slack
            
            env:
            - name: DB_HOST
              value: postgres.database.svc.cluster.local
            - name: DB_USER
              valueFrom:
                secretKeyRef:
                  name: db-credentials
                  key: username
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: db-credentials
                  key: password
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: access-key
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: secret-key
            - name: SLACK_WEBHOOK
              valueFrom:
                secretKeyRef:
                  name: slack-webhook
                  key: url
            
            resources:
              requests:
                cpu: 500m
                memory: 512Mi
              limits:
                cpu: 1000m
                memory: 1024Mi
----
]

---

=== 3.7 Comparativa de Controladores

[cols="1,1,1,1,1", options="header"]
|===
| Característica | Deployment | StatefulSet | DaemonSet | Job
| Propósito | Apps stateless | Apps stateful | Monitoreo/Mantenimiento | Tareas únicas
| Réplicas | Escalable | Ordenado | Un Pod por nodo | Configurable
| Identidad | Efímera | Estable | Efímera | Efímera
| Almacenamiento | Compartido | Dedicado | Host path | Temporal
| Actualizaciones | Rolling/Recreate | Rolling | RollingUpdate | N/A
| Uso común | Web, APIs | BD, Colas | Monitoring | Backups, Procesamiento
|===

---

=== 3.8 Resumen del Módulo 3

En este módulo aprendiste:

1. **Deployments**: Orquestación de Pods stateless, actualizaciones, rollbacks
2. **ReplicaSets**: Bajo nivel, gestión de replicas (raramente directo)
3. **StatefulSets**: Aplicaciones con estado, identidades estables, almacenamiento dedicado
4. **DaemonSets**: Un Pod por nodo, monitoreo, logging centralizado
5. **Jobs**: Tareas únicas, batch processing, manejo de fallos
6. **CronJobs**: Tareas programadas, backups automáticos, mantenimiento

Con estos conocimientos, estás listo para aprender sobre **Servicios y Networking** en el Módulo 4.

---

== MÓDULO 4: Servicios y Networking

Los **Servicios** en Kubernetes son abstracciones que definen un conjunto lógico de Pods y cómo acceder a ellos. Sin Servicios, los Pods serían inaccesibles desde otros Pods o desde el exterior.

**Problema que resuelven los Servicios:**
- Los Pods tienen IPs que cambian constantemente
- Necesitamos una forma estable de acceder a Pods
- Necesitamos balancear carga entre múltiples Pods
- Necesitamos exponer aplicaciones al exterior

---

=== 4.1 Servicios Basics

==== 4.1.1 Concepto de Servicio

Un **Servicio** es un objeto Kubernetes que:
1. Agrupa Pods usando **labels**
2. Asigna un **IP y nombre DNS estable**
3. Proporciona **balanceo de carga** entre Pods
4. Es **independiente de la ubicación** de los Pods

**Analogía:**
Un Servicio es como un teléfono de atención al cliente:
- Clientes llaman a un número (Servicio)
- El teléfono dirige la llamada a agentes disponibles (Pods)
- Si un agente se va, el sistema redirecciona automáticamente
- Clientes no necesitan saber quién atiende

**Diagrama Visual:**
[source,
----
┌─────────────────────────────────────────────┐
│         KUBERNETES CLUSTER                  │
│                                             │
│  ┌──────────────────────────────────────┐   │
│  │   SERVICE (estable)                  │   │
│  │   IP: 10.0.1.10                      │   │
│  │   DNS: nginx.default.svc.cluster.local
│  └──────┬──────────────────────┬────────┘   │
│         │                      │            │
│    ┌────▼────┐           ┌─────▼────┐     │
│    │ Pod 1   │           │ Pod 2    │     │
│    │ (nginx) │           │ (nginx)  │     │
│    │ 10.1.1.5│           │ 10.1.2.7 │     │
│    └─────────┘           └──────────┘     │
│                                             │
│  Client requests: nginx.default.svc...     │
│  Service elige aleatoriamente Pod 1 o 2   │
└─────────────────────────────────────────────┘
----
]

==== 4.1.2 Por Qué se Necesitan Servicios

**Problema 1: IPs de Pods Cambian**

Con Pods solos:
```
Pod creado con IP 10.0.0.5
↓
Pod se reinicia
↓
Nueva IP 10.0.0.15
↓
Cliente intenta conectar a 10.0.0.5
↓
❌ Falla
```

Con Servicios:
```
Servicio con IP estable 10.0.1.10
↓
Pod 1 falla (10.0.0.5)
↓
Deployment crea Pod 2 (10.0.0.99)
↓
Servicio redirige a Pod 2 automáticamente
↓
✅ Cliente conecta sin cambios
```

**Problema 2: Balanceo de Carga**

Sin Servicio:
```
Cliente debe saber todas las IPs de Pods
→ 10.0.0.5, 10.0.0.7, 10.0.0.9
→ Cliente debe balancear manualmente
```

Con Servicio:
```
Cliente conecta a: nginx-service
Servicio distribuye tráfico a todos los Pods
Balanceo automático y transparente
```

==== 4.1.3 Selector Labels

Los Servicios usan **labels** para identificar qué Pods gestiona.

**Ejemplo:**
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx        # ← Busca Pods con este label
    environment: prod # ← Y este
  ports:
  - port: 80
    targetPort: 8080

---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx        # ← Coincide con selector
    environment: prod # ← Coincide con selector
    version: v1       # ← No afecta (no está en selector)
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 8080
----
]

El Servicio automáticamente descubre y agrupa todos los Pods con `app: nginx` y `environment: prod`.

==== 4.1.4 Endpoints

**Endpoints** son los Pods reales que el Servicio mantiene automáticamente.

[source,bash]
----
# Ver Pods que gestiona un Servicio
kubectl get endpoints nginx-service

# Salida:
# NAME              ENDPOINTS
# nginx-service     10.0.0.5:8080,10.0.0.7:8080,10.0.0.9:8080

# Ver con detalles
kubectl describe service nginx-service
# Endpoints: 10.0.0.5:8080, 10.0.0.7:8080, 10.0.0.9:8080

# Ver el objeto Endpoints directamente
kubectl get endpoints
kubectl describe endpoints nginx-service
----
]

**Cómo funciona:**
```yaml
1. Servicio define selector: app=nginx
2. Kubernetes busca Pods con ese label
3. Crea objeto Endpoints con las IPs de Pods
4. kube-proxy usa Endpoints para balanceo de carga
5. Si Pod falla, lo elimina de Endpoints automáticamente
6. Si nuevo Pod nace, lo agrega
```

---

=== 4.2 Tipos de Servicios

==== 4.2.1 ClusterIP

**ClusterIP** es el tipo de Servicio por defecto. Proporciona acceso **dentro del cluster**.

**Características:**
- IP interna (solo accesible desde dentro del cluster)
- DNS: `nombre.namespace.svc.cluster.local`
- Balanceo de carga automático
- No accesible desde afuera

**YAML:**
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  namespace: default
  labels:
    app: nginx
spec:
  type: ClusterIP        # Por defecto, puede omitirse
  
  # IP interna específica (opcional, genera una si no se especifica)
  clusterIP: 10.0.1.100
  
  selector:
    app: nginx
  
  ports:
  - name: http
    protocol: TCP
    port: 80              # Puerto del Servicio (acceso interno)
    targetPort: 8080      # Puerto del contenedor
    
  - name: https
    protocol: TCP
    port: 443
    targetPort: 8443
  
  # SessionAffinity para sticky sessions (opcional)
  sessionAffinity: None   # None, ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
----
]

**Acceso dentro del cluster:**
[source,bash]
----
# Desde otro Pod, accedes así:
curl http://nginx-service           # En el mismo namespace
curl http://nginx-service:80        # Especificar puerto
curl http://nginx-service.default.svc.cluster.local  # FQDN completo

# Puedes usar variables de entorno:
curl http://$NGINX_SERVICE_HOST:$NGINX_SERVICE_PORT
----
]

**Ejemplo Completo: Frontend a Backend**
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-api
spec:
  replicas: 2
  selector:
    matchLabels:
      app: backend-api
  template:
    metadata:
      labels:
        app: backend-api
    spec:
      containers:
      - name: api
        image: backend:1.0
        ports:
        - containerPort: 5000

---
apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  type: ClusterIP
  selector:
    app: backend-api
  ports:
  - port: 5000
    targetPort: 5000

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
      - name: web
        image: frontend:1.0
        ports:
        - containerPort: 3000
        env:
        - name: BACKEND_URL
          value: "http://backend-service:5000"  # ← Acceso automático

---
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
spec:
  type: NodePort  # Para acceso externo
  selector:
    app: frontend
  ports:
  - port: 3000
    targetPort: 3000
    nodePort: 30000
----
]

==== 4.2.2 NodePort

**NodePort** expone el Servicio en un puerto específico en **cada nodo del cluster**.

**Características:**
- Crea un ClusterIP internamente
- Expone cada nodo en un puerto (30000-32767)
- Accesible desde fuera del cluster
- Acceso: `<nodo-ip>:<nodePort>`

**Diagrama:**
[source,
----
Internet
   │
   ├─ 192.168.1.10:30080 ─────┐
   ├─ 192.168.1.11:30080 ─────┤
   ├─ 192.168.1.12:30080 ─────┤
   │                          │
   └──────────────────────────┘
                │
          NodePort Service
          (redirige al Pod)
                │
         ┌──────┴──────┐
         │              │
      Pod 1 (10.0.0.5)  Pod 2 (10.0.0.7)
----
]

**YAML:**
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: web-service
  labels:
    app: web
spec:
  type: NodePort
  
  selector:
    app: web
  
  ports:
  - name: http
    protocol: TCP
    port: 80                # Puerto interno
    targetPort: 8080        # Puerto del contenedor
    nodePort: 30080         # Puerto en los nodos (30000-32767)
                            # Si no especificas, K8s elige uno
  
  # Rango de puertos:
  # - Por defecto: 30000-32767
  # - Configurable en API Server: --service-node-port-range=30000:35000
----
]

**Acceso:**
[source,bash]
----
# Obtener nodos
kubectl get nodes -o wide
# NAME      STATUS  ROLES  ... EXTERNAL-IP   INTERNAL-IP
# master    Ready   ...       203.0.113.10   192.168.1.10
# worker1   Ready   ...       203.0.113.11   192.168.1.11

# Obtener NodePort
kubectl get svc web-service
# NAME          TYPE      PORT(S)        AGE
# web-service   NodePort  80:30080/TCP   5m

# Acceder desde internet
curl http://203.0.113.10:30080  # IP externa del nodo
curl http://192.168.1.10:30080  # IP interna del nodo
curl http://node-hostname:30080 # También funciona

# Los tres nodos sirven el mismo servicio:
curl http://203.0.113.10:30080
curl http://203.0.113.11:30080
curl http://203.0.113.12:30080
# Todas van al mismo Pod (con balanceo)
----
]

**Casos de Uso:**
- Desarrollo/testing
- Acceso temporal
- No recomendado para producción (usa LoadBalancer o Ingress)

==== 4.2.3 LoadBalancer

**LoadBalancer** usa un balanceador de carga externo del proveedor cloud para exponer el Servicio.

**Características:**
- Crea ClusterIP + NodePort + LoadBalancer
- IP externa asignada por el cloud provider
- Acceso directo desde internet
- Recomendado para producción
- Requiere integración con cloud provider

**Diagrama:**
[source,
----
Internet (203.0.113.50:80)
          │
     ┌────▼────────┐
     │ Cloud LB    │ ← Balanceador del proveedor
     │ (AWS ELB)   │   (AWS, Google Cloud, Azure, etc.)
     └────┬────────┘
          │
     ┌────┴──────────┐
     │               │
  Node1:30080    Node2:30080   (NodePort)
     │               │
  Pod-1 (5000)   Pod-2 (5000)  (Servicio)
----
]

**YAML:**
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: web-lb-service
  namespace: production
  labels:
    app: web
spec:
  type: LoadBalancer
  
  selector:
    app: web
  
  ports:
  - name: http
    protocol: TCP
    port: 80              # Puerto externo (en el LB)
    targetPort: 8080      # Puerto del Pod
    nodePort: 30080       # NodePort (generado automáticamente)
  
  # IP específica para el LoadBalancer (si el cloud lo permite)
  loadBalancerIP: 203.0.113.50
  
  # Restricción de acceso (si el cloud lo permite)
  loadBalancerSourceRanges:
  - 203.0.113.0/24
  - 198.51.100.0/24
  
  # Política de tráfico
  externalTrafficPolicy: Local  # Local, Cluster (por defecto)
  # Local: Solo Pods en el nodo que recibe tráfico
  # Cluster: kube-proxy redirecciona entre nodos
  
  # Session affinity
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
----
]

**Acceso:**
[source,bash]
----
# Ver el Servicio LoadBalancer
kubectl get svc web-lb-service
# NAME              TYPE          EXTERNAL-IP    PORT(S)
# web-lb-service    LoadBalancer  203.0.113.50   80:30080/TCP

# Esperar a que el cloud asigne IP (puede tomar minutos)
kubectl get svc web-lb-service --watch

# Acceder
curl http://203.0.113.50:80

# Ver detalles
kubectl describe svc web-lb-service
----
]

**Ejemplo en AWS:**
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: web-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"  # Network Load Balancer
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "http"
spec:
  type: LoadBalancer
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 8080
----
]

==== 4.2.4 ExternalName

**ExternalName** mapea un Servicio a un nombre DNS externo.

**Características:**
- No proporciona balanceo de carga
- Es un alias DNS
- Útil para integrar servicios externos

**YAML:**
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: external-db
  namespace: default
spec:
  type: ExternalName
  
  # El FQDN del servicio externo
  externalName: postgres.example.com
  
  # Puertos (opcional)
  ports:
  - port: 5432
    protocol: TCP
----
]

**Acceso desde dentro del cluster:**
[source,bash]
----
# Dentro de cualquier Pod
psql -h external-db -p 5432

# Se resuelve a: postgres.example.com
# Y luego a la IP real
----
]

**Caso de Uso: Migración Gradual**
[source,yaml]
----
# Primero: Todos usan ExternalName que apunta al DB externo
apiVersion: v1
kind: Service
metadata:
  name: database
spec:
  type: ExternalName
  externalName: old-db.example.com
  ports:
  - port: 5432

---
# Luego: Cambias ExternalName a un ClusterIP con StatefulSet
apiVersion: v1
kind: Service
metadata:
  name: database
spec:
  type: ClusterIP
  selector:
    app: postgres
  ports:
  - port: 5432

# Aplicaciones no necesitan cambios, solo cambia el Servicio
----
]

**Tabla Comparativa de Tipos:**
[cols="1,1,1,1,1", options="header"]
|===
| Tipo | Acceso Interno | Acceso Externo | IP Estable | Caso de Uso
| ClusterIP | ✅ | ❌ | ✅ | Inter-Pod communication
| NodePort | ✅ | ✅ (limitado) | ✅ | Testing, desarrollo
| LoadBalancer | ✅ | ✅ | ✅ | Producción, aplicaciones públicas
| ExternalName | ✅ | ❌ | ✅ | Mapeo a servicios externos
|===

---

=== 4.3 Discovery y DNS

==== 4.3.1 Kubernetes DNS

**CoreDNS** es el servidor DNS de Kubernetes. Cada cluster tiene un Servicio CoreDNS que resuelve nombres automáticamente.

[source,bash]
----
# Ver CoreDNS en acción
kubectl get pods -n kube-system | grep coredns

# CoreDNS responde a todas las consultas DNS del cluster
----
]

**Características:**
- Cada Pod tiene acceso automático al DNS del cluster
- Resolución de nombres de Servicios
- Resolución de Pods individuales
- Caché local
- Forwarding a DNS externos

==== 4.3.2 Nombres de Dominio Internos

**Estructura FQDN completo:**
```
service-name.namespace.svc.cluster.local
```

**Niveles:**
```
service-name        → Nombre del Servicio
namespace           → Namespace donde está
svc                 → Service (tipo de recurso)
cluster.local       → Dominio raíz del cluster
```

**Ejemplos:**
[source,bash]
----
# En el mismo namespace:
ping nginx-service              # ← Resuelve dentro del namespace
ping nginx-service.default      # ← Con namespace explícito

# Desde otro namespace:
ping nginx-service.production.svc.cluster.local
# ← FQDN completo necesario

# En el namespace default:
curl http://nginx-service:80
# En el namespace kube-system (necesitas FQDN):
curl http://nginx-service.default.svc.cluster.local:80
----
]

**DNS de Pods:**
[source,bash]
----
# Pod individual:
pod-ip-dash-separated.namespace.pod.cluster.local

# Ejemplo: Pod con IP 10.0.1.5 en namespace default
# 10-0-1-5.default.pod.cluster.local

# Acceso a Pod directo (raro, no recomendado):
curl http://10-0-1-5.default.pod.cluster.local
----
]

==== 4.3.3 Búsqueda de Servicios

**Variables de entorno automáticas:**
[source,bash]
----
# Kubernetes inyecta automáticamente variables de entorno
# para cada Servicio en el namespace

env | grep NGINX
# NGINX_SERVICE_HOST=10.0.1.100
# NGINX_SERVICE_PORT=80
# NGINX_SERVICE_PORT_HTTP=80
----
]

**Ejemplo en aplicación:**
[source,python]
----
import os

# Método 1: Variables de entorno (más antiguo)
host = os.getenv('NGINX_SERVICE_HOST')
port = os.getenv('NGINX_SERVICE_PORT')

# Método 2: DNS directo (recomendado)
host = 'nginx-service'  # O 'nginx-service.default.svc.cluster.local'
port = 80
----
]

==== 4.3.4 Configuración de Resolución DNS

**Ver configuración DNS en un Pod:**
[source,bash]
----
# Dentro de un Pod
cat /etc/resolv.conf

# Salida típica:
# nameserver 10.96.0.10          ← IP de CoreDNS
# search default.svc.cluster.local svc.cluster.local cluster.local
# options ndots:5 single-request-tcp
----
]

**Explicación:**
- `nameserver 10.96.0.10`: IP del Servicio CoreDNS
- `search`: Dominios que buscar automáticamente
- `ndots:5`: Si hay más de 5 puntos, busca FQDN primero

**Configurar DNS personalizado:**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: dns-config-pod
spec:
  dnsPolicy: Default  # Usar DNS del nodo
  dnsConfig:
    nameservers:
    - 8.8.8.8
    - 8.8.4.4
    searches:
    - my.custom.domain
    options:
    - name: ndots
      value: "2"
  
  containers:
  - name: app
    image: busybox:latest
----
]

==== 4.3.5 Headless Services

Un **Headless Service** no asigna ClusterIP. Se usa para aplicaciones que necesitan comunicación directa entre Pods.

**Cuándo usar:**
- StatefulSets (identidades estables)
- Aplicaciones de máster-esclavo
- Servicios que no necesitan balanceo de carga

**YAML:**
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: mysql-headless
spec:
  clusterIP: None  # ← Headless
  selector:
    app: mysql
  ports:
  - port: 3306
    targetPort: 3306

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql-headless  # ← Vinculado al Headless Service
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        ports:
        - containerPort: 3306
----
]

**DNS con Headless Service:**
[source,bash]
----
# Con ClusterIP normal:
$ nslookup nginx-service
# nginx-service.default.svc.cluster.local → 10.0.1.100 (una IP)

# Con Headless Service:
$ nslookup mysql-headless
# mysql-headless.default.svc.cluster.local → 10.0.0.5
#                                         → 10.0.0.7
#                                         → 10.0.0.9
# (Todas las IPs de Pods)

# Acceso a Pod específico en StatefulSet:
$ nslookup mysql-0.mysql-headless
# mysql-0.mysql-headless.default.svc.cluster.local → 10.0.0.5

# Conexión directa a Pod específico:
mysql -h mysql-0.mysql-headless.default.svc.cluster.local
----
]

---

=== 4.4 Ingress

==== 4.4.1 Concepto de Ingress

**Ingress** es un objeto Kubernetes que expone **rutas HTTP/HTTPS** a Servicios dentro del cluster.

**Problema que resuelve:**
- LoadBalancer crea un LB por Servicio (costoso)
- Ingress permite múltiples Servicios con un LB

**Analogía:**
- LoadBalancer = Línea telefónica dedicada (costosa)
- Ingress = Centralita que redirecciona llamadas según número

**Diagrama:**
[source,
----
Internet
   │
   └─ 203.0.113.50:80
        │
    ┌───▼──────────┐
    │ Ingress      │
    │ Controller   │
    └───┬──────────┘
        │
    ┌───┴──────────────────┐
    │                      │
┌───▼───┐            ┌────▼───┐
│ Service1 (api)    │ Service2 (web)
│ nginx:80          │ frontend:3000
└─────────┘         └────────┘
----
]

**Ventajas:**
- Un IP/hostname para múltiples servicios
- Enrutamiento basado en ruta y host
- HTTPS/TLS centralizado
- Más económico que múltiples LoadBalancers

==== 4.4.2 Ingress Controller

El **Ingress Controller** implementa las reglas de Ingress. Es un programa que corre en el cluster.

**Controladores disponibles:**
- **nginx-ingress**: El más popular
- **Traefik**: Moderno, fácil de usar
- **HAProxy**: Robusto
- **AWS ALB Controller**: Para AWS
- **GCE Ingress**: Para Google Cloud

**Instalación de nginx-ingress:**
[source,bash]
----
# Instalación estándar
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.0/deploy/static/provider/cloud/deploy.yaml

# O con Helm
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm install my-ingress ingress-nginx/ingress-nginx \
  --namespace ingress-nginx \
  --create-namespace

# Verificar instalación
kubectl get pods -n ingress-nginx
kubectl get svc -n ingress-nginx
----
]

==== 4.4.3 Reglas de Ingress

**YAML Mínimo:**
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
spec:
  ingressClassName: nginx  # Especificar qué controller
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
----
]

**YAML Completo con Múltiples Servicios:**
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: multi-service-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/rate-limit: "100"
spec:
  ingressClassName: nginx
  
  rules:
  # API backend
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 5000
  
  # Frontend
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 3000
      
      # Admin
      - path: /admin
        pathType: Prefix
        backend:
          service:
            name: admin-service
            port:
              number: 8080
  
  # Ruta por defecto
  - host: any.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: default-service
            port:
              number: 80
----
]

**PathType Explained:**
```yaml
Exact      - /admin solo coincide /admin, no /admin/
Prefix     - /admin coincide /admin, /admin/, /admin/users
ImplementationSpecific - Depende del Ingress Controller
```

==== 4.4.4 Hosts Virtuales

Puedes servir diferentes aplicaciones en diferentes hosts con un solo Ingress.

**Ejemplo:**
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: vhost-ingress
spec:
  ingressClassName: nginx
  
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 5000
  
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 3000
  
  - host: admin.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: admin-service
            port:
              number: 8080
  
  - host: docs.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: docs-service
            port:
              number: 3000
----
]

**DNS debe apuntar a Ingress:**
[source,bash]
----
# Obtener IP/hostname del Ingress
kubectl get ingress web-ingress

# Salida:
# NAME          CLASS   HOSTS                           ADDRESS       PORTS
# web-ingress   nginx   api.example.com,www.example.com 203.0.113.50  80, 443

# En tu DNS:
api.example.com         → 203.0.113.50
www.example.com         → 203.0.113.50
admin.example.com       → 203.0.113.50
docs.example.com        → 203.0.113.50

# Ingress Controller redirige según Host header
----
]

==== 4.4.5 TLS/SSL

Ingress puede terminar conexiones HTTPS usando Secrets.

**Crear certificado:**
[source,bash]
----
# Generar certificado auto-firmado
openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes \
  -subj "/CN=example.com"

# Crear Secret con certificado
kubectl create secret tls my-tls-secret \
  --cert=cert.pem \
  --key=key.pem
----
]

**Usar TLS en Ingress:**
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tls-ingress
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  ingressClassName: nginx
  
  # TLS configuration
  tls:
  - hosts:
    - example.com
    - www.example.com
    secretName: my-tls-secret  # Secret con cert + key
  
  - hosts:
    - api.example.com
    secretName: api-tls-secret
  
  # HTTP rules
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
  
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 5000
----
]

**Con Let's Encrypt automático (require cert-manager):**
[source,yaml]
----
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: admin@example.com
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          class: nginx

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tls-ingress
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  ingressClassName: nginx
  
  tls:
  - hosts:
    - example.com
    secretName: example-com-tls  # Se genera automáticamente
  
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
----
]

==== 4.4.6 Path-Based Routing

Diferentes Servicios basados en ruta:

**Ejemplo:**
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: path-routing
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /v1
        pathType: Prefix
        backend:
          service:
            name: api-v1-service
            port:
              number: 5000
      
      - path: /v2
        pathType: Prefix
        backend:
          service:
            name: api-v2-service
            port:
              number: 5001
      
      - path: /admin
        pathType: Prefix
        backend:
          service:
            name: admin-service
            port:
              number: 8080
      
      - path: /
        pathType: Prefix
        backend:
          service:
            name: default-service
            port:
              number: 5000
----
]

**Requests:**
```
GET api.example.com/v1/users     → api-v1-service
GET api.example.com/v2/users     → api-v2-service
GET api.example.com/admin/users  → admin-service
GET api.example.com/status       → default-service
```

==== 4.4.7 Host-Based Routing

Diferentes Servicios basados en hostname:

**Ejemplo:**
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: host-routing
spec:
  ingressClassName: nginx
  
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 5000
  
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 3000
  
  - host: cdn.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: cdn-service
            port:
              number: 9000
  
  - host: shop.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: shop-service
            port:
              number: 8080
----
]

---

=== 4.5 Network Policies

==== 4.5.1 Concepto de Network Policy

**Network Policy** es un objeto Kubernetes que define reglas de seguridad de red entre Pods.

**Sin Network Policy:**
```
Todos los Pods pueden comunicarse entre sí
↓
Riesgo de seguridad
```

**Con Network Policy:**
```
Solo tráfico permitido por reglas
↓
Defensa en profundidad
```

==== 4.5.2 Ingress Rules

Define qué Pods pueden **recibir** tráfico.

**Ejemplo 1: Solo desde namespace específico**
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-network-policy
spec:
  podSelector:
    matchLabels:
      app: database  # Aplica a Pods con este label
  
  policyTypes:
  - Ingress
  
  ingress:
  - from:
    - namespaceSelector:  # Solo desde este namespace
        matchLabels:
          name: production
    ports:
    - protocol: TCP
      port: 5432
----
]

**Ejemplo 2: Solo desde Pods específicos**
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-network-policy
spec:
  podSelector:
    matchLabels:
      app: api  # Aplica a API Pods
  
  policyTypes:
  - Ingress
  
  ingress:
  - from:
    - podSelector:  # Solo desde Pods con este label
        matchLabels:
          app: frontend  # Todos los frontends en el mismo namespace
    ports:
    - protocol: TCP
      port: 5000
    
    - podSelector:
        matchLabels:
          role: admin  # O desde Pods admin (en cualquier namespace)
      namespaceSelector:
        matchLabels:
          name: management
----
]

**Ejemplo 3: Múltiples origenes**
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: multi-source-policy
spec:
  podSelector:
    matchLabels:
      app: backend
  
  policyTypes:
  - Ingress
  
  ingress:
  # Opción 1: Frontend en cualquier lugar
  - from:
    - podSelector:
        matchLabels:
          app: frontend
  
  # Opción 2: Admin desde namespace management
  - from:
    - namespaceSelector:
        matchLabels:
          name: management
      podSelector:
        matchLabels:
          role: admin
  
  # Opción 3: Monitoreo desde namespace monitoring
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 9090  # Prometheus
----
]

==== 4.5.3 Egress Rules

Define qué Pods pueden **enviar** tráfico.

**Ejemplo: Backend solo a database y API externa**
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-egress-policy
spec:
  podSelector:
    matchLabels:
      app: backend
  
  policyTypes:
  - Egress
  
  egress:
  # Permitir a database
  - to:
    - podSelector:
        matchLabels:
          app: database
    ports:
    - protocol: TCP
      port: 5432
  
  # Permitir a API externa
  - to:
    - namespaceSelector:
        matchLabels:
          name: default
    podSelector:
        matchLabels:
          app: external-api
    ports:
    - protocol: TCP
      port: 443
  
  # Permitir DNS (necesario para resolver nombres)
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    podSelector:
      matchLabels:
        k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
----
]

==== 4.5.4 Selección de Pods

**PodSelector:**
```yaml
# Todos los Pods en el namespace
podSelector: {}

# Pods con label específico
podSelector:
  matchLabels:
    app: nginx

# Pods con múltiples labels
podSelector:
  matchLabels:
    app: nginx
    tier: frontend

# Expresiones (más flexibilidad)
podSelector:
  matchExpressions:
  - key: app
    operator: In
    values: [nginx, apache]
```

**NamespaceSelector:**
```yaml
# Todos los namespaces
namespaceSelector: {}

# Namespaces con label específico
namespaceSelector:
  matchLabels:
    name: production

# Namespace actual (raro)
namespaceSelector:
  matchLabels:
    kubernetes.io/metadata.name: default
```

**IPBlock (por rango IP):**
```yaml
# Desde rango IP específico
ipBlock:
  cidr: 203.0.113.0/24
  except:
  - 203.0.113.50  # Excepto esta IP

# A rango IP externo
to:
- ipBlock:
    cidr: 0.0.0.0/0
    except:
    - 10.0.0.0/8  # Excepto red interna
```

==== 4.5.5 Casos de Uso de Seguridad

**Caso 1: Arquitectura de 3 niveles**
[source,yaml]
----
# Frontend solo recibe desde internet
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-policy
spec:
  podSelector:
    matchLabels:
      tier: frontend
  policyTypes:
  - Ingress
  ingress:
  - from: []  # Permite de cualquier lugar
    ports:
    - protocol: TCP
      port: 80

---
# API solo recibe desde frontend
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-policy
spec:
  podSelector:
    matchLabels:
      tier: api
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: frontend
    ports:
    - protocol: TCP
      port: 5000

---
# Database solo recibe desde API
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-policy
spec:
  podSelector:
    matchLabels:
      tier: database
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: api
    ports:
    - protocol: TCP
      port: 5432
----
]

**Caso 2: Deny All, Allow Only Specific (Default Deny)**
[source,yaml]
----
# Denegar todo tráfico de entrada
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}  # Aplica a todos los Pods
  policyTypes:
  - Ingress

---
# Denegar todo tráfico de salida
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-egress
spec:
  podSelector: {}  # Aplica a todos los Pods
  policyTypes:
  - Egress

---
# Luego permite específicamente
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend
spec:
  podSelector:
    matchLabels:
      app: frontend
  policyTypes:
  - Ingress
  ingress:
  - from: []  # Permite desde cualquier lugar
    ports:
    - protocol: TCP
      port: 80
----
]

**Caso 3: Microservicios isolados**
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: microservices-isolation
spec:
  podSelector: {}  # Aplicar a todos
  
  policyTypes:
  - Ingress
  - Egress
  
  # Permitir solo tráfico desde ServiceMonitor/Prometheus
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
      podSelector:
        matchLabels:
          app: prometheus
    ports:
    - protocol: TCP
      port: 9090
  
  # Permitir DNS y servicios internos
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
  
  - to:
    - podSelector: {}  # Otros Pods del cluster
    ports:
    - protocol: TCP
----
]

==== 4.5.6 Debugging de Network Policies

**Verificar policies aplicadas:**
[source,bash]
----
# Ver todas las network policies
kubectl get networkpolicies

# Ver en namespace específico
kubectl get networkpolicies -n production

# Ver detalles
kubectl describe networkpolicy frontend-policy

# Ver en YAML
kubectl get networkpolicy frontend-policy -o yaml

# Verificar tráfico permitido
# Crear Pod de debug
kubectl run -it --rm debug --image=nicolaka/netshoot --restart=Never -- /bin/bash

# Dentro del Pod de debug
curl http://api-service:5000  # ¿Funciona o está bloqueado?
----
]

**Verificar si el tráfico está bloqueado:**
[source,bash]
----
# Ver logs del network plugin (depende del CNI)
# Para Calico:
kubectl logs -n calico-system -l k8s-app=calico-node

# Para Cilium:
kubectl logs -n cilium -l k8s-app=cilium

# Verificar si tráfico es rechazado
kubectl top pods  # Ver uso de red
----
]

---

=== 4.6 Ejemplos Completos

==== Ejemplo 1: Aplicación Web Multiservicio

[source,yaml]
----
---
# Frontend Service
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
  labels:
    app: frontend
spec:
  type: ClusterIP
  selector:
    app: frontend
  ports:
  - port: 3000
    targetPort: 3000

---
# Backend Service (ClusterIP - acceso interno)
apiVersion: v1
kind: Service
metadata:
  name: backend-service
  labels:
    app: backend
spec:
  type: ClusterIP
  selector:
    app: backend
  ports:
  - name: http
    port: 5000
    targetPort: 5000

---
# Database Service (Headless)
apiVersion: v1
kind: Service
metadata:
  name: postgres-headless
spec:
  clusterIP: None
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432

---
# Ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/rate-limit: "100"
spec:
  ingressClassName: nginx
  
  tls:
  - hosts:
    - example.com
    - api.example.com
    secretName: example-com-tls
  
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 3000
  
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: backend-service
            port:
              number: 5000

---
# Network Policies
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-policy
spec:
  podSelector:
    matchLabels:
      app: frontend
  policyTypes:
  - Ingress
  ingress:
  - from: []  # De cualquier lugar
    ports:
    - protocol: TCP
      port: 3000

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-policy
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 5000

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-policy
spec:
  podSelector:
    matchLabels:
      app: postgres
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: backend
    ports:
    - protocol: TCP
      port: 5432
----
]

==== Ejemplo 2: Microservicios con múltiples versiones

[source,yaml]
----
---
# API v1 Service
apiVersion: v1
kind: Service
metadata:
  name: api-v1-service
spec:
  type: ClusterIP
  selector:
    app: api
    version: v1
  ports:
  - port: 5000
    targetPort: 5000

---
# API v2 Service
apiVersion: v1
kind: Service
metadata:
  name: api-v2-service
spec:
  type: ClusterIP
  selector:
    app: api
    version: v2
  ports:
  - port: 5000
    targetPort: 5000

---
# Ingress con path-based routing
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: api-versioning
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /v1
        pathType: Prefix
        backend:
          service:
            name: api-v1-service
            port:
              number: 5000
      
      - path: /v2
        pathType: Prefix
        backend:
          service:
            name: api-v2-service
            port:
              number: 5000
----
]

==== Ejemplo 3: LoadBalancer Service

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: web-loadbalancer
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
spec:
  type: LoadBalancer
  selector:
    app: web
  
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 8080
  
  - name: https
    protocol: TCP
    port: 443
    targetPort: 8443
  
  # Restricción de acceso
  loadBalancerSourceRanges:
  - 203.0.113.0/24
  - 198.51.100.0/24
  
  # Session persistence
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
  
  # External traffic policy
  externalTrafficPolicy: Local  # Evita hop extra
----
]

---

=== 4.7 Resumen del Módulo 4

En este módulo aprendiste:

1. **Servicios Basics**: Concepto, labels, endpoints
2. **Tipos de Servicios:**
   - ClusterIP: Acceso interno
   - NodePort: Acceso externo limitado
   - LoadBalancer: Acceso externo robusto
   - ExternalName: Mapeo DNS
3. **Discovery y DNS**: Nombres internos, resolución, headless services
4. **Ingress**: Enrutamiento HTTP/HTTPS, múltiples servicios, TLS
5. **Network Policies**: Control de tráfico entre Pods, seguridad

Con estos conocimientos, estás listo para aprender sobre **Configuración y Secretos** en el Módulo 5.

---

== MÓDULO 5: Configuración y Secretos

Los **ConfigMaps** y **Secrets** son objetos de Kubernetes que almacenan datos de configuración. La diferencia principal:
- **ConfigMaps**: Para datos no sensibles (configuración, variables, archivos)
- **Secrets**: Para datos sensibles (contraseñas, tokens, certificados)

**Principio de 12 Factor Apps:**
Separar configuración de código es una best practice. ConfigMaps y Secrets hacen precisamente esto.

---

=== 5.1 ConfigMaps

==== 5.1.1 ¿Qué es un ConfigMap?

Un **ConfigMap** es un objeto Kubernetes que almacena datos de configuración como pares clave-valor.

**Características:**
- Datos no encriptados
- Máximo 1 MB por ConfigMap
- No debe contener secrets
- Se puede actualizar sin recrear Pods
- Accesible a través de variables de entorno o volúmenes

**Analogía:**
Un ConfigMap es como un archivo de configuración (config.ini, application.properties) que vive en Kubernetes en lugar de en el Dockerfile.

**Tipos de datos:**
```
- Valores simples: "production", "5000"
- Archivos: archivos.conf, nginx.conf
- JSON/YAML: Configuraciones estructuradas
```

==== 5.1.2 Crear ConfigMaps

===== Método 1: Desde Valores Literales

[source,bash]
----
# Crear ConfigMap simple
kubectl create configmap app-config \
  --from-literal=app.env=production \
  --from-literal=app.port=5000 \
  --from-literal=app.debug=false

# Verificar
kubectl get configmap app-config -o yaml

# Salida:
# apiVersion: v1
# kind: ConfigMap
# metadata:
#   name: app-config
# data:
#   app.env: production
#   app.port: "5000"
#   app.debug: "false"
----
]

===== Método 2: Desde Archivos

[source,bash]
----
# Crear archivos de configuración
cat > app.conf << EOF
server {
  listen 80;
  location / {
    proxy_pass http://backend:5000;
  }
}
EOF

cat > database.ini << EOF
[postgres]
host=postgres.default.svc.cluster.local
port=5432
user=admin
EOF

# Crear ConfigMap desde archivos
kubectl create configmap app-files \
  --from-file=app.conf \
  --from-file=database.ini

# Ver contenido
kubectl get configmap app-files -o yaml

# Salida:
# data:
#   app.conf: |
#     server {
#       listen 80;
#       ...
#     }
#   database.ini: |
#     [postgres]
#     ...
----
]

===== Método 3: Desde Directorio

[source,bash]
----
# Crear directorio con archivos
mkdir config-files
cat > config-files/app.conf << EOF
app_name=MyApp
version=1.0
EOF
cat > config-files/logging.conf << EOF
log_level=INFO
log_file=/var/log/app.log
EOF

# Crear ConfigMap desde directorio
kubectl create configmap app-dir-config \
  --from-file=config-files/

# Verifica: Cada archivo es una key
kubectl get configmap app-dir-config -o yaml
----
]

===== Método 4: Declarativo (YAML)

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: production
  labels:
    app: myapp
    environment: prod
data:
  # Valores simples
  APP_ENV: production
  APP_PORT: "5000"
  APP_DEBUG: "false"
  LOG_LEVEL: info
  
  # Archivo de configuración
  nginx.conf: |
    server {
      listen 80;
      server_name _;
      
      location / {
        proxy_pass http://backend:5000;
        proxy_set_header Host $host;
      }
    }
  
  # JSON
  database.json: |
    {
      "host": "postgres.default.svc.cluster.local",
      "port": 5432,
      "user": "admin",
      "pool_size": 10
    }
  
  # YAML
  app.yaml: |
    server:
      port: 5000
      timeout: 30s
    database:
      url: postgres://postgres:5432/mydb
      pool_size: 10
----
]

==== 5.1.3 Usar ConfigMaps en Pods

===== Método 1: Variables de Entorno

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENV: production
  APP_PORT: "5000"
  LOG_LEVEL: info

---
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  containers:
  - name: app
    image: myapp:1.0
    
    # Opción A: Importar una clave específica
    env:
    - name: ENVIRONMENT
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: APP_ENV
    
    - name: PORT
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: APP_PORT
    
    # Opción B: Importar todas las claves (más limpio)
    envFrom:
    - configMapRef:
        name: app-config
    
    # Combinación: Importar todas + override
    envFrom:
    - configMapRef:
        name: app-config
    env:
    - name: CUSTOM_VAR
      value: custom_value  # Sobrescribe si existe
----
]

**En tu aplicación:**
[source,python]
----
import os

# Acceder a variables de entorno
environment = os.getenv('APP_ENV')  # 'production'
port = os.getenv('APP_PORT')        # '5000'
log_level = os.getenv('LOG_LEVEL')  # 'info'
----
]

===== Método 2: Volúmenes

Montar archivos del ConfigMap como volúmenes:

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  nginx.conf: |
    server {
      listen 80;
      location / {
        proxy_pass http://backend:5000;
      }
    }
  
  mime.types: |
    types {
      text/html html htm;
      text/css css;
      application/javascript js;
    }

---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  - name: nginx
    image: nginx:1.21
    
    ports:
    - containerPort: 80
    
    # Montar ConfigMap como volumen
    volumeMounts:
    - name: config
      mountPath: /etc/nginx/conf.d
      readOnly: true
    
    - name: config
      mountPath: /etc/nginx/mime.types
      subPath: mime.types  # Archivo específico
      readOnly: true
  
  # Definir volumen desde ConfigMap
  volumes:
  - name: config
    configMap:
      name: nginx-config
      # Modo de permisos (octal)
      defaultMode: 0644
      
      # Mapeo de archivos específicos
      items:
      - key: nginx.conf
        path: default.conf
      - key: mime.types
        path: mime.types
----
]

**Ventaja:** Los cambios en ConfigMap se reflejan automáticamente (con pequeño delay)

===== Método 3: Combinación

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config-complete
data:
  # Variables simples
  APP_ENV: production
  APP_PORT: "5000"
  
  # Archivo de configuración
  app.conf: |
    server:
      port: 5000
      workers: 4

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: myapp:1.0
        
        # Variables de entorno desde ConfigMap
        envFrom:
        - configMapRef:
            name: app-config-complete
        
        # Montar archivos de configuración
        volumeMounts:
        - name: config-files
          mountPath: /etc/app
          readOnly: true
      
      # Volumen desde ConfigMap
      volumes:
      - name: config-files
        configMap:
          name: app-config-complete
----
]

==== 5.1.4 Actualizar ConfigMaps

**Opción 1: Imperativa (para testing)**
[source,bash]
----
# Ver ConfigMap actual
kubectl get configmap app-config -o yaml

# Editar inline
kubectl patch configmap app-config \
  -p '{"data":{"APP_ENV":"staging"}}'

# Reemplazar completamente
kubectl create configmap app-config \
  --from-literal=APP_ENV=production \
  --dry-run=client -o yaml | kubectl apply -f -

# Editar interactivamente
kubectl edit configmap app-config
# Se abre editor, guardas y aplica
----
]

**Opción 2: Declarativa (recomendada)**
[source,bash]
----
# Editar el YAML
kubectl apply -f configmap.yaml

# Ver cambios
kubectl describe configmap app-config

# Rollout de cambios
kubectl rollout restart deployment app-deployment
# (Causa que los Pods se reinicien y carguen nuevo ConfigMap)
----
]

**Importante:** Los cambios en ConfigMap **no reinician Pods automáticamente**. Las variables de entorno se cargan al inicio del Pod, así que necesitas:
[source,bash]
----
# Opción 1: Reiniciar Deployment
kubectl rollout restart deployment app-deployment

# Opción 2: Usar volúmenes (cambios automáticos)
# Si montas ConfigMap como volumen, se actualiza sin reinicio
# (pero la aplicación debe recargar el archivo)

# Opción 3: Usar webhooks para detectar cambios
# (herramientas como Reloader)
----
]

==== 5.1.5 Casos de Uso

**Aplicación Web:**
```yaml
- Configuración de servidor (puerto, workers)
- Niveles de logging
- Timeouts y límites
```

**Microservicios:**
```yaml
- Endpoints de otros servicios
- Configuración de caché
- Feature flags
```

**Aplicación de Base de Datos:**
```yaml
- Parámetros de conexión (no contraseñas)
- Tamaño de pool
- Configuración de replicación
```

**DevOps:**
```yaml
- Archivos de configuración (nginx.conf, apache2.conf)
- Scripts de inicialización
- Archivos de estado
```

---

=== 5.2 Secrets

==== 5.2.1 ¿Qué es un Secret?

Un **Secret** es un objeto Kubernetes que almacena datos sensibles como pares clave-valor.

**Características:**
- Datos encriptados **opcionalmente**
- Base64 encoded por defecto (NO es encriptación)
- Máximo 1 MB por Secret
- Para datos sensibles (contraseñas, tokens, certificados)
- Accesible por variables de entorno o volúmenes

**⚠️ Advertencia Crítica:**
Base64 NO es encriptación. Cualquiera puede decodificarlo:
```bash
echo "cGFzc3dvcmQxMjM=" | base64 -d  # → password123
```

**Para seguridad real**, necesitas:
- Encriptación en reposo (etcd encryption)
- RBAC restrictivo
- Network policies
- Secret management externo (Vault, AWS Secrets Manager)

==== 5.2.2 Tipos de Secrets

===== Opaque (Genérico)

El tipo más común. Almacena datos arbitrarios.

[source,bash]
----
# Crear desde literal
kubectl create secret generic db-secret \
  --from-literal=username=admin \
  --from-literal=password=secret123

# Crear desde archivo
kubectl create secret generic api-key \
  --from-file=api_key.txt

# Ver (nota: está base64 encoded)
kubectl get secret db-secret -o yaml
# data:
#   password: c2VjcmV0MTIz  # base64
#   username: YWRtaW4=      # base64
----
]

===== docker-registry

Para conectarse a registros privados de Docker:

[source,bash]
----
# Crear Secret para Docker Hub privado
kubectl create secret docker-registry docker-secret \
  --docker-server=docker.io \
  --docker-username=myuser \
  --docker-password=mypassword \
  --docker-email=user@example.com

# Para un registro personalizado
kubectl create secret docker-registry private-registry \
  --docker-server=docker.mycompany.com:5000 \
  --docker-username=user \
  --docker-password=pass
----
]

**Usar en Pod:**
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-with-private-image
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      # ← Especificar imagen pull secret
      imagePullSecrets:
      - name: docker-secret
      
      containers:
      - name: app
        image: docker.io/myuser/private-app:1.0
        # Desde registro privado
----
]

===== basic-auth

Para HTTP Basic Authentication:

[source,bash]
----
# Crear Secret de basic auth
kubectl create secret generic basic-auth \
  --from-literal=username=admin \
  --from-literal=password=secretpassword
  --type=kubernetes.io/basic-auth
----
]

===== ssh-auth

Para claves SSH privadas:

[source,bash]
----
# Crear Secret de SSH
kubectl create secret generic ssh-key \
  --from-file=ssh-privatekey=~/.ssh/id_rsa \
  --from-file=ssh-publickey=~/.ssh/id_rsa.pub \
  --type=kubernetes.io/ssh-auth
----
]

===== tls

Para certificados TLS:

[source,bash]
----
# Crear Secret TLS
kubectl create secret tls my-tls \
  --cert=path/to/cert.pem \
  --key=path/to/key.pem

# Ver estructura
kubectl get secret my-tls -o yaml
# data:
#   tls.crt: LS0tLS1...  (certificado base64)
#   tls.key: LS0tLS1...  (clave privada base64)
----
]

==== 5.2.3 Crear Secrets

===== Método 1: Imperativo

[source,bash]
----
# Simple
kubectl create secret generic my-secret \
  --from-literal=key1=value1 \
  --from-literal=key2=value2

# Desde archivo
kubectl create secret generic file-secret \
  --from-file=config.json

# Desde directorio
kubectl create secret generic dir-secret \
  --from-file=./config/
----
]

===== Método 2: Declarativo (YAML)

[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
  namespace: production
type: Opaque
data:
  # Base64 encoded (echo -n 'value' | base64)
  username: YWRtaW4=         # admin
  password: cGFzc3dvcmQxMjM=  # password123
  api_key: YWJjZGVmMTIzNDU2  # abcdef123456

---
# Alternativa: stringData (se codifica automáticamente)
apiVersion: v1
kind: Secret
metadata:
  name: app-secret-v2
type: Opaque
stringData:
  # No necesita base64, más legible
  username: admin
  password: password123
  api_key: abcdef123456
----
]

==== 5.2.4 Usar Secrets en Pods

===== Método 1: Variables de Entorno

[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
stringData:
  DB_USER: postgres
  DB_PASSWORD: secret_password
  DB_HOST: postgres.default.svc.cluster.local

---
apiVersion: v1
kind: Pod
metadata:
  name: app-with-secrets
spec:
  containers:
  - name: app
    image: myapp:1.0
    
    # Opción A: Importar variable específica
    env:
    - name: DATABASE_URL
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: DB_HOST
    
    - name: DB_USER
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: DB_USER
    
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: DB_PASSWORD
    
    # Opción B: Importar todo (más limpio)
    envFrom:
    - secretRef:
        name: db-secret
----
]

===== Método 2: Volúmenes

[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: tls-secret
type: tls
data:
  tls.crt: LS0tLS1CRUdJTi...  # certificado
  tls.key: LS0tLS1CRUdJTi...  # clave privada

---
apiVersion: v1
kind: Pod
metadata:
  name: web-server
spec:
  containers:
  - name: nginx
    image: nginx:1.21
    
    volumeMounts:
    # Montar Secret como volumen
    - name: tls-certs
      mountPath: /etc/tls
      readOnly: true
  
  volumes:
  - name: tls-certs
    secret:
      secretName: tls-secret
      defaultMode: 0600  # Solo lectura
      items:
      - key: tls.crt
        path: server.crt
      - key: tls.key
        path: server.key
----
]

===== Método 3: Ejemplo Completo

[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: app-credentials
  namespace: production
stringData:
  # Base de datos
  DB_HOST: postgres.default.svc.cluster.local
  DB_PORT: "5432"
  DB_USER: app_user
  DB_PASSWORD: "complex_password_123!@#"
  
  # API
  API_KEY: "sk-1234567890abcdef"
  API_SECRET: "secret_key_xyz"
  
  # AWS
  AWS_ACCESS_KEY: "AKIAIOSFODNN7EXAMPLE"
  AWS_SECRET_KEY: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-deployment
  namespace: production
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: myapp:1.0
        
        # Cargar todas las credenciales como variables
        envFrom:
        - secretRef:
            name: app-credentials
        
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
----
]

==== 5.2.5 Seguridad de Secrets

===== Encriptación en Reposo

Por defecto, etcd NO encripta Secrets. Para habilitarlo:

[source,bash]
----
# Editar API Server config
sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml

# Agregar:
spec:
  containers:
  - command:
    - kube-apiserver
    - --encryption-provider-config=/etc/kubernetes/enc/encryption.yaml
    # ... resto de flags

# Crear archivo de configuración
cat > /etc/kubernetes/enc/encryption.yaml << EOF
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: $(head -c 32 /dev/urandom | base64)
    - identity: {}
EOF

# Reiniciar API Server
kubectl rollout restart -n kube-system deployment/kube-apiserver
----
]

===== RBAC Restrictivo

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: secret-reader
  namespace: production
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "list"]
  # Restricción: solo ciertos secrets
  resourceNames: ["app-secret", "api-key"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-secrets
  namespace: production
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: secret-reader
subjects:
- kind: ServiceAccount
  name: app-sa
  namespace: production
----
]

===== Audit Logging

[source,bash]
----
# Ver acceso a Secrets
kubectl get events --field-selector involvedObject.kind=Secret

# Ver quién leyó qué Secret
kubectl logs -n kube-system -l component=kube-apiserver \
  | grep "get.*secrets"
----
]

==== 5.2.6 Secret Management Externo

Para máxima seguridad, usa Vault o servicios en la nube:

**Con HashiCorp Vault:**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-vault
  annotations:
    vault.hashicorp.com/agent-inject: "true"
    vault.hashicorp.com/role: "app-role"
    vault.hashicorp.com/agent-inject-secret-database: "secret/data/database"
spec:
  containers:
  - name: app
    image: myapp:1.0
    # Los secrets se inyectan desde Vault automáticamente
----
]

**Con AWS Secrets Manager:**
[source,bash]
----
# Instalar AWS Secrets Manager provider
helm install secrets-provider-aws \
  https://github.com/aws/secrets-store-csi-driver-provider-aws/releases/download/v1.0.r2-gce0ba3f/aws-provider-installer.yaml

# Luego usar en Pods
----
]

---

=== 5.3 Variables de Entorno

==== 5.3.1 Pasar Variables a Pods

**Valores literales:**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: env-demo
spec:
  containers:
  - name: app
    image: myapp:1.0
    env:
    - name: APP_ENV
      value: "production"
    - name: LOG_LEVEL
      value: "info"
    - name: DATABASE_URL
      value: "postgres://localhost:5432/mydb"
----
]

==== 5.3.2 Usar Valores de Objetos

**Field References (metadatos del Pod):**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: field-ref-demo
spec:
  containers:
  - name: app
    image: myapp:1.0
    env:
    # Nombre del Pod
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    
    # Namespace del Pod
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    
    # IP del Pod
    - name: POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    
    # IP del nodo
    - name: NODE_IP
      valueFrom:
        fieldRef:
          fieldPath: status.hostIP
    
    # UID del Pod
    - name: POD_UID
      valueFrom:
        fieldRef:
          fieldPath: metadata.uid
    
    # Labels del Pod
    - name: POD_LABELS
      valueFrom:
        fieldRef:
          fieldPath: metadata.labels['app']
    
    # Anotaciones del Pod
    - name: POD_ANNOTATIONS
      valueFrom:
        fieldRef:
          fieldPath: metadata.annotations['owner']
----
]

**Resource References (límites de recursos):**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: resource-ref-demo
spec:
  containers:
  - name: app
    image: myapp:1.0
    
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "500m"
    
    env:
    # Límite de CPU
    - name: CONTAINER_CPU_LIMIT
      valueFrom:
        resourceFieldRef:
          containerName: app
          resource: limits.cpu
    
    # Request de CPU
    - name: CONTAINER_CPU_REQUEST
      valueFrom:
        resourceFieldRef:
          containerName: app
          resource: requests.cpu
    
    # Límite de memoria
    - name: CONTAINER_MEM_LIMIT
      valueFrom:
        resourceFieldRef:
          containerName: app
          resource: limits.memory
    
    # Request de memoria
    - name: CONTAINER_MEM_REQUEST
      valueFrom:
        resourceFieldRef:
          containerName: app
          resource: requests.memory
----
]

==== 5.3.3 Combinación Completa

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENV: production
  LOG_LEVEL: info
  WORKERS: "4"

---
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
stringData:
  DB_PASSWORD: secret123

---
apiVersion: v1
kind: Pod
metadata:
  name: complete-env-demo
  labels:
    app: myapp
    tier: backend
  annotations:
    owner: "team-backend"
    description: "Complete environment demo"
spec:
  containers:
  - name: app
    image: myapp:1.0
    
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "500m"
    
    # 1. Variables literales
    env:
    - name: APP_VERSION
      value: "1.0.0"
    
    # 2. Desde ConfigMap
    - name: APP_ENV
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: APP_ENV
    
    - name: LOG_LEVEL
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: LOG_LEVEL
    
    # 3. Desde Secret
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: DB_PASSWORD
    
    # 4. Desde metadatos del Pod
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    
    - name: POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    
    # 5. Desde recursos
    - name: MEM_LIMIT
      valueFrom:
        resourceFieldRef:
          containerName: app
          resource: limits.memory
    
    # 6. O importar todo de una vez
    envFrom:
    - configMapRef:
        name: app-config
----
]

---

=== 5.4 Image Pull Secrets

==== 5.4.1 Registros Privados

Si tu imagen está en un registro privado, necesitas credenciales.

**Problema:**
```
docker pull myregistry.com/myapp:1.0
# Error: unauthorized: authentication required
```

**Solución:**
Crear un Secret y usarlo en el Pod/Deployment.

==== 5.4.2 Crear Image Pull Secrets

**Para Docker Hub privado:**
[source,bash]
----
kubectl create secret docker-registry dockerhub-secret \
  --docker-server=docker.io \
  --docker-username=myuser \
  --docker-password=mypassword \
  --docker-email=user@example.com

# Verificar
kubectl get secret dockerhub-secret -o yaml
----
]

**Para registro privado en Google Cloud:**
[source,bash]
----
# Obtener credenciales
gcloud auth configure-docker gcr.io

# Crear Secret con archivo
kubectl create secret docker-registry gcr-secret \
  --docker-server=gcr.io \
  --docker-username=_json_key \
  --docker-password="$(cat ~/key.json)" \
  --docker-email=user@example.com
----
]

**Para registro privado personalizado:**
[source,bash]
----
kubectl create secret docker-registry private-registry-secret \
  --docker-server=registry.mycompany.com:5000 \
  --docker-username=reguser \
  --docker-password=regpassword \
  --docker-email=user@company.com
----
]

**Declarativo (YAML):**
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: docker-registry-secret
type: kubernetes.io/dockercfg
data:
  # Generar: cat ~/.docker/config.json | base64 -w0
  .dockercfg: eyJyZWdpc3RyeS5teWNvbXBhbnkuY29tOjUwMDAiOnsidXNlcm5hbWUiOiJ...
----
]

==== 5.4.3 Usar en Deployments

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-with-private-image
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      # ← Especificar image pull secret
      imagePullSecrets:
      - name: docker-registry-secret
      
      containers:
      - name: app
        image: registry.mycompany.com:5000/myapp:1.0
        ports:
        - containerPort: 5000
----
]

**Múltiples registros:**
[source,yaml]
----
spec:
  imagePullSecrets:
  - name: docker-registry-secret
  - name: gcr-secret
  - name: quay-secret
  
  containers:
  - name: app
    image: registry.mycompany.com:5000/myapp:1.0
  
  - name: sidecar
    image: gcr.io/myproject/sidecar:1.0
  
  - name: logging
    image: quay.io/org/logging:1.0
----
]

==== 5.4.4 Configuración por Defecto

Para evitar especificar `imagePullSecrets` en cada Deployment:

**Opción 1: Usar en ServiceAccount por defecto**
[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: default
  namespace: production
imagePullSecrets:
- name: docker-registry-secret

---
# Ahora todos los Pods en este namespace usan el secret automáticamente
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
  namespace: production
spec:
  # No necesitas imagePullSecrets aquí
  template:
    spec:
      containers:
      - name: app
        image: registry.mycompany.com:5000/myapp:1.0
----
]

**Opción 2: Configurar en todos los namespaces**
[source,bash]
----
# Script para crear secret en múltiples namespaces
for ns in production staging development; do
  kubectl create secret docker-registry docker-secret \
    --docker-server=registry.mycompany.com:5000 \
    --docker-username=user \
    --docker-password=pass \
    -n $ns
done
----
]

---

=== 5.5 Proyección de Secrets

==== 5.5.1 Proyección de Volúmenes

Proyectar Secrets y ConfigMaps en un solo volumen:

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  app.conf: |
    server {
      port = 5000
    }

---
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
stringData:
  api_key: "secret_key_123"
  db_password: "db_pass_123"

---
apiVersion: v1
kind: Pod
metadata:
  name: projected-volume-demo
spec:
  containers:
  - name: app
    image: myapp:1.0
    
    volumeMounts:
    - name: all-in-one
      mountPath: /etc/app
      readOnly: true
  
  volumes:
  - name: all-in-one
    projected:
      sources:
      # Incluir ConfigMap
      - configMap:
          name: app-config
          items:
          - key: app.conf
            path: config/app.conf
      
      # Incluir Secret
      - secret:
          name: app-secret
          items:
          - key: api_key
            path: secrets/api.key
          - key: db_password
            path: secrets/db.pass
      
      # Incluir Service Account Token
      - serviceAccountToken:
          path: token
          expirationSeconds: 3600
      
      # Incluir downward API
      - downwardAPI:
          items:
          - path: pod_name
            fieldRef:
              fieldPath: metadata.name
          - path: namespace
            fieldRef:
              fieldPath: metadata.namespace
      
      # Modo de permisos
      defaultMode: 0444
----
]

**Resultado en el Pod:**
```
/etc/app/
├── config/
│   └── app.conf
├── secrets/
│   ├── api.key
│   └── db.pass
├── token
├── pod_name
└── namespace
```

==== 5.5.2 Token Automático de Service Account

Cada Pod obtiene automáticamente un token para autenticarse con la API de Kubernetes:

[source,bash]
----
# Dentro de un Pod
cat /var/run/secrets/kubernetes.io/serviceaccount/token
# Retorna JWT token

# Usar para autenticarse
curl -H "Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)" \
  https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1/namespaces

# También disponible:
# /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
# /var/run/secrets/kubernetes.io/serviceaccount/namespace
----
]

==== 5.5.3 Casos de Uso Complejos

**Caso 1: Inicialización con secretos**
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: init-script
type: Opaque
stringData:
  init.sh: |
    #!/bin/bash
    echo "Initializing with secret"
    API_KEY=$(cat /etc/secrets/api_key)
    # Usar API_KEY

---
apiVersion: v1
kind: Pod
metadata:
  name: init-with-secret
spec:
  initContainers:
  - name: init
    image: busybox:latest
    command: ['/bin/sh']
    args: ['/etc/init/init.sh']
    volumeMounts:
    - name: init-scripts
      mountPath: /etc/init
    - name: app-secrets
      mountPath: /etc/secrets
  
  containers:
  - name: app
    image: myapp:1.0
    volumeMounts:
    - name: app-secrets
      mountPath: /etc/secrets
  
  volumes:
  - name: init-scripts
    secret:
      secretName: init-script
      defaultMode: 0755
  
  - name: app-secrets
    secret:
      secretName: app-secret
----
]

**Caso 2: Aplicación multiconfig**
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: multi-config-app
spec:
  containers:
  - name: app
    image: myapp:1.0
    volumeMounts:
    - name: configs
      mountPath: /etc/app
  
  volumes:
  - name: configs
    projected:
      sources:
      # Configuración pública
      - configMap:
          name: public-config
          items:
          - key: default.conf
            path: default.conf
          - key: logging.conf
            path: logging.conf
      
      # Configuración privada
      - secret:
          name: private-config
          items:
          - key: db.conf
            path: db.conf
          - key: ssl.cert
            path: ssl.cert
      
      # Service account para API
      - serviceAccountToken:
          path: api_token
          expirationSeconds: 3600
----
]

---

=== 5.6 Ejemplos Completos

==== Ejemplo 1: Aplicación con ConfigMap y Secret

[source,yaml]
----
# ConfigMap con configuración pública
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: production
data:
  APP_ENV: production
  APP_PORT: "5000"
  LOG_LEVEL: info
  CACHE_TTL: "3600"
  WORKERS: "4"
  
  app.conf: |
    server {
      port = 5000
      workers = 4
      timeout = 30s
    }
    
    cache {
      ttl = 3600
      size = 1gb
    }

---
# Secret con datos sensibles
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
  namespace: production
stringData:
  DATABASE_URL: postgres://admin:password@postgres.default.svc.cluster.local:5432/mydb
  API_KEY: sk-1234567890abcdef
  JWT_SECRET: your-secret-key-here
  ENCRYPTION_KEY: 32-character-encryption-key-12345

---
# Deployment usando ConfigMap y Secret
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-deployment
  namespace: production
  labels:
    app: myapp
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  
  selector:
    matchLabels:
      app: myapp
  
  template:
    metadata:
      labels:
        app: myapp
      annotations:
        prometheus.io/scrape: "true"
    
    spec:
      containers:
      - name: app
        image: myapp:1.0
        imagePullPolicy: IfNotPresent
        
        ports:
        - containerPort: 5000
          name: http
        
        # Cargar todas las variables de configuración
        envFrom:
        - configMapRef:
            name: app-config
        - secretRef:
            name: app-secret
        
        # Variables adicionales
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        
        # Montar archivos de configuración
        volumeMounts:
        - name: config-files
          mountPath: /etc/app
          readOnly: true
        - name: tmp
          mountPath: /tmp
        
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        
        livenessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 10
          periodSeconds: 10
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 5000
          initialDelaySeconds: 5
          periodSeconds: 5
      
      volumes:
      - name: config-files
        configMap:
          name: app-config
      - name: tmp
        emptyDir: {}
----
]

==== Ejemplo 2: Aplicación con Registro Privado

[source,yaml]
----
# Secret para Docker Registry
apiVersion: v1
kind: Secret
metadata:
  name: docker-registry-secret
  namespace: production
type: kubernetes.io/dockercfg
data:
  .dockercfg: eyJyZWdpc3RyeS5teWNvbXBhbnkuY29tOjUwMDAiOnsidXNlcm5hbWUiOiJkZXBsb3llciIsInBhc3N3b3JkIjoic2VjdXJlcGFzc3dvcmQiLCJlbWFpbCI6ImFkbWluQG15Y29tcGFueS5jb20ifX0=

---
# ServiceAccount con image pull secret
apiVersion: v1
kind: ServiceAccount
metadata:
  name: app-sa
  namespace: production
imagePullSecrets:
- name: docker-registry-secret

---
# Deployment usando registro privado
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-private-image
  namespace: production
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      # Usar service account con credenciales
      serviceAccountName: app-sa
      
      containers:
      - name: app
        # Imagen desde registro privado
        image: registry.mycompany.com:5000/myapp:1.0
        imagePullPolicy: Always  # Siempre descargar
        
        ports:
        - containerPort: 5000
----
]

==== Ejemplo 3: Aplicación con Certificados y Secretos

[source,yaml]
----
# Generar certificado
# openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 365 -nodes

apiVersion: v1
kind: Secret
metadata:
  name: app-tls
  namespace: production
type: tls
data:
  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0t...  # cat cert.pem | base64 -w0
  tls.key: LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0t...  # cat key.pem | base64 -w0

---
# Configuração para HTTPS
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-https-config
  namespace: production
data:
  nginx.conf: |
    server {
      listen 443 ssl;
      server_name _;
      
      ssl_certificate /etc/tls/tls.crt;
      ssl_certificate_key /etc/tls/tls.key;
      
      location / {
        proxy_pass http://app:5000;
      }
    }

---
# Pod com HTTPS
apiVersion: v1
kind: Pod
metadata:
  name: secure-app
  namespace: production
spec:
  containers:
  - name: app
    image: myapp:1.0
    ports:
    - containerPort: 5000
    - containerPort: 443
  
  - name: nginx-proxy
    image: nginx:1.21
    ports:
    - containerPort: 443
    
    volumeMounts:
    - name: tls-certs
      mountPath: /etc/tls
      readOnly: true
    - name: nginx-config
      mountPath: /etc/nginx/conf.d
      readOnly: true
  
  volumes:
  - name: tls-certs
    secret:
      secretName: app-tls
      defaultMode: 0600
  - name: nginx-config
    configMap:
      name: app-https-config
----
]

---

=== 5.7 Best Practices

**1. Usar Secrets para datos sensibles, ConfigMaps para no sensibles**
```yaml
✅ Secret: contraseñas, tokens, certificados
✅ ConfigMap: configuración, archivos, variables
```

**2. No encriptación = no seguridad real**
```bash
❌ Base64 no es encriptación
✅ Habilitar encryption-at-rest en etcd
✅ Usar RBAC restrictivo
✅ Considerar Vault o servicio en la nube
```

**3. Usar volúmenes para archivos, variables de entorno para valores**
```yaml
✅ Volúmenes: nginx.conf, app.conf
✅ Variables: puerto, timeout, flags
```

**4. No guardar Secrets en Git**
```bash
✅ Git: archivos de Deployment
❌ Git: valores en Secrets
✅ Usar: sealed-secrets, kyverno, policies
```

**5. Rotar Secrets regularmente**
```bash
# Cambiar secretos sin downtime
kubectl patch secret app-secret \
  -p '{"data":{"password":"new_encoded_password"}}'

# Luego reiniciar Deployments
kubectl rollout restart deployment app-deployment
```

---

=== 5.8 Resumen del Módulo 5

En este módulo aprendiste:

1. **ConfigMaps**: Almacenamiento de configuración no sensible
   - Crear desde valores, archivos, directorios
   - Usar como variables de entorno o volúmenes
   - Actualizar sin downtime

2. **Secrets**: Almacenamiento seguro de datos sensibles
   - Tipos: Opaque, docker-registry, basic-auth, ssh-auth, tls
   - Encriptación (en reposo y en tránsito)
   - RBAC y audit logging

3. **Variables de Entorno**: Múltiples fuentes
   - Literales, ConfigMap, Secret
   - Field references (metadatos del Pod)
   - Resource references (límites)

4. **Image Pull Secrets**: Registros privados
   - Docker Hub privado
   - Registros personalizados
   - Configuración por defecto

5. **Proyección de Secrets**: Casos avanzados
   - Múltiples fuentes en un volumen
   - Service Account tokens
   - Inicialización compleja

Con estos conocimientos, estás listo para aprender sobre **Storage** en el Módulo 6.

---

---

== MÓDULO 6: Storage

El **almacenamiento persistente** es crítico en Kubernetes. A diferencia de los volúmenes ephemeral (que se pierden cuando el Pod termina), necesitamos datos que persistan.

**Problema:**
```
Pod A escribe archivo.txt → Pod termina → archivo.txt desaparece
```

**Solución:**
```
Pod A escribe a PersistentVolume → Pod termina → datos permanecen
Pod B lee datos de PersistentVolume
```

---

=== 6.1 Persistent Volumes (PV)

==== 6.1.1 ¿Qué es un Persistent Volume?

Un **Persistent Volume (PV)** es un recurso de almacenamiento en el cluster que existe independientemente de los Pods.

**Características:**
- Ciclo de vida separado del Pod
- Existe en el cluster como objeto
- Accesible por múltiples Pods
- Respaldado por almacenamiento real (disco, nube, NFS, etc)
- Admin crea PVs, usuarios crean PVCs

**Analogía:**
- Pod = Proceso (temporal)
- PersistentVolume = Disco duro (permanente)
- PersistentVolumeClaim = Solicitud para usar el disco

**Diagrama de ciclo de vida:**
```
┌─────────────────────────────────────────────────────────────┐
│  1. ADMIN: Crear PV (recursos físicos)                     │
│     ↓                                                        │
│  2. USUARIO: Crear PVC (solicitud)                         │
│     ↓                                                        │
│  3. BINDING: Kubernetes vincula PVC a PV                   │
│     ↓                                                        │
│  4. POD: Usar PVC en Pod (volumen montado)                 │
│     ↓                                                        │
│  5. LIBERACIÓN: Pod termina, PVC liberado                  │
│     ↓                                                        │
│  6. RECLAMACIÓN: Política decide qué hacer con datos       │
└─────────────────────────────────────────────────────────────┘
```

==== 6.1.2 Tipos de Storage Backends

===== Local (Nodo)

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv
spec:
  capacity:
    storage: 10Gi  # Capacidad
  accessModes:
    - ReadWriteOnce  # Un nodo puede escribir
  persistentVolumeReclaimPolicy: Delete
  
  # Storage local en el nodo
  local:
    path: /mnt/data  # Ruta física en el nodo
  
  # Especificar qué nodo (obligatorio para local)
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker-node-1
----
]

**Ventajas:** Muy rápido (disco directo)
**Desventajas:** No portátil entre nodos, se pierde si nodo falla

===== NFS (Network File System)

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 50Gi
  accessModes:
    - ReadWriteMany  # Múltiples nodos pueden acceder
    - ReadOnlyMany
  persistentVolumeReclaimPolicy: Retain
  
  # Storage NFS
  nfs:
    server: nfs-server.example.com  # Servidor NFS
    path: "/data/kubernetes"        # Ruta en servidor NFS
----
]

**Ventajas:** Compartible entre nodos, fácil de usar
**Desventajas:** Requiere servidor NFS externo, rendimiento variable

===== iSCSI

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: iscsi-pv
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  
  # Storage iSCSI (block storage)
  iscsi:
    targetPortal: 192.168.1.100:3260
    iqn: iqn.2020-01.com.example:storage.disk1.sys1.xyz
    lun: 0
----
]

**Ventajas:** Block storage, buena performance
**Desventajas:** Requiere infraestructura iSCSI compleja

===== AWS EBS

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: ebs-pv
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  awsElasticBlockStore:
    volumeID: vol-123abc456def78901  # ID del volumen EBS
    fsType: ext4

---
# En AWS EKS, generalmente usan Storage Classes en lugar de PVs manuales
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ebs-gp3
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  size: 20
  iops: 3000
  throughput: 125
----
]

===== GCP GCE Persistent Disk

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: gce-pv
spec:
  capacity:
    storage: 50Gi
  accessModes:
    - ReadWriteOnce
  gcePersistentDisk:
    pdName: gce-disk-1       # Nombre del disk en GCP
    fsType: ext4
----
]

===== Azure Disk

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: azure-pv
spec:
  capacity:
    storage: 30Gi
  accessModes:
    - ReadWriteOnce
  azureDisk:
    kind: Managed
    diskName: azure-disk-1
    diskURI: /subscriptions/.../resourceGroups/.../providers/Microsoft.Compute/disks/azure-disk-1
    fsType: ext4
----
]

==== 6.1.3 Crear Persistent Volumes

**Método 1: Desde almacenamiento local**

[source,bash]
----
# En el nodo worker, crear directorio
sudo mkdir -p /mnt/data
sudo chmod 777 /mnt/data
echo "persistentvolume-test" | sudo tee /mnt/data/test.txt

# Crear PV que apunta a ese directorio
cat > local-pv.yaml << EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv-1
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  local:
    path: /mnt/data
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker-node-1
EOF

kubectl apply -f local-pv.yaml

# Verificar
kubectl get pv local-pv-1
# NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM
# local-pv-1    5Gi        RWO            Delete           Available
----
]

**Método 2: Desde NFS externo**

[source,bash]
----
# Asumiendo que hay servidor NFS en nfs.example.com:/data

cat > nfs-pv.yaml << EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv-1
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: nfs.example.com
    path: "/data"
EOF

kubectl apply -f nfs-pv.yaml

# Verificar
kubectl get pv nfs-pv-1
----
]

**Método 3: Usando Storage Class (dinámico)**

Para la mayoría de casos modernos, usas Storage Classes en lugar de crear PVs manualmente.

==== 6.1.4 Políticas de Reclamación

Define qué pasa cuando un PVC es liberado:

===== Delete (predeterminado)

```yaml
persistentVolumeReclaimPolicy: Delete
```

El volumen es eliminado automáticamente cuando PVC es liberado.

[source,bash]
----
# 1. Crear y usar PVC
kubectl apply -f pvc.yaml

# 2. Pod usa el PVC

# 3. Eliminar PVC
kubectl delete pvc my-pvc

# → Automáticamente se elimina el PV
# → Los datos se pierden
----
]

===== Retain

```yaml
persistentVolumeReclaimPolicy: Retain
```

El volumen se mantiene, pero no puede ser reclamado automáticamente.

[source,bash]
----
# Después de eliminar PVC:
# - PV existe pero está "Released"
# - No puede ser usado por otro PVC automáticamente
# - Datos se conservan

# Para reutilizar:
kubectl patch pv pv-name -p '{"spec":{"claimRef": null}}'
# Ahora puede ser reclamado por otro PVC
----
]

===== Recycle (deprecado, usar Delete o Retain)

```yaml
persistentVolumeReclaimPolicy: Recycle
```

Los datos se borran pero el volumen se reutiliza.

==== 6.1.5 Access Modes

Define cómo múltiples Pods pueden acceder al volumen:

| Modo | Símbolo | Significado |
|------|---------|-------------|
| ReadWriteOnce | RWO | Un nodo puede leer y escribir |
| ReadOnlyMany | ROX | Múltiples nodos pueden solo leer |
| ReadWriteMany | RWX | Múltiples nodos pueden leer y escribir |
| ReadWriteOncePod | RWOP | Un Pod puede leer y escribir |

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: multi-node-pv
spec:
  capacity:
    storage: 50Gi
  
  # Múltiples nodos pueden leer y escribir
  accessModes:
    - ReadWriteMany
  
  nfs:
    server: nfs.example.com
    path: "/data"

---
# Este PVC solo puede ser montado en un nodo
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: single-node-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

---
# Este PVC puede ser compartido entre múltiples nodos
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Gi
----
]

---

=== 6.2 PersistentVolumeClaims (PVC)

==== 6.2.1 ¿Qué es un PersistentVolumeClaim?

Un **PersistentVolumeClaim (PVC)** es una solicitud de almacenamiento hecha por un usuario.

**Relación PV ↔ PVC:**
```
PersistentVolume (PV)     ← Administrador proporciona
        ↑
        │ vinculado (bound)
        ↓
PersistentVolumeClaim (PVC) ← Usuario solicita
        ↑
        │ usado en
        ↓
Pod/Deployment
```

**Características:**
- Usuario solicita sin conocer detalles físicos
- Kubernetes busca PV que coincida
- Si no hay PV, espera (o crea dinámicamente con Storage Class)
- Multiple Pods pueden usar mismo PVC (si access mode lo permite)

==== 6.2.2 Crear PersistentVolumeClaims

[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-claim
  namespace: default
spec:
  # Access mode debe ser soportado por PV
  accessModes:
    - ReadWriteOnce
  
  # Storage class (vacío = storage class por defecto)
  # storageClassName: standard
  
  # Capacidad solicitada
  resources:
    requests:
      storage: 10Gi
----
]

**Verificar vinculación:**

[source,bash]
----
# Ver PVCs
kubectl get pvc
# NAME       STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS   AGE
# my-claim   Bound    local-pv-1    5Gi        RWO            -              5m

# Ver detalles
kubectl describe pvc my-claim
# Name:          my-claim
# Namespace:     default
# Status:        Bound
# Volume:        local-pv-1
# Labels:        <none>
# Annotations:   <none>
# Capacity:      5Gi
# Access Modes:  RWO
----
]

==== 6.2.3 Usar PVC en Pods

**Método Simple:**

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: app-with-storage
spec:
  containers:
  - name: app
    image: nginx:1.21
    
    # Montar el volumen
    volumeMounts:
    - name: storage
      mountPath: /data
  
  # Referencia al PVC
  volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: my-claim
----
]

**Uso en Deployment:**

[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: app-storage
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-with-pvc
spec:
  replicas: 1  # Solo 1 con ReadWriteOnce
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: myapp:1.0
        
        volumeMounts:
        - name: app-data
          mountPath: /data
        
        # Escribir datos
        command: ["/bin/sh"]
        args:
          - -c
          - |
            while true; do
              echo "Data written at $(date)" >> /data/output.log
              sleep 10
            done
      
      volumes:
      - name: app-data
        persistentVolumeClaim:
          claimName: app-storage
----
]

==== 6.2.4 Estados de PVC

| Estado | Significado |
|--------|-------------|
| Pending | Esperando PV disponible |
| Bound | Vinculado a un PV |
| Lost | El PV fue eliminado |

[source,bash]
----
# PVC en estado Pending (no hay PV disponible)
kubectl get pvc
# NAME           STATUS    VOLUME   CAPACITY   AGE
# waiting-pvc    Pending            -          2m

# Ver por qué está pendiente
kubectl describe pvc waiting-pvc
# Events:
#   Type    Reason         Message
#   Normal  FailedBinding  no persistent volumes available
----
]

---

=== 6.3 Storage Classes

==== 6.3.1 ¿Qué es un Storage Class?

Un **Storage Class** define cómo el sistema **provisiona automáticamente** volúmenes.

**Flujo sin Storage Class:**
```
1. Admin crea PV manualmente → 2. Usuario crea PVC → 3. Vinculación
(lento, manual, limitado)
```

**Flujo con Storage Class:**
```
1. Usuario crea PVC → 2. Storage Class provisiona PV automáticamente
(rápido, dinámico, escalable)
```

**Características:**
- Provisioning dinámico
- Parámetros personalizables por backend
- Automático para cada PVC
- Escalable y cloud-native

==== 6.3.2 Storage Classes en Diferentes Clouds

===== AWS EKS

[source,yaml]
----
# Storage Class para AWS EBS
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ebs-gp3
provisioner: ebs.csi.aws.com
parameters:
  # Tipo de volumen
  type: gp3
  
  # Tamaño
  size: "20"
  
  # IOPS (entrada/salida por segundo)
  iops: "3000"
  
  # Throughput (MB/s)
  throughput: "125"
  
  # Encriptación
  encrypted: "true"
  
  # Zona de disponibilidad
  # availabilityZone: us-east-1a

volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Delete

---
# Usar en PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: aws-data
spec:
  storageClassName: ebs-gp3
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
----
]

===== Google GKE

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gce-pd-ssd
provisioner: pd.csi.storage.gke.io
parameters:
  # Tipo de disco
  type: pd-ssd
  
  # Zona
  # replication-type: regional-pd  # Para replicación multi-zona

volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Delete

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gcp-data
spec:
  storageClassName: gce-pd-ssd
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
----
]

===== Azure AKS

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: azure-disk-premium
provisioner: disk.csi.azure.com
parameters:
  # SKU del disco
  skuName: Premium_LRS
  
  # LocationConstraint: Para zonas específicas
  # cachingmode: ReadWrite
  
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Delete

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: azure-data
spec:
  storageClassName: azure-disk-premium
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 30Gi
----
]

===== NFS Local (Minikube/On-Premises)

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-provisioner
provisioner: nfs-provisioner/nfs
parameters:
  # Servidor NFS
  server: nfs.example.com
  # Ruta en servidor
  path: "/data/kubernetes"
  
allowVolumeExpansion: true
reclaimPolicy: Retain
volumeBindingMode: Immediate

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-data
spec:
  storageClassName: nfs-provisioner
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Gi
----
]

==== 6.3.3 Storage Class Predeterminado

Cada cluster tiene un Storage Class por defecto:

[source,bash]
----
# Ver Storage Classes
kubectl get storageclass
# NAME                    PROVISIONER             RECLAIM POLICY   VOLUME BINDING MODE
# standard (default)      kubernetes.io/aws-ebs   Delete           Immediate
# fast                    kubernetes.io/aws-ebs   Delete           Immediate

# Ver cuál es por defecto
kubectl get storageclass standard -o yaml
# provisioner: kubernetes.io/aws-ebs
# metadata:
#   annotations:
#     storageclass.kubernetes.io/is-default-class: "true"

# Crear PVC sin especificar storageClassName usa el por defecto
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: default-storage
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  # Si no especificas storageClassName, usa el por defecto
----
]

==== 6.3.4 Parámetros Avanzados

===== Volume Binding Mode

**Immediate:** Vincula PV inmediatamente
[source,yaml]
----
volumeBindingMode: Immediate
# PVC se vincula tan pronto como se crea
# Puede no coincidir con nodo que usa el PVC
----
]

**WaitForFirstConsumer:** Espera hasta que Pod use el PVC
[source,yaml]
----
volumeBindingMode: WaitForFirstConsumer
# PVC espera hasta que un Pod la use
# Garantiza que PV está en la misma zona que el Pod
# Recomendado para alta disponibilidad
----
]

===== Expansion

[source,yaml]
----
allowVolumeExpansion: true

# Luego puedes expandir:
kubectl patch pvc my-pvc -p \
  '{"spec":{"resources":{"requests":{"storage":"50Gi"}}}}'

# O editar:
kubectl edit pvc my-pvc
# Cambiar "storage: 20Gi" a "storage: 50Gi"
----
]

===== Reclaim Policy

[source,yaml]
----
reclaimPolicy: Delete        # Eliminar datos
reclaimPolicy: Retain        # Mantener datos
reclaimPolicy: Recycle       # Limpiar y reutilizar (deprecado)
----
]

---

=== 6.4 StatefulSets y Storage

==== 6.4.1 ¿Por qué StatefulSet + Storage?

**Problema con Deployment:**
```
Pod-1 usa PVC-A
Pod-1 reinicia → nuevas PVC se crean
→ cada Pod tiene diferentes datos
```

**Solución con StatefulSet:**
```
Pod-0 usa PVC-0 (siempre)
Pod-1 usa PVC-1 (siempre)
Pod-2 usa PVC-2 (siempre)
→ Identidad persistente + Storage persistente
```

==== 6.4.2 volumeClaimTemplates

Define plantillas de PVC para cada Pod:

[source,yaml]
----
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-cluster
spec:
  serviceName: mysql
  replicas: 3
  
  selector:
    matchLabels:
      app: mysql
  
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        
        ports:
        - containerPort: 3306
        
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        
        # Montar volumen
        volumeMounts:
        - name: mysql-storage
          mountPath: /var/lib/mysql
        
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi
  
  # ← Aquí está lo importante: volumeClaimTemplates
  volumeClaimTemplates:
  - metadata:
      name: mysql-storage
    spec:
      accessModes:
        - ReadWriteOnce
      storageClassName: gp3-ssd
      resources:
        requests:
          storage: 100Gi
----
]

**Qué sucede:**
```
StatefulSet crea 3 Pods:
├── mysql-cluster-0 → PVC: mysql-cluster-0-mysql-storage → PV
├── mysql-cluster-1 → PVC: mysql-cluster-1-mysql-storage → PV
└── mysql-cluster-2 → PVC: mysql-cluster-2-mysql-storage → PV

Si mysql-cluster-0 reinicia:
├── Pod termina
├── Pod se recrea
├── Usa misma PVC: mysql-cluster-0-mysql-storage
├── PVC vincula al mismo PV
├── Datos recuperados ✓
```

[source,bash]
----
# Ver PVCs creadas automáticamente
kubectl get pvc
# NAME                              STATUS   VOLUME   CAPACITY
# mysql-cluster-0-mysql-storage     Bound    pv-1     100Gi
# mysql-cluster-1-mysql-storage     Bound    pv-2     100Gi
# mysql-cluster-2-mysql-storage     Bound    pv-3     100Gi

# Ver Pods con almacenamiento persistente
kubectl get pod
# NAME                READY   STATUS
# mysql-cluster-0     1/1     Running
# mysql-cluster-1     1/1     Running
# mysql-cluster-2     1/1     Running

# Eliminar un Pod (se recrea pero usa misma PVC)
kubectl delete pod mysql-cluster-0

# Ver que se recrea con misma identidad
kubectl get pod
# mysql-cluster-0 reiniciado (0/1 → 1/1)
----
]

==== 6.4.3 PostgreSQL con StatefulSet

Ejemplo completo: Base de datos PostgreSQL con almacenamiento:

[source,yaml]
----
# Secret para credenciales
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
stringData:
  POSTGRES_PASSWORD: "super_secure_password_123"
  POSTGRES_USER: postgres
  POSTGRES_DB: appdb

---
# ConfigMap con configuración
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-config
data:
  postgresql.conf: |
    listen_addresses = '*'
    max_connections = 200
    shared_buffers = 256MB
    effective_cache_size = 1GB
    work_mem = 4MB
    maintenance_work_mem = 64MB

---
# Service headless (importante para StatefulSet)
apiVersion: v1
kind: Service
metadata:
  name: postgres
spec:
  clusterIP: None  # ← Headless
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432

---
# StatefulSet de PostgreSQL
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres
  replicas: 1  # Una instancia principal (más réplicas con streaming replication)
  
  selector:
    matchLabels:
      app: postgres
  
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:15
        
        ports:
        - containerPort: 5432
          name: postgres
        
        # Variables de entorno desde Secret
        envFrom:
        - secretRef:
            name: postgres-secret
        
        # Montar volumen de datos
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
          subPath: postgres  # PostgreSQL necesita subPath
        
        - name: config
          mountPath: /etc/postgresql/postgresql.conf
          subPath: postgresql.conf
        
        # Health checks
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U postgres
          initialDelaySeconds: 30
          periodSeconds: 10
        
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U postgres
          initialDelaySeconds: 5
          periodSeconds: 5
        
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi
      
      volumes:
      - name: config
        configMap:
          name: postgres-config
  
  # PVC automáticas para cada Pod
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes:
        - ReadWriteOnce
      storageClassName: gp3-ssd
      resources:
        requests:
          storage: 50Gi
----
]

**Usar la base de datos:**

[source,bash]
----
# Conectarse a PostgreSQL desde dentro del cluster
kubectl run psql-client --image=postgres:15 -it --rm \
  -- psql -h postgres.default.svc.cluster.local -U postgres -d appdb

# Crear tabla
CREATE TABLE users (
  id SERIAL PRIMARY KEY,
  name VARCHAR(100),
  email VARCHAR(100)
);

# Insertar datos
INSERT INTO users (name, email) VALUES ('Alice', 'alice@example.com');

# Verificar persistencia
# Si eliminas el Pod, reinicia y los datos permanecen
----
]

==== 6.4.4 Escalado de StatefulSets

**Aumentar replicas:**

[source,bash]
----
# Aumentar de 1 a 3 Pods con almacenamiento
kubectl patch statefulset postgres -p '{"spec":{"replicas":3}}'

# Ver progreso
kubectl rollout status statefulset postgres

# Verificar PVCs (se crean automáticamente)
kubectl get pvc
# postgres-0-postgres-storage
# postgres-1-postgres-storage
# postgres-2-postgres-storage
----
]

**Reducir replicas:**

[source,bash]
----
# Reducir de 3 a 1 (elimina Pods en orden inverso)
kubectl scale statefulset postgres --replicas=1

# Importante: PVCs NO se eliminan automáticamente
# Puedes reutilizarlas o eliminarlas manualmente
kubectl delete pvc postgres-1-postgres-storage
kubectl delete pvc postgres-2-postgres-storage
----
]

---

=== 6.5 Snapshots y Backups

==== 6.5.1 VolumeSnapshots

Un **VolumeSnapshot** es una copia de un volumen en un punto en el tiempo.

**Requisitos:**
- VolumeSnapshotClass definida
- Volume Snapshot Controller instalado

[source,bash]
----
# Instalar Volume Snapshot CRDs
kubectl apply -f https://github.com/kubernetes-csi/external-snapshotter/raw/master/client/config/crd/

# Verificar
kubectl get crd | grep snapshot
# volumesnapshotclasses.snapshot.storage.k8s.io
# volumesnapshots.snapshot.storage.k8s.io
----
]

==== 6.5.2 Crear Snapshots

**VolumeSnapshotClass (AWS EBS):**

[source,yaml]
----
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: ebs-snapshots
driver: ebs.csi.aws.com
deletionPolicy: Delete  # Delete = eliminar snapshot real también
----
]

**Crear Snapshot:**

[source,yaml]
----
# PVC existente
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: app-data
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: ebs-gp3
  resources:
    requests:
      storage: 20Gi

---
# Snapshot del PVC
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: app-data-snapshot-v1
  namespace: production
spec:
  volumeSnapshotClassName: ebs-snapshots
  source:
    persistentVolumeClaimName: app-data
----
]

[source,bash]
----
# Crear snapshot
kubectl apply -f snapshot.yaml

# Ver snapshots
kubectl get volumesnapshot
# NAME                         READYTOUSE   SOURCEPVC    SOURCESIZE   SNAPSHOTCLASS   AGE
# app-data-snapshot-v1         true         app-data     20Gi         ebs-snapshots   5m

# Ver detalles
kubectl describe volumesnapshot app-data-snapshot-v1
----
]

==== 6.5.3 Restaurar desde Snapshot

**Crear PVC desde Snapshot:**

[source,yaml]
----
# Restaurar datos de snapshot anterior
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: app-data-restored
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: ebs-gp3
  resources:
    requests:
      storage: 20Gi
  
  # ← Restaurar desde snapshot
  dataSource:
    name: app-data-snapshot-v1
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
----
]

[source,bash]
----
# Crear PVC restaurada
kubectl apply -f pvc-restored.yaml

# Verificar que se crea desde snapshot
kubectl get pvc app-data-restored -o yaml
# dataSource:
#   apiGroup: snapshot.storage.k8s.io
#   kind: VolumeSnapshot
#   name: app-data-snapshot-v1

# Montar en un Pod de prueba
kubectl run restore-test --image=busybox -it --rm \
  -- sh -c "mount /dev/xvdf /data && cat /data/important-file.txt"
----
]

==== 6.5.4 Estrategias de Backup

===== Opción 1: Snapshots Periódicos

[source,yaml]
----
# CronJob que crea snapshots cada 6 horas
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-snapshots
spec:
  schedule: "0 */6 * * *"  # Cada 6 horas
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: backup-sa
          containers:
          - name: snapshot-creator
            image: bitnami/kubectl:latest
            command:
            - /bin/sh
            - -c
            - |
              TIMESTAMP=$(date +%Y%m%d-%H%M%S)
              cat <<EOF | kubectl apply -f -
              apiVersion: snapshot.storage.k8s.io/v1
              kind: VolumeSnapshot
              metadata:
                name: app-data-snapshot-$TIMESTAMP
              spec:
                volumeSnapshotClassName: ebs-snapshots
                source:
                  persistentVolumeClaimName: app-data
              EOF
          restartPolicy: OnFailure

---
# RBAC para CronJob
apiVersion: v1
kind: ServiceAccount
metadata:
  name: backup-sa

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: snapshot-creator
rules:
- apiGroups: ["snapshot.storage.k8s.io"]
  resources: ["volumesnapshots"]
  verbs: ["create", "get", "list"]
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: snapshot-creator-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: snapshot-creator
subjects:
- kind: ServiceAccount
  name: backup-sa
----
]

===== Opción 2: Backup Externo con Velero

Para backups completos (Pods, configuración, datos):

[source,bash]
----
# Instalar Velero
wget https://github.com/vmware-tanzu/velero/releases/download/v1.13.0/velero-v1.13.0-linux-amd64.tar.gz
tar -xvf velero-v1.13.0-linux-amd64.tar.gz
sudo mv velero-v1.13.0-linux-amd64/velero /usr/local/bin/

# Configurar para AWS
velero install \
  --provider aws \
  --bucket velero-backups \
  --secret-file ./credentials-velero

# Crear backup
velero backup create prod-backup-20240101

# Ver backups
velero backup get

# Restaurar desde backup
velero restore create --from-backup prod-backup-20240101
----
]

===== Opción 3: Backup Application-Level

Algunos datos requieren backups a nivel de aplicación:

[source,yaml]
----
# CronJob para backup de base de datos
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
spec:
  schedule: "0 2 * * *"  # Cada día a las 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: postgres-backup
            image: postgres:15
            command:
            - /bin/bash
            - -c
            - |
              BACKUP_DATE=$(date +%Y%m%d_%H%M%S)
              pg_dump -h postgres -U postgres appdb | \
                gzip > /backups/postgres_${BACKUP_DATE}.sql.gz
              
              # Opcional: enviar a S3
              # aws s3 cp /backups/postgres_${BACKUP_DATE}.sql.gz \
              #   s3://my-backups/postgres/
            
            volumeMounts:
            - name: backups
              mountPath: /backups
            
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: POSTGRES_PASSWORD
          
          volumes:
          - name: backups
            persistentVolumeClaim:
              claimName: backups-pvc
          
          restartPolicy: OnFailure
----
]

==== 6.5.5 Disaster Recovery

**Caso: Datacenter falla**

```
1. Backup en múltiples regiones ✓
2. Restaurar en otra región
3. Actualizar DNS/load balancer
```

[source,bash]
----
# En región primaria
velero backup create prod-backup-full \
  --wait

# Esperar a que se sincronice a otra región
# (si usas Velero con S3 cross-region replication)

# En región secundaria
velero backup-location get
# NAME      PROVIDER   BUCKET/PREFIX     ACCESS MODE

# Restaurar
velero restore create --from-backup prod-backup-full

# Verificar que todo funciona
kubectl get pods -A
kubectl get svc -a
----
]

---

=== 6.6 Ejemplos Completos

==== Ejemplo 1: WordPress con Storage

[source,yaml]
----
# Storage Class para WordPress
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: wordpress-storage
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Delete

---
# PVC para WordPress
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wordpress-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: wordpress-storage
  resources:
    requests:
      storage: 30Gi

---
# Secret para MySQL
apiVersion: v1
kind: Secret
metadata:
  name: mysql-secret
stringData:
  MYSQL_ROOT_PASSWORD: "secure_mysql_root_pass"
  MYSQL_DATABASE: wordpress
  MYSQL_USER: wordpress
  MYSQL_PASSWORD: "secure_wordpress_pass"

---
# Deployment de WordPress
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
spec:
  replicas: 2
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
      - name: wordpress
        image: wordpress:latest
        
        ports:
        - containerPort: 80
        
        env:
        - name: WORDPRESS_DB_HOST
          value: "mysql.default.svc.cluster.local"
        - name: WORDPRESS_DB_NAME
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: MYSQL_DATABASE
        - name: WORDPRESS_DB_USER
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: MYSQL_USER
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: MYSQL_PASSWORD
        
        volumeMounts:
        - name: wordpress-data
          mountPath: /var/www/html
        
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
      
      volumes:
      - name: wordpress-data
        persistentVolumeClaim:
          claimName: wordpress-pvc

---
# Service para WordPress
apiVersion: v1
kind: Service
metadata:
  name: wordpress-service
spec:
  type: LoadBalancer
  selector:
    app: wordpress
  ports:
  - port: 80
    targetPort: 80

---
# StatefulSet de MySQL
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        
        ports:
        - containerPort: 3306
        
        envFrom:
        - secretRef:
            name: mysql-secret
        
        volumeMounts:
        - name: mysql-storage
          mountPath: /var/lib/mysql
        
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi
  
  volumeClaimTemplates:
  - metadata:
      name: mysql-storage
    spec:
      accessModes:
        - ReadWriteOnce
      storageClassName: wordpress-storage
      resources:
        requests:
          storage: 50Gi

---
# Service headless para MySQL
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  clusterIP: None
  selector:
    app: mysql
  ports:
  - port: 3306
    targetPort: 3306
----
]

==== Ejemplo 2: Backup y Restauración

[source,yaml]
----
# Crear snapshot
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: wordpress-backup-weekly
spec:
  volumeSnapshotClassName: ebs-snapshots
  source:
    persistentVolumeClaimName: wordpress-pvc

---
# Semana siguiente, restaurar si algo falla
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wordpress-pvc-restored
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: wordpress-storage
  resources:
    requests:
      storage: 30Gi
  dataSource:
    name: wordpress-backup-weekly
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io

---
# Pod con datos restaurados
apiVersion: v1
kind: Pod
metadata:
  name: wordpress-recovery
spec:
  containers:
  - name: restore
    image: busybox:latest
    command: ["/bin/sh"]
    args: ["-c", "ls -la /recovery && sleep infinity"]
    volumeMounts:
    - name: recovery-data
      mountPath: /recovery
  
  volumes:
  - name: recovery-data
    persistentVolumeClaim:
      claimName: wordpress-pvc-restored
----
]

---

=== 6.7 Best Practices

**1. Usar Storage Classes para provisioning dinámico**
```yaml
❌ Crear PVs manualmente
✅ Storage Classes + PVCs
```

**2. Especificar volumeBindingMode: WaitForFirstConsumer**
```yaml
✅ Garantiza PV en la misma zona que el Pod
✅ Evita issues de latencia
```

**3. Backups periódicos**
```bash
✅ VolumeSnapshots cada 6 horas
✅ Backup application-level (bases de datos)
✅ Disaster recovery plan
```

**4. StatefulSets para datos persistentes**
```yaml
✅ StatefulSet con volumeClaimTemplates
✅ Identidad y almacenamiento vinculados
```

**5. Monitoreo de storage**
```bash
✅ Alertas cuando PVC usa 80%+ capacidad
✅ Tendencias de crecimiento
✅ Rotación de backups
```

---

=== 6.8 Resumen del Módulo 6

En este módulo aprendiste:

1. **Persistent Volumes**: Almacenamiento cluster-wide
   - Tipos: local, NFS, iSCSI, cloud
   - Access modes: RWO, ROX, RWX, RWOP
   - Reclaim policies: Delete, Retain, Recycle

2. **PersistentVolumeClaims**: Solicitudes de almacenamiento
   - Vinculación automática con PVs
   - Estados: Pending, Bound, Lost
   - Uso en Pods/Deployments

3. **Storage Classes**: Provisioning dinámico
   - AWS EBS, GCP PD, Azure Disk
   - Parameters y selectors
   - Expansion y binding modes

4. **StatefulSets + Storage**: Identidad persistente
   - volumeClaimTemplates
   - PostgreSQL, MySQL examples
   - Escalado seguro

5. **Snapshots y Backups**: Protección de datos
   - VolumeSnapshots
   - Restauración
   - Estrategias: periódicas, externas, application-level

Con estos conocimientos, estás listo para aprender sobre **Escalado** en el Módulo 7.

---

== MÓDULO 7: Scaling y Autoscaling

El **escalado** es fundamental para aplicaciones en Kubernetes. Hay dos dimensiones:
- **Escalado horizontal**: Más Pods (HPA)
- **Escalado vertical**: Más recursos por Pod (VPA)
- **Escalado de cluster**: Más nodos (Cluster Autoscaler)

**Diagrama de escalado:**
```
┌─────────────────────────────────────────────┐
│ Tráfico aumenta                             │
│         ↓                                   │
│ HPA detecta CPU > 70%                       │
│         ↓                                   │
│ Crear más Pods (horizontal)                 │
│         ↓                                   │
│ Si no hay recursos → Cluster Autoscaler     │
│         ↓                                   │
│ Crear nuevos nodos                          │
│         ↓                                   │
│ Distribuir Pods en nodos                    │
└─────────────────────────────────────────────┘
```

---

=== 7.1 Escalado Manual

==== 7.1.1 Concepto Básico

El **escalado manual** es cambiar el número de replicas sin automatización.

**Casos de uso:**
- Testing y desarrollo
- Prepararse para evento específico
- Control manual temporal

==== 7.1.2 Kubectl Scale

**Aumentar replicas:**

[source,bash]
----
# Cambiar Deployment de 2 a 5 replicas
kubectl scale deployment myapp --replicas=5

# Verificar
kubectl get deployment myapp
# NAME    READY   UP-TO-DATE   AVAILABLE   AGE
# myapp   5/5     5            5           10m

# Ver Pods creados
kubectl get pods
# myapp-6c9f4d7f88-1ab2c   1/1   Running
# myapp-6c9f4d7f88-2cd3e   1/1   Running
# myapp-6c9f4d7f88-3ef4g   1/1   Running
# myapp-6c9f4d7f88-4gh5h   1/1   Running
# myapp-6c9f4d7f88-5ij6i   1/1   Running
----
]

**Reducir replicas:**

[source,bash]
----
# Reducir de 5 a 2
kubectl scale deployment myapp --replicas=2

# Ver progreso
kubectl rollout status deployment myapp
# deployment "myapp" successfully rolled out

# Verificar estado final
kubectl get deployment myapp
# NAME    READY   UP-TO-DATE   AVAILABLE   AGE
# myapp   2/2     2            2           10m
----
]

==== 7.1.3 Editar Deployment

**Método imperativo avanzado:**

[source,bash]
----
# Editar directamente (abre editor)
kubectl edit deployment myapp

# Cambiar spec.replicas: 2 a 5
# Guardar y cerrar editor
# → Deployment se actualiza automáticamente

# Patch directo
kubectl patch deployment myapp -p '{"spec":{"replicas":3}}'

# O con JSON merge
kubectl patch deployment myapp --type merge \
  -p '{"spec":{"replicas":3}}'
----
]

==== 7.1.4 Escalado Declarativo

**En YAML (mejor práctica):**

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 5  # ← Cambiar aquí
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: myapp:1.0
----
]

[source,bash]
----
# Aplicar cambios
kubectl apply -f deployment.yaml

# Verificar cambios
kubectl get deployment myapp -o yaml | grep replicas
# replicas: 5
----
]

---

=== 7.2 Horizontal Pod Autoscaler (HPA)

==== 7.2.1 ¿Qué es HPA?

Un **Horizontal Pod Autoscaler** escala automáticamente el número de Pods basándose en métricas.

**Concepto:**
```
CPU > 70% → Crear más Pods
CPU < 30% → Reducir Pods
```

**Diagrama:**
```
┌──────────────────────────────────┐
│ Metrics Server (monitoreo)       │
│ mide: CPU, memoria, custom       │
│         ↓                        │
│ HPA controller                   │
│ compara con threshold            │
│         ↓                        │
│ Ajusta replicas                  │
│ scale-up o scale-down            │
└──────────────────────────────────┘
```

**Requisitos:**
- Metrics Server instalado
- Resource requests definidos en Pods
- Métricas disponibles (puede tardar 1-2 min)

==== 7.2.2 Instalar Metrics Server

En la mayoría de clusters cloud ya está instalado. Para Minikube:

[source,bash]
----
# Verificar si Metrics Server existe
kubectl get deployment metrics-server -n kube-system

# Si no existe, instalar
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# Esperar a que esté listo
kubectl wait --for=condition=available --timeout=300s \
  deployment/metrics-server -n kube-system

# Verificar que recolecta métricas
kubectl top nodes
# NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
# minikube   234m         11%    1023Mi          26%

kubectl top pods
# NAME                     CPU(m)   MEMORY(Mi)
# myapp-6c9f4d7f88-1ab2c   45m      128Mi
----
]

==== 7.2.3 HPA Basado en CPU

**Caso simple: Escalar por CPU:**

[source,yaml]
----
# Deployment con resource requests
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
      - name: app
        image: myapp:1.0
        
        # IMPORTANTE: Resource requests son requeridos para HPA
        resources:
          requests:
            cpu: 100m        # 100 millicores
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        
        ports:
        - containerPort: 8080

---
# Service
apiVersion: v1
kind: Service
metadata:
  name: web-app
spec:
  selector:
    app: web-app
  ports:
  - port: 80
    targetPort: 8080
  type: ClusterIP

---
# HPA - Escalar basado en CPU
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-app-hpa
spec:
  # Qué escalable
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  
  # Rango de replicas
  minReplicas: 2
  maxReplicas: 10
  
  # Métricas
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Escalar cuando CPU > 70%
  
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # Escalar cuando memoria > 80%
  
  # Comportamiento
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # Esperar 5 min antes de reducir
      policies:
      - type: Percent
        value: 50  # Reducir max 50% de Pods actuales
        periodSeconds: 60
    
    scaleUp:
      stabilizationWindowSeconds: 0  # Escalar inmediatamente
      policies:
      - type: Percent
        value: 100  # Duplicar Pods
        periodSeconds: 15
      - type: Pods
        value: 4  # O agregar 4 Pods
        periodSeconds: 15
      selectPolicy: Max  # Usar la política más agresiva
----
]

**Verificar HPA:**

[source,bash]
----
# Ver HPA
kubectl get hpa web-app-hpa
# NAME             REFERENCE              TARGETS      MINPODS  MAXPODS  REPLICAS  AGE
# web-app-hpa      Deployment/web-app     45%/70%      2        10       2         5m

# Ver detalles
kubectl describe hpa web-app-hpa
# Name:                      web-app-hpa
# Namespace:                 default
# Reference:                 Deployment/web-app
# Metrics:                   ( current / target )
#   resource cpu on pods  (avg):       45% / 70%
#   resource memory on pods  (avg):    60% / 80%
# Min replicas:              2
# Max replicas:              10
# Deployment pods:           2 current / 2 desired
# Conditions:
#   Type            Status  Reason
#   AbleToScale     True    ScaleDownStabilized
#   ScalingActive   True    ValidMetricsFound
----
]

==== 7.2.4 HPA v1 (Simplificado)

Para casos simples, puedes usar autoscaling/v1:

[source,yaml]
----
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: simple-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 80  # CPU > 80%
----
]

==== 7.2.5 Métricas Personalizadas

Para aplicaciones que exponen métricas propias (no CPU/memoria):

[source,yaml]
----
# Ejemplo: Escalar por solicitudes HTTP por segundo

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-custom-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app
  
  minReplicas: 1
  maxReplicas: 20
  
  metrics:
  # Métrica personalizada de Prometheus
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second  # Métrica custom
      target:
        type: AverageValue
        averageValue: "1k"  # Escalar cuando > 1000 req/s por Pod
  
  # También pueden ser métricas externas
  - type: External
    external:
      metric:
        name: queue_depth
        selector:
          matchLabels:
            queue_name: tasks
      target:
        type: Value
        value: "30"  # Escalar cuando queue depth > 30
----
]

**Para usar métricas personalizadas, necesitas:**
1. Prometheus o similar recolectando métricas
2. Adaptor de métricas instalado (custom-metrics-apiserver)

[source,bash]
----
# Instalar Prometheus Adapter
helm install prometheus-adapter prometheus-community/prometheus-adapter \
  -n prometheus

# Ver métricas disponibles
kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1 | jq .
----
]

==== 7.2.6 Comportamiento de Escalado

Define cómo y cuándo escalar:

[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: controlled-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app
  
  minReplicas: 1
  maxReplicas: 100
  
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  
  behavior:
    # Escalado hacia abajo (reducir Pods)
    scaleDown:
      # Esperar 5 minutos antes de reducir
      # Evita fluctuaciones
      stabilizationWindowSeconds: 300
      
      policies:
      # Política 1: Reducir al 50% de Pods actuales
      - type: Percent
        value: 50
        periodSeconds: 60  # Cada minuto
      
      # Política 2: Reducir máximo 2 Pods
      - type: Pods
        value: 2
        periodSeconds: 60
      
      # Usar la política menos agresiva (conservador)
      selectPolicy: Min
    
    # Escalado hacia arriba (crear Pods)
    scaleUp:
      # Escalar inmediatamente sin esperar
      stabilizationWindowSeconds: 0
      
      policies:
      # Política 1: Duplicar Pods
      - type: Percent
        value: 100
        periodSeconds: 15  # Cada 15 segundos
      
      # Política 2: Agregar 4 Pods
      - type: Pods
        value: 4
        periodSeconds: 15
      
      # Usar la política más agresiva (rápido)
      selectPolicy: Max
----
]

**Tipos de políticas:**
- `Percent`: % del número actual de Pods
- `Pods`: Número absoluto de Pods

**selectPolicy:**
- `Max`: Usar la que agregue más Pods (escala rápido)
- `Min`: Usar la que agregue menos Pods (conservador)

---

=== 7.3 Vertical Pod Autoscaler (VPA)

==== 7.3.1 ¿Qué es VPA?

El **Vertical Pod Autoscaler** ajusta automáticamente los resource requests/limits de Pods.

**Diferencia HPA vs VPA:**
```
HPA: 2 Pods con 100m CPU → CPU alta → 4 Pods con 100m CPU
VPA: 2 Pods con 100m CPU → CPU alta → 2 Pods con 200m CPU
```

**Casos de uso:**
- Aplicaciones que crecen lentamente
- Right-sizing de recursos
- Aplicaciones con patrón de uso predecible

==== 7.3.2 Instalar VPA

[source,bash]
----
# Clonar repositorio
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler

# Instalar
./hack/vpa-up.sh

# Verificar instalación
kubectl get deployment -n kube-system | grep vpa
# vpa-admission-controller
# vpa-recommender
# vpa-updater

# Ver logs
kubectl logs -n kube-system deployment/vpa-recommender
----
]

==== 7.3.3 VPA Modes

VPA tiene cuatro modos de operación:

===== Off

No hace nada, solo genera recomendaciones:

[source,yaml]
----
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: app-vpa-off
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app
  
  updatePolicy:
    updateMode: "Off"  # Solo recomendaciones
----
]

[source,bash]
----
# Ver recomendaciones sin aplicarlas
kubectl get vpa app-vpa-off --watch
# Ver detalles
kubectl describe vpa app-vpa-off
# Recommendation:
#   Container Recommendations:
#   - Container Name: app
#     Lower Bound:
#       Cpu:     25m
#       Memory:  32Mi
#     Target:
#       Cpu:     50m
#       Memory:  64Mi
#     Upper Bound:
#       Cpu:     100m
#       Memory:  256Mi
----
]

===== Initial

Ajusta recursos solo cuando se crea el Pod (init):

[source,yaml]
----
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: app-vpa-initial
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app
  
  updatePolicy:
    updateMode: "Initial"
  
  resourcePolicy:
    containerPolicies:
    - containerName: app
      minAllowed:
        cpu: 25m
        memory: 32Mi
      maxAllowed:
        cpu: 1000m
        memory: 1Gi
----
]

**Flujo:**
1. Crear Deployment sin VPA
2. Activar VPA-Initial
3. Eliminar Pods (se recrean con nuevos recursos)
4. VPA asigna recursos iniciales
5. No se modifica después

===== Recreate

Reinicia Pods cuando hay que cambiar recursos:

[source,yaml]
----
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: app-vpa-recreate
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app
  
  updatePolicy:
    updateMode: "Recreate"
    minReplicas: 2  # Mantener mínimo
  
  resourcePolicy:
    containerPolicies:
    - containerName: app
      minAllowed:
        cpu: 50m
        memory: 64Mi
      maxAllowed:
        cpu: 2000m
        memory: 2Gi
----
]

**Flujo:**
1. VPA monitorea uso de recursos
2. Si necesita ajustar, termina Pod
3. Nuevo Pod se crea con recursos ajustados
4. Continúa monitoreando y ajustando

===== Auto (predeterminado)

Usa Recreate en la mayoría de casos, Initial para StatefulSets:

[source,yaml]
----
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: app-vpa-auto
spec:
  targetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: db
  
  updatePolicy:
    updateMode: "Auto"  # Recomendado
  
  resourcePolicy:
    containerPolicies:
    - containerName: db
      minAllowed:
        cpu: 500m
        memory: 512Mi
      maxAllowed:
        cpu: 4000m
        memory: 4Gi
      
      # Controlar comportamiento
      controlledValues: RequestsAndLimits  # Ambos
      # o
      # controlledValues: RequestsOnly       # Solo requests
----
]

==== 7.3.4 Ejemplo Completo: VPA + HPA

Combinar VPA (ajustar tamaño) + HPA (ajustar cantidad):

[source,yaml]
----
# Deployment sin resource requests especificados
apiVersion: apps/v1
kind: Deployment
metadata:
  name: smart-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: smart-app
  template:
    metadata:
      labels:
        app: smart-app
    spec:
      containers:
      - name: app
        image: myapp:1.0
        # Sin resource requests → VPA las agregará

---
# VPA: Ajusta requests iniciales
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: smart-app-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: smart-app
  
  updatePolicy:
    updateMode: "Initial"  # O "Auto"
  
  resourcePolicy:
    containerPolicies:
    - containerName: app
      minAllowed:
        cpu: 50m
        memory: 64Mi
      maxAllowed:
        cpu: 1000m
        memory: 1Gi

---
# HPA: Escala horizontalmente
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: smart-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: smart-app
  
  minReplicas: 1
  maxReplicas: 10
  
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
----
]

**Resultado:**
- VPA asegura que cada Pod tiene los requests correctos
- HPA escala horizontalmente según carga
- Combinación óptima: scaling eficiente

---

=== 7.4 Cluster Autoscaler

==== 7.4.1 ¿Qué es Cluster Autoscaler?

El **Cluster Autoscaler** agrega/elimina **nodos** del cluster basándose en demanda de Pods.

**Flujo:**
```
Pod no puede asignarse (no hay espacio)
         ↓
Cluster Autoscaler lo detecta
         ↓
Solicita nuevo nodo al cloud provider
         ↓
Nodo se une al cluster
         ↓
Pod se asigna al nuevo nodo
```

**Requisitos:**
- Cloud provider (AWS, GCP, Azure)
- Cluster Autoscaler instalado
- Auto Scaling Group (ASG) o equivalente

==== 7.4.2 Instalar Cluster Autoscaler

===== AWS EKS

[source,bash]
----
# 1. Crear policy IAM
cat > autoscaler-policy.json << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "autoscaling:DescribeAutoScalingGroups",
        "autoscaling:DescribeAutoScalingInstances",
        "autoscaling:DescribeLaunchConfigurations",
        "autoscaling:SetDesiredCapacity",
        "autoscaling:TerminateInstanceInAutoScalingGroup",
        "ec2:DescribeLaunchTemplateVersions"
      ],
      "Resource": "*"
    }
  ]
}
EOF

aws iam create-policy \
  --policy-name EKSClusterAutoscalerPolicy \
  --policy-document file://autoscaler-policy.json

# 2. Crear IRSA (IAM Role for Service Account)
eksctl create iamserviceaccount \
  --name cluster-autoscaler \
  --namespace kube-system \
  --cluster=my-cluster \
  --attach-policy-arn=arn:aws:iam::ACCOUNT:policy/EKSClusterAutoscalerPolicy \
  --override-existing-serviceaccounts \
  --approve

# 3. Instalar Cluster Autoscaler
helm repo add autoscaler https://kubernetes.github.io/autoscaler
helm install cluster-autoscaler autoscaler/cluster-autoscaler \
  --namespace kube-system \
  --set awsRegion=us-east-1 \
  --set autoDiscovery.clusterName=my-cluster \
  --set rbac.serviceAccount.create=false \
  --set rbac.serviceAccount.name=cluster-autoscaler

# 4. Verificar
kubectl get deployment cluster-autoscaler -n kube-system
kubectl logs -n kube-system deployment/cluster-autoscaler
----
]

===== GKE (Google Cloud)

[source,bash]
----
# GKE lo instala automáticamente si habilitas autoscaling
gcloud container clusters create my-cluster \
  --enable-autoscaling \
  --min-nodes=1 \
  --max-nodes=10 \
  --zone=us-central1-a

# Verificar en nodos existentes
gcloud container clusters describe my-cluster \
  --zone=us-central1-a \
  --format="value(nodePools[0].autoscaling)"
----
]

===== Azure AKS

[source,bash]
----
# AKS lo instala automáticamente
az aks create \
  --resource-group myResourceGroup \
  --name myAKSCluster \
  --enable-cluster-autoscaler \
  --min-count 1 \
  --max-count 10

# Verificar
kubectl get deployment -n kube-system | grep autoscaler
----
]

==== 7.4.3 Configuración

**Limites de escalado:**

[source,bash]
----
# Ver configuración actual
kubectl get nodes -o wide
# NAME                      STATUS   ROLES
# ip-10-0-1-100.ec2.internal   Ready    <none>

# Ver Cluster Autoscaler logs
kubectl logs -n kube-system deployment/cluster-autoscaler | grep "Scaling up"

# Ver eventos de escalado
kubectl get events -n kube-system | grep "cluster-autoscaler"
----
]

**Node Groups con diferentes configuraciones:**

[source,yaml]
----
# Ejemplo: AWS - Multiple node groups con Cluster Autoscaler
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-status
data:
  nodes.max: "100"      # Máximo 100 nodos total
  nodes.min: "1"        # Mínimo 1 nodo
  
---
# Pod que DEBE correr (no puede escalarse)
apiVersion: v1
kind: Pod
metadata:
  name: critical-addon
  labels:
    tier: system
spec:
  priorityClassName: system-cluster-critical  # Alta prioridad
  containers:
  - name: addon
    image: critical-app:1.0

---
# Pod que puede ser eviccionado
apiVersion: v1
kind: Pod
metadata:
  name: interruptible-job
spec:
  containers:
  - name: job
    image: batch-job:1.0
  
  # No puede ser movido (por defecto sí)
  # affinity:
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #       - matchExpressions:
  #         - key: instance-type
  #           operator: In
  #           values:
  #           - t3.medium
----
]

==== 7.4.4 Combinación: HPA + Cluster Autoscaler

**Flujo completo:**

[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: full-auto-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app
  
  minReplicas: 2
  maxReplicas: 100  # Puede crecer mucho
  
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

---
# Nodos en cluster:
# Inicialmente: 1 nodo con 4 CPUs
# Capacidad: 40 Pods × 100m = 4000m = 4 CPUs
# 
# Si HPA crea 50 Pods (5000m requerido):
# 1. 40 Pods se asignan al nodo existente
# 2. 10 Pods quedan sin asignar (Pending)
# 3. Cluster Autoscaler detecta Pods Pending
# 4. Agrega nuevo nodo
# 5. 10 Pods restantes se asignan
----
]

==== 7.4.5 Debugging de Cluster Autoscaler

[source,bash]
----
# Ver logs detallados
kubectl logs -n kube-system deployment/cluster-autoscaler -f

# Ver eventos de cluster
kubectl describe node NODE_NAME

# Verificar si hay Pods pending
kubectl get pods -A --field-selector=status.phase=Pending

# Ver nodos disponibles
kubectl get nodes -o wide

# Ver resource allocation
kubectl describe node NODE_NAME | grep "Allocated resources"

# Forzar evaluación de escalado
# (La mayoría de autoscalers revisan cada 10-15 segundos)
# Crear Pod que no cabe
kubectl run test-pod --image=nginx \
  -n default -- sleep 1000

# Ver si Cluster Autoscaler agrega nodos
kubectl get nodes --watch
----
]

---

=== 7.5 Pruebas de Carga

==== 7.5.1 Generar Carga

===== Usando Apache Bench

[source,bash]
----
# Instalar
sudo apt-get install apache2-utils

# Obtener URL del servicio
kubectl port-forward svc/web-app 8080:80 &

# Generar tráfico
ab -n 10000 -c 100 http://localhost:8080/

# Interpretación:
# -n: 10000 solicitudes totales
# -c: 100 conexiones concurrentes
# 
# Resultado:
# Requests per second: 150
# Documento Length: 2048 bytes
----
]

===== Usando Siege

[source,bash]
----
# Instalar
sudo apt-get install siege

# Test de carga sostenida
siege -b -c 50 -r 100 http://localhost:8080/

# -b: Benchmark mode
# -c 50: 50 usuarios concurrentes
# -r 100: Cada usuario hace 100 peticiones
----
]

===== Usando Kubench (en el cluster)

[source,yaml]
----
# Pod de testing que genera carga
apiVersion: v1
kind: Pod
metadata:
  name: load-generator
spec:
  containers:
  - name: load-gen
    image: busybox:latest
    command:
    - /bin/sh
    - -c
    - |
      while true; do
        wget -q -O- http://web-app.default.svc.cluster.local/
        sleep 0.1
      done
  restartPolicy: Never
----
]

Lanzar múltiples copias:

[source,bash]
----
# Crear varios Pods de carga
for i in {1..10}; do
  kubectl run load-$i --image=busybox -i --tty=false \
    -- /bin/sh -c \
    "while true; do wget -q -O- http://web-app/; done"
done

# Ver Pods creándose
kubectl get pods -w

# Ver HPA escalando
kubectl get hpa -w
# Debe aumentar replicas
----
]

==== 7.5.2 Monitorar Escalado

**En tiempo real:**

[source,bash]
----
# Terminal 1: Ver HPA
kubectl get hpa -w

# Terminal 2: Ver Pods
kubectl get pods -w

# Terminal 3: Ver nodes
kubectl get nodes -w

# Terminal 4: Ver métricas
kubectl top pods -w

# Terminal 5: Ver eventos
kubectl get events -w --sort-by='.lastTimestamp'
----
]

**Con kubectl watch:**

[source,bash]
----
# Todo en una terminal
watch -n 1 'echo "=== HPA ===" && kubectl get hpa && \
            echo "=== PODS ===" && kubectl get pods | tail -5 && \
            echo "=== NODES ===" && kubectl get nodes'
----
]

==== 7.5.3 Caso Práctico: E-Commerce Black Friday

**Prepararse para pico de tráfico:**

[source,yaml]
----
# 1. Deployment con requests claros
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ecommerce
  labels:
    app: ecommerce
spec:
  replicas: 5  # Base inicial
  selector:
    matchLabels:
      app: ecommerce
  template:
    metadata:
      labels:
        app: ecommerce
    spec:
      containers:
      - name: api
        image: ecommerce-api:1.0
        
        ports:
        - containerPort: 8080
        
        # Resource requests CRÍTICOS
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
        
        # Health checks
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 2

---
# 2. HPA agresivo para Black Friday
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ecommerce-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ecommerce
  
  minReplicas: 5     # Mínimo 5 Pods siempre
  maxReplicas: 200   # Puede crecer mucho
  
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60  # Más agresivo de lo normal
  
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 70
  
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100  # Duplicar cada 15 segundos
        periodSeconds: 15
      - type: Pods
        value: 10   # O agregar 10 Pods
        periodSeconds: 15
      selectPolicy: Max
    
    scaleDown:
      stabilizationWindowSeconds: 600  # Esperar 10 minutos
      policies:
      - type: Percent
        value: 25   # Muy conservador
        periodSeconds: 60

---
# 3. PodDisruptionBudget (No matar Pods durante escalado)
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ecommerce-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: ecommerce

---
# 4. Service
apiVersion: v1
kind: Service
metadata:
  name: ecommerce
spec:
  type: LoadBalancer
  selector:
    app: ecommerce
  ports:
  - port: 80
    targetPort: 8080
  sessionAffinity: ClientIP  # Sticky sessions
----
]

**Monitoreo durante el evento:**

[source,bash]
----
# Dashboard en tiempo real
while true; do
  clear
  echo "=== ECOMMERCE BLACK FRIDAY DASHBOARD ==="
  echo "Time: $(date)"
  echo ""
  
  echo "=== HPA Status ==="
  kubectl get hpa ecommerce-hpa
  
  echo ""
  echo "=== Pod Count ==="
  kubectl get pods -l app=ecommerce | wc -l
  
  echo ""
  echo "=== Node Count ==="
  kubectl get nodes | tail -n +2 | wc -l
  
  echo ""
  echo "=== Top CPU Consumers ==="
  kubectl top pods -l app=ecommerce --no-headers | \
    sort -k2 -nr | head -5
  
  echo ""
  echo "=== Load Balancer ==="
  kubectl get svc ecommerce
  
  sleep 5
done
----
]

==== 7.5.4 Debugging de Autoscaling

**HPA no escala:**

[source,bash]
----
# Verificar HPA status
kubectl describe hpa myapp-hpa

# Razones comunes:
# 1. Metrics no disponibles
kubectl get hpa myapp-hpa
# Si TARGETS muestra "<unknown>", las métricas no están listos

# Solución:
kubectl get deployment myapp -o yaml | grep -A5 "resources:"
# Debe tener requests definidos

# 2. Metrics Server no instalado
kubectl get deployment metrics-server -n kube-system

# 3. CPU muy baja
kubectl top pods
# Si promedio << threshold, no escala

# 4. Ya está en maxReplicas
kubectl get hpa myapp-hpa
# Si REPLICAS == MAXPODS, está en el límite
----
]

**Cluster Autoscaler no agrega nodos:**

[source,bash]
----
# Ver logs
kubectl logs -n kube-system deployment/cluster-autoscaler

# Buscar errores comunes:
# - "Failed to increase node group size"
#   → Límite de ASG alcanzado
#   → Aumentar max-size en AWS

# - "No node template found"
#   → Node group mal configurada
#   → Verificar tags de Auto Scaling Group

# Verificar Pods pending
kubectl get pods -A --field-selector=status.phase=Pending

# Ver por qué no se asigna
kubectl describe pod POD_NAME
# Buscar "Events" section
----
]

---

=== 7.6 Ejemplos Completos

==== Ejemplo 1: Stack Completo de Autoscaling

[source,yaml]
----
# Full autoscaling stack: HPA + Cluster Autoscaler
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api-server
  template:
    metadata:
      labels:
        app: api-server
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - api-server
              topologyKey: kubernetes.io/hostname
      
      containers:
      - name: api
        image: api-server:1.0
        
        ports:
        - containerPort: 8080
        
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: api-server
spec:
  type: LoadBalancer
  selector:
    app: api-server
  ports:
  - port: 80
    targetPort: 8080

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-server-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  
  minReplicas: 3
  maxReplicas: 50
  
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 5
        periodSeconds: 15
      selectPolicy: Max
    
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-server-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: api-server
----
]

==== Ejemplo 2: Stateful App con VPA

[source,yaml]
----
# StatefulSet (aplicación stateful) + VPA
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cache-server
spec:
  serviceName: cache
  replicas: 3
  selector:
    matchLabels:
      app: cache-server
  template:
    metadata:
      labels:
        app: cache-server
    spec:
      containers:
      - name: redis
        image: redis:7
        
        ports:
        - containerPort: 6379
        
        # Sin requests iniciales → VPA las asignará
        resources:
          limits:
            cpu: 2000m
            memory: 2Gi

---
apiVersion: v1
kind: Service
metadata:
  name: cache
spec:
  clusterIP: None
  selector:
    app: cache-server
  ports:
  - port: 6379
    targetPort: 6379

---
# VPA ajusta automáticamente
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: cache-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: cache-server
  
  updatePolicy:
    updateMode: "Auto"
  
  resourcePolicy:
    containerPolicies:
    - containerName: redis
      minAllowed:
        cpu: 100m
        memory: 256Mi
      maxAllowed:
        cpu: 2000m
        memory: 2Gi
----
]

---

=== 7.7 Best Practices

**1. Siempre definir resource requests**
```yaml
❌ Sin requests → HPA no funciona
✅ requests: cpu: 100m, memory: 128Mi
```

**2. Usar HPA v2 (no v1)**
```yaml
❌ autoscaling/v1 → solo CPU
✅ autoscaling/v2 → CPU, memoria, métricas custom
```

**3. Combinar HPA + Cluster Autoscaler**
```yaml
✅ HPA escala Pods
✅ Cluster Autoscaler escala nodos
✅ Combinados = escalado completo
```

**4. Usar PodDisruptionBudget**
```yaml
✅ Evita apagar Pods críticos durante escalado
```

**5. Monitorear y alertar**
```bash
✅ Alertas cuando HPA en maxReplicas
✅ Alertas si escalado no responde
```

---

=== 7.8 Resumen del Módulo 7

En este módulo aprendiste:

1. **Escalado Manual**: kubectl scale, editar replicas
   - Casos: testing, eventos específicos
   - Imperativo vs declarativo

2. **HPA (Horizontal Pod Autoscaler)**: Escalar Pods
   - Basado en CPU, memoria, métricas custom
   - Comportamiento configurable
   - v1 vs v2

3. **VPA (Vertical Pod Autoscaler)**: Ajustar recursos
   - Modos: Off, Initial, Recreate, Auto
   - Right-sizing automático

4. **Cluster Autoscaler**: Escalar nodos
   - AWS, GCP, Azure
   - Detecta Pods Pending
   - Agrega nodos automáticamente

5. **Pruebas de Carga**: Validar escalado
   - Apache Bench, Siege
   - Monitoreo en tiempo real
   - Casos prácticos: Black Friday

Con estos conocimientos, estás listo para aprender sobre **Seguridad** en el Módulo 8.

---

== MÓDULO 8: Seguridad

La **seguridad en Kubernetes** es multicapa. No es solo RBAC, sino autenticación, autorización, encriptación, network policies, y más.

**Capas de seguridad:**
```
┌──────────────────────────────────────────┐
│ 1. Autenticación: ¿Quién eres?          │
│ 2. Autorización: ¿Qué puedes hacer?     │
│ 3. Admission Control: ¿Cumples políticas?│
│ 4. Pod Security: ¿Eres seguro?          │
│ 5. Network Policies: ¿Dónde comunicas?  │
│ 6. Encriptación: ¿Están seguros datos?  │
└──────────────────────────────────────────┘
```

---

=== 8.1 Autenticación y RBAC

==== 8.1.1 Usuarios y Service Accounts

Hay dos tipos de identidades en Kubernetes:

===== Usuarios Normales

Usuarios del cluster (personas, equipos, CI/CD):

[source,bash]
----
# Crear usuario con certificado x509
# Generar clave privada
openssl genrsa -out alice.key 2048

# Crear solicitud de firma de certificado (CSR)
openssl req -new -key alice.key -out alice.csr \
  -subj "/CN=alice/O=developers"

# Firma el certificado (como admin)
sudo openssl x509 -req -in alice.csr \
  -CA /etc/kubernetes/pki/ca.crt \
  -CAkey /etc/kubernetes/pki/ca.key \
  -CAcreateserial \
  -out alice.crt -days 365

# Crear contexto kubeconfig para Alice
kubectl config set-credentials alice \
  --client-certificate=alice.crt \
  --client-key=alice.key

kubectl config set-context alice-context \
  --cluster=kubernetes \
  --user=alice

# Alice usa su contexto
kubectl config use-context alice-context
kubectl get pods  # Sin permisos aún
# Error: pods is forbidden
----
]

===== Service Accounts

Para Pods/aplicaciones dentro del cluster:

[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: app-sa
  namespace: production

---
# Usar en Pod
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
  namespace: production
spec:
  serviceAccountName: app-sa  # ← Asignar service account
  containers:
  - name: app
    image: myapp:1.0
----
]

[source,bash]
----
# Ver token del service account
kubectl get secret -n production \
  $(kubectl get secret -n production | grep app-sa | awk '{print $1}') \
  -o jsonpath='{.data.token}' | base64 -d

# Usar token en API calls
TOKEN=$(kubectl get secret -n production \
  $(kubectl get secret -n production | grep app-sa | awk '{print $1}') \
  -o jsonpath='{.data.token}' | base64 -d)

curl -H "Authorization: Bearer $TOKEN" \
  https://kubernetes.default.svc.cluster.local/api/v1/namespaces/production/pods \
  --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
----
]

==== 8.1.2 RBAC (Role-Based Access Control)

**Concepto RBAC:**
```
User → RoleBinding → Role → Permissions
      ↑              ↑
      │              └─ Define acciones (get, create, delete)
      └───────────────── Asigna role al usuario
```

===== Roles (Namespaced)

Permiso dentro de un namespace:

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: production
rules:
# Regla 1: Leer Pods
- apiGroups: [""]  # API core
  resources: ["pods"]
  verbs: ["get", "list", "watch"]

# Regla 2: Acceder a logs
- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get"]

# Regla 3: Ejecutar comandos en Pods
- apiGroups: [""]
  resources: ["pods/exec"]
  verbs: ["create", "get"]

# Regla 4: Crear Deployments (limitado)
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list"]
  resourceNames: ["myapp"]  # Solo myapp, no otros
----
]

===== RoleBinding (Namespaced)

Asignar Role a usuario/grupo/service account:

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: pod-reader-binding
  namespace: production
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: pod-reader
subjects:
# Usuario normal
- kind: User
  name: alice
  apiGroup: rbac.authorization.k8s.io

# Grupo de usuarios
- kind: Group
  name: developers
  apiGroup: rbac.authorization.k8s.io

# Service Account
- kind: ServiceAccount
  name: app-sa
  namespace: production
----
]

===== ClusterRoles (Cluster-wide)

Permiso a nivel de cluster (no namespaced):

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: admin-role
rules:
# Todo en API core
- apiGroups: [""]
  resources: ["*"]
  verbs: ["*"]

# Todo en apps
- apiGroups: ["apps"]
  resources: ["*"]
  verbs: ["*"]

# Recursos no-namespaced
- apiGroups: [""]
  resources: ["nodes", "persistentvolumes"]
  verbs: ["get", "list", "watch"]

# Modificar RBAC
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["*"]
  verbs: ["*"]
----
]

===== ClusterRoleBinding

Asignar ClusterRole a nivel de cluster:

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: admin-role
subjects:
- kind: User
  name: alice
  apiGroup: rbac.authorization.k8s.io

---
# Rol de solo lectura
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: read-only
rules:
- apiGroups: [""]
  resources: ["*"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps", "batch"]
  resources: ["*"]
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: read-only-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: read-only
subjects:
- kind: User
  name: bob
  apiGroup: rbac.authorization.k8s.io
----
]

==== 8.1.3 Roles Predefinidos

Kubernetes proporciona roles por defecto:

[source,bash]
----
# Ver roles predefinidos
kubectl get clusterroles | grep system

# Roles comunes:
# - system:basic-user
# - system:discover
# - cluster-admin (acceso total)
# - edit (create, update, delete)
# - view (solo lectura)

# Ver qué permisos tiene un role
kubectl describe clusterrole edit

# Usar roles predefinidos
kubectl create clusterrolebinding dev-binding \
  --clusterrole=edit \
  --user=developer@company.com

# Verificar permisos
kubectl auth can-i create pods --as=alice
# yes/no

kubectl auth can-i delete nodes --as=alice --all-namespaces
# no
----
]

==== 8.1.4 Ejemplo Completo RBAC

**Escenario:** 3 usuarios con diferentes permisos

[source,yaml]
----
# 1. Alice: Admin del cluster
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: alice-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: User
  name: alice

---
# 2. Bob: Desarrollador (solo production namespace)
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: dev-role
  namespace: production
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps", "secrets"]
  verbs: ["get", "list", "create", "update", "patch"]
- apiGroups: ["apps"]
  resources: ["deployments", "statefulsets"]
  verbs: ["get", "list", "create", "update", "patch"]
- apiGroups: [""]
  resources: ["pods/logs", "pods/exec"]
  verbs: ["get", "create"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: dev-binding
  namespace: production
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: dev-role
subjects:
- kind: User
  name: bob

---
# 3. Charlie: CI/CD (solo crear/actualizar, no eliminar)
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cicd-role
  namespace: production
rules:
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "create", "update", "patch"]
- apiGroups: [""]
  resources: ["services"]
  verbs: ["get", "list"]
- apiGroups: [""]
  resources: ["secrets", "configmaps"]
  verbs: ["get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cicd-binding
  namespace: production
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cicd-role
subjects:
- kind: ServiceAccount
  name: cicd-sa
  namespace: production

---
# 4. Service Account para CI/CD
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cicd-sa
  namespace: production
----
]

---

=== 8.2 Pod Security

==== 8.2.1 Security Context

Define permisos de seguridad para Pods y contenedores:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: secure-pod
spec:
  # Pod-level security
  securityContext:
    runAsUser: 1000  # UID del usuario
    runAsGroup: 3000  # GID del grupo
    fsGroup: 2000     # GID para filesystem
    seccompProfile:
      type: RuntimeDefault  # Seccomp profile

  containers:
  - name: app
    image: myapp:1.0
    
    # Container-level security (override pod-level)
    securityContext:
      allowPrivilegeEscalation: false  # No puede usar sudo
      capabilities:
        drop:
        - ALL  # Drop todas las capabilities
        add:
        - NET_BIND_SERVICE  # Excepto esta
      
      readOnlyRootFilesystem: true  # Filesystem read-only
      runAsNonRoot: true  # No correr como root
      runAsUser: 2000  # UID específico

    ports:
    - containerPort: 8080
    
    volumeMounts:
    - name: tmp
      mountPath: /tmp
    - name: data
      mountPath: /data
  
  volumes:
  - name: tmp
    emptyDir: {}
  - name: data
    emptyDir: {}
----
]

**Explicación:**
- `runAsUser`: Usuario no-root (UID 2000)
- `allowPrivilegeEscalation: false`: No puede usar setuid
- `capabilities`: Permisos específicos del kernel
- `readOnlyRootFilesystem`: No escribir en /
- `fsGroup`: Para permisos de volúmenes

==== 8.2.2 Pod Security Standards (PSS)

Kubernetes proporciona tres estándares de seguridad:

===== Restricted (Más seguro)

[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: secure-namespace
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted

---
# Pod que cumple Restricted
apiVersion: v1
kind: Pod
metadata:
  name: compliant-pod
  namespace: secure-namespace
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000
    seccompProfile:
      type: RuntimeDefault

  containers:
  - name: app
    image: myapp:1.0
    
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true
      runAsNonRoot: true
      runAsUser: 1000
    
    volumeMounts:
    - name: tmp
      mountPath: /tmp
  
  volumes:
  - name: tmp
    emptyDir: {}
----
]

===== Baseline (Permisivo)

Permite algunos riesgos de seguridad:

[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: baseline-namespace
  labels:
    pod-security.kubernetes.io/enforce: baseline

---
# Pod que cumple Baseline (menos restricciones)
apiVersion: v1
kind: Pod
metadata:
  name: baseline-pod
  namespace: baseline-namespace
spec:
  containers:
  - name: app
    image: legacy-app:1.0
    # Sin securityContext requerido
----
]

===== Privileged (Menos seguro)

Para aplicaciones que lo necesitan:

[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: privileged-namespace
  labels:
    pod-security.kubernetes.io/enforce: privileged

---
# Pod privilegiado (Docker, kubelet, etc)
apiVersion: v1
kind: Pod
metadata:
  name: privileged-pod
  namespace: privileged-namespace
spec:
  securityContext:
    runAsUser: 0  # root
  
  containers:
  - name: daemon
    image: privileged-app:1.0
    
    securityContext:
      privileged: true  # Acceso completo al host
      capabilities:
        add:
        - SYS_ADMIN
        - SYS_TIME
----
]

==== 8.2.3 Policies de Admisión

Controla qué Pods pueden crearse:

[source,yaml]
----
# ValidatingAdmissionPolicy: Validar (v1.27+)
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: require-security-context
spec:
  failurePolicy: fail  # Rechazar si falla validación
  
  matchResources:
    resourceRules:
    - operations: ["CREATE", "UPDATE"]
      apiGroups: [""]
      apiVersions: ["v1"]
      resources: ["pods"]
  
  validations:
  - expression: "object.spec.securityContext.runAsNonRoot == true"
    message: "Root containers not allowed"
  
  - expression: "object.spec.securityContext.fsGroup > 0"
    message: "fsGroup must be set"

---
# MutatingAdmissionPolicy: Modificar (v1.27+)
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingAdmissionPolicy
metadata:
  name: add-security-defaults
spec:
  failurePolicy: fail
  
  matchResources:
    resourceRules:
    - operations: ["CREATE"]
      apiGroups: [""]
      apiVersions: ["v1"]
      resources: ["pods"]
  
  mutations:
  # Si no tiene securityContext, agregarlo
  - expression: "!has(object.spec, 'securityContext') ? object.spec.securityContext = {runAsNonRoot: true, fsGroup: 1000} : object.spec"
----
]

==== 8.2.4 Ejemplo: Deployment Seguro

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: secure-app
  namespace: production
spec:
  replicas: 2
  selector:
    matchLabels:
      app: secure-app
  template:
    metadata:
      labels:
        app: secure-app
      annotations:
        container.apparmor.security.beta.kubernetes.io/app: runtime/default
    
    spec:
      # Pod-level security
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 3000
        fsGroup: 2000
        seccompProfile:
          type: RuntimeDefault
        seLinuxOptions:
          level: "s0:c123,c456"
      
      serviceAccountName: app-sa
      
      containers:
      - name: app
        image: secure-app:1.0
        imagePullPolicy: Always
        
        ports:
        - containerPort: 8080
          name: http
        
        # Container security
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
            add:
            - NET_BIND_SERVICE
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
        
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
        
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: cache
          mountPath: /cache
      
      volumes:
      - name: tmp
        emptyDir: {}
      - name: cache
        emptyDir: {}

---
# Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: app-sa
  namespace: production
----
]

---

=== 8.3 Network Policies

==== 8.3.1 Concepto

Una **Network Policy** es un firewall para Pods. Define tráfico permitido (ingress/egress).

**Sin Network Policy:**
```
Todos los Pods pueden hablar con todos
                ↓
┌─────┐     ┌─────┐     ┌─────┐
│Pod A│←───→│Pod B│←───→│Pod C│
└─────┘     └─────┘     └─────┘
    ↑         ↓         ↑
    │         └─────────┤
    └───────────────────┘
```

**Con Network Policy (default deny):**
```
Tráfico bloqueado por defecto, solo permitir explícitamente
          ↓
┌─────┐  X  ┌─────┐  X  ┌─────┐
│Pod A│     │Pod B│     │Pod C│
└─────┘     └─────┘     └─────┘
    ↑         ↓
    │    (permitido)
    └─────────┘
```

==== 8.3.2 Default Deny

Bloquear todo por defecto:

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  # Aplica a todos los Pods
  podSelector: {}
  
  # Negar todo (no hay reglas)
  policyTypes:
  - Ingress
  - Egress
----
]

Ahora **todos los Pods están bloqueados**. Necesitas permitir explícitamente.

==== 8.3.3 Allow Tráfico Ingress

Permitir tráfico **hacia** un Pod:

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-web-ingress
  namespace: production
spec:
  # Aplica a Pods con label app=web
  podSelector:
    matchLabels:
      app: web
  
  policyTypes:
  - Ingress
  
  ingress:
  # Regla 1: Permitir desde Ingress Controller
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8080
  
  # Regla 2: Permitir desde otros Pods con label client=api
  - from:
    - podSelector:
        matchLabels:
          client: api
    ports:
    - protocol: TCP
      port: 8080
  
  # Regla 3: Permitir desde IP específica
  - from:
    - ipBlock:
        cidr: 10.0.0.0/8
        except:
        - 10.1.1.0/24
    ports:
    - protocol: TCP
      port: 8080
----
]

==== 8.3.4 Allow Tráfico Egress

Permitir tráfico **desde** un Pod:

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-api-egress
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: web
  
  policyTypes:
  - Egress
  
  egress:
  # Permitir DNS
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: UDP
      port: 53
  
  # Permitir hacia backend
  - to:
    - podSelector:
        matchLabels:
          app: backend
    ports:
    - protocol: TCP
      port: 5000
  
  # Permitir hacia internet
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 169.254.169.254/32  # No AWS metadata
    ports:
    - protocol: TCP
      port: 443
----
]

==== 8.3.5 Caso Práctico: 3-Tier Architecture

Frontend → Backend → Database

[source,yaml]
----
# 1. Default deny all
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
# 2. Frontend: Permite ingress desde Internet
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-ingress
  namespace: production
spec:
  podSelector:
    matchLabels:
      tier: frontend
  
  policyTypes:
  - Ingress
  
  ingress:
  # Desde Ingress Controller
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 80
    - protocol: TCP
      port: 443

---
# 3. Frontend: Permite egress hacia Backend
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-egress
  namespace: production
spec:
  podSelector:
    matchLabels:
      tier: frontend
  
  policyTypes:
  - Egress
  
  egress:
  # DNS
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: UDP
      port: 53
  
  # Hacia Backend
  - to:
    - podSelector:
        matchLabels:
          tier: backend
    ports:
    - protocol: TCP
      port: 8080

---
# 4. Backend: Permite ingress desde Frontend
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-backend-ingress
  namespace: production
spec:
  podSelector:
    matchLabels:
      tier: backend
  
  policyTypes:
  - Ingress
  
  ingress:
  # Solo desde Frontend
  - from:
    - podSelector:
        matchLabels:
          tier: frontend
    ports:
    - protocol: TCP
      port: 8080

---
# 5. Backend: Permite egress hacia Database
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-backend-egress
  namespace: production
spec:
  podSelector:
    matchLabels:
      tier: backend
  
  policyTypes:
  - Egress
  
  egress:
  # DNS
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: UDP
      port: 53
  
  # Hacia Database
  - to:
    - podSelector:
        matchLabels:
          tier: database
    ports:
    - protocol: TCP
      port: 5432

---
# 6. Database: Permite ingress solo desde Backend
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-database-ingress
  namespace: production
spec:
  podSelector:
    matchLabels:
      tier: database
  
  policyTypes:
  - Ingress
  
  ingress:
  # Solo desde Backend
  - from:
    - podSelector:
        matchLabels:
          tier: backend
    ports:
    - protocol: TCP
      port: 5432

---
# 7. Deployment: Frontend
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: production
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: frontend
        image: nginx:1.21
        ports:
        - containerPort: 80

---
# 8. Deployment: Backend
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  namespace: production
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: backend
  template:
    metadata:
      labels:
        tier: backend
    spec:
      containers:
      - name: backend
        image: api:1.0
        ports:
        - containerPort: 8080

---
# 9. StatefulSet: Database
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database
  namespace: production
spec:
  serviceName: database
  replicas: 1
  selector:
    matchLabels:
      tier: database
  template:
    metadata:
      labels:
        tier: database
    spec:
      containers:
      - name: postgres
        image: postgres:15
        ports:
        - containerPort: 5432

---
# 10. Services
apiVersion: v1
kind: Service
metadata:
  name: frontend
  namespace: production
spec:
  type: LoadBalancer
  selector:
    tier: frontend
  ports:
  - port: 80
    targetPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: backend
  namespace: production
spec:
  type: ClusterIP
  selector:
    tier: backend
  ports:
  - port: 8080
    targetPort: 8080

---
apiVersion: v1
kind: Service
metadata:
  name: database
  namespace: production
spec:
  clusterIP: None
  selector:
    tier: database
  ports:
  - port: 5432
    targetPort: 5432
----
]

==== 8.3.6 Debugging Network Policies

[source,bash]
----
# Ver Network Policies
kubectl get networkpolicies -n production

# Ver detalles
kubectl describe networkpolicy default-deny -n production

# Verificar conectividad
# 1. Ejecutar dentro de Pod
kubectl exec -it pod-name -n production -- /bin/sh

# 2. Intentar conectar
telnet backend.production.svc.cluster.local 8080
# Si falla → Network Policy bloqueando

# 3. Ver logs de red (si está disponible)
kubectl logs -n kube-system -l app=calico-node | grep -i policy

# Herramientas de testing
kubectl run test-pod --image=busybox -it --rm -n production \
  -- sh -c "wget -O- http://backend:8080"
# Éxito o timeout = Network Policy bloqueando

# Ver flujo exacto
kubectl get networkpolicies -A -o yaml | grep -A 20 "to:"
----
]

---

=== 8.4 Encriptación y Certificados

==== 8.4.1 TLS/SSL Certificates

===== Obtener Certificado (Let's Encrypt)

[source,bash]
----
# Instalar cert-manager
helm repo add jetstack https://charts.jetstack.io
helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager --create-namespace \
  --set installCRDs=true

# Crear ClusterIssuer
cat > cluster-issuer.yaml << EOF
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: admin@example.com
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          class: nginx
EOF

kubectl apply -f cluster-issuer.yaml

# Verificar
kubectl get clusterissuer
# NAME                READY
# letsencrypt-prod    True
----
]

===== Usar Certificados en Ingress

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: secure-app-ingress
  namespace: production
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  ingressClassName: nginx
  
  tls:
  - hosts:
    - app.example.com
    secretName: app-tls-cert  # cert-manager crea esto
  
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: app
            port:
              number: 8080
----
]

[source,bash]
----
# Cert-manager crea Secret automáticamente
kubectl get secret app-tls-cert -n production -o yaml

# Renovación automática (cada 30 días)
kubectl logs -n cert-manager deployment/cert-manager | grep renew

# Ver certificados
kubectl get certificate -n production

# Verificar certificado
kubectl get secret app-tls-cert -n production \
  -o jsonpath='{.data.tls\.crt}' | base64 -d | openssl x509 -text -noout
----
]

==== 8.4.2 Encriptación en Reposo (etcd)

[source,bash]
----
# Habilitar encriptación en etcd
# Editar API Server en /etc/kubernetes/manifests/kube-apiserver.yaml

# Crear archivo de configuración
cat > /etc/kubernetes/enc/encryption.yaml << EOF
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
    - secrets
    - configmaps
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: $(head -c 32 /dev/urandom | base64)
    - identity: {}
EOF

# Agregar a API Server
spec:
  containers:
  - command:
    - kube-apiserver
    - --encryption-provider-config=/etc/kubernetes/enc/encryption.yaml
    volumeMounts:
    - name: encryption-config
      mountPath: /etc/kubernetes/enc
      readOnly: true

volumes:
- name: encryption-config
  hostPath:
    path: /etc/kubernetes/enc
    type: DirectoryOrCreate

# Reiniciar
sudo systemctl restart kubelet

# Verificar encriptación (en la base de datos)
# Secrets estarán encriptados en etcd
----
]

==== 8.4.3 Secrets con Encryption

[source,yaml]
----
# Secret encriptado automáticamente
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
  namespace: production
stringData:
  database_password: "secure_password_123"
  api_key: "secret_key_xyz"

---
# Usar en Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
  namespace: production
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app
  template:
    metadata:
      labels:
        app: app
    spec:
      containers:
      - name: app
        image: app:1.0
        env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: app-secret
              key: database_password
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: app-secret
              key: api_key
----
]

---

=== 8.5 Auditoría y Monitoreo

==== 8.5.1 Audit Logging

Registra todas las acciones en el cluster:

[source,bash]
----
# Crear archivo de política de auditoría
cat > /etc/kubernetes/audit-policy.yaml << EOF
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
  # Log all requests at Metadata level
  - level: Metadata
    omitStages:
    - RequestReceived

  # Log pod exec at RequestResponse level
  - level: RequestResponse
    verbs: ["create"]
    resources:
    - group: ""
      resources: ["pods/exec"]

  # Log secret access
  - level: RequestResponse
    verbs: ["get", "create", "update", "patch", "delete"]
    resources:
    - group: ""
      resources: ["secrets"]
    
  # Log everything else at Metadata level
  - level: Metadata
EOF

# Configurar en API Server
--audit-policy-file=/etc/kubernetes/audit-policy.yaml
--audit-log-path=/var/log/kubernetes/audit.log
--audit-log-maxage=30
--audit-log-maxbackup=10
----
]

==== 8.5.2 RBAC Auditoría

[source,bash]
----
# Ver quién puede hacer qué
kubectl get clusterrolebindings -A

# Ver acceso negado
kubectl get events -A | grep Forbidden

# Verificar permisos de usuario
kubectl auth can-i list pods --as=alice
kubectl auth can-i delete deployments --as=bob
kubectl auth can-i '*' '*' --as=alice --all-namespaces
----
]

==== 8.5.3 Falco (Runtime Security)

Monitorea comportamiento anómalo:

[source,bash]
----
# Instalar Falco
kubectl create namespace falco
helm repo add falcosecurity https://falcosecurity.github.io/charts
helm install falco falcosecurity/falco \
  --namespace falco \
  --set falco.grpc.enabled=true

# Ver alertas
kubectl logs -n falco -f -l app=falco
# Detectable anomalies:
# - Process running as root
# - Unexpected network connection
# - Unauthorized file access
# - Privilege escalation attempt
----
]

---

=== 8.6 Ejemplo Completo: Aplicación Segura

[source,yaml]
----
# Namespace seguro
apiVersion: v1
kind: Namespace
metadata:
  name: secure-app
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted

---
# Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: app-sa
  namespace: secure-app

---
# Role limitado
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: app-role
  namespace: secure-app
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get"]
  resourceNames: ["app-secret"]

---
# RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: app-role-binding
  namespace: secure-app
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: app-role
subjects:
- kind: ServiceAccount
  name: app-sa
  namespace: secure-app

---
# Secret
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
  namespace: secure-app
stringData:
  db_password: "secure_password"
  api_key: "secret_key"

---
# Network Policy: Default Deny
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
  namespace: secure-app
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
# Network Policy: Allow Ingress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress
  namespace: secure-app
spec:
  podSelector:
    matchLabels:
      app: secure-app
  
  policyTypes:
  - Ingress
  
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8080

---
# Network Policy: Allow Egress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-egress
  namespace: secure-app
spec:
  podSelector:
    matchLabels:
      app: secure-app
  
  policyTypes:
  - Egress
  
  egress:
  # DNS
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: UDP
      port: 53
  
  # Externa API
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 169.254.169.254/32
    ports:
    - protocol: TCP
      port: 443

---
# Deployment Seguro
apiVersion: apps/v1
kind: Deployment
metadata:
  name: secure-app
  namespace: secure-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: secure-app
  template:
    metadata:
      labels:
        app: secure-app
    
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 3000
        fsGroup: 2000
        seccompProfile:
          type: RuntimeDefault
      
      serviceAccountName: app-sa
      
      containers:
      - name: app
        image: secure-app:1.0
        imagePullPolicy: Always
        
        ports:
        - containerPort: 8080
          name: http
        
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
            add:
            - NET_BIND_SERVICE
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
        
        env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: app-secret
              key: db_password
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: app-secret
              key: api_key
        
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
        
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: cache
          mountPath: /cache
      
      volumes:
      - name: tmp
        emptyDir: {}
      - name: cache
        emptyDir: {}

---
# Service
apiVersion: v1
kind: Service
metadata:
  name: secure-app
  namespace: secure-app
spec:
  type: LoadBalancer
  selector:
    app: secure-app
  ports:
  - port: 8080
    targetPort: 8080
    name: http

---
# PodDisruptionBudget
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: secure-app-pdb
  namespace: secure-app
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: secure-app
----
]

---

=== 8.7 Best Practices

**1. Principio de Menor Privilegio**
```yaml
❌ runAsUser: 0 (root)
✅ runAsUser: 1000 (non-root)
✅ allowPrivilegeEscalation: false
```

**2. RBAC Restrictivo**
```yaml
❌ cluster-admin para todos
✅ Roles específicos por función
✅ Service Accounts con permisos limitados
```

**3. Network Policies Defaut Deny**
```yaml
❌ Sin Network Policies
✅ Default deny all + explicit allow
```

**4. Secrets Encriptados**
```yaml
❌ Passwords en ConfigMaps
✅ Secrets con encryption-at-rest
✅ Secret management externo (Vault)
```

**5. Auditoría y Monitoreo**
```bash
✅ Audit logging habilitado
✅ Falco para runtime security
✅ Alertas en acciones sospechosas
```

---

=== 8.8 Resumen del Módulo 8

En este módulo aprendiste:

1. **Autenticación y RBAC**: Control de acceso
   - Usuarios y Service Accounts
   - Roles, RoleBindings, ClusterRoles
   - RBAC granular

2. **Pod Security**: Hardening de Pods
   - Security Context (runAsUser, capabilities)
   - Pod Security Standards (Restricted, Baseline)
   - Admission Policies

3. **Network Policies**: Firewall para Pods
   - Default deny
   - Ingress/Egress rules
   - 3-tier architecture example

4. **Encriptación**: Proteger datos
   - TLS/SSL Certificates
   - Encryption-at-rest en etcd
   - Secrets management

5. **Auditoría y Monitoreo**: Detección de amenazas
   - Audit logging
   - RBAC auditoría
   - Falco runtime security

Con estos conocimientos, estás listo para aprender sobre **Monitoreo y Logging** en el Módulo 9.

---

== MÓDULO 9: Monitoreo y Logging

**Observabilidad** es la capacidad de entender qué está pasando en tu cluster. Se basa en **3 pilares**:

```
┌──────────────────────────────────────────────┐
│       OBSERVABILIDAD EN KUBERNETES           │
├──────────────────────────────────────────────┤
│ 1. MÉTRICAS: ¿Cuál es el estado actual?     │
│    (CPU, memoria, requests/segundo)         │
│                                              │
│ 2. LOGS: ¿Qué eventos ocurrieron?           │
│    (errores, warnings, info)                │
│                                              │
│ 3. TRAZAS: ¿Cuál es el flujo exacto?        │
│    (distributed tracing, latencia)          │
└──────────────────────────────────────────────┘
```

---

=== 9.1 Metrics Server y Recolección de Métricas

==== 9.1.1 ¿Qué es Metrics Server?

Recolecta métricas de CPU y memoria de Pods y Nodes. Es el base para HPA y kubectl top.

[source,bash]
----
# Instalar Metrics Server
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# Verificar instalación
kubectl get deployment metrics-server -n kube-system
kubectl get apiservice v1beta1.metrics.k8s.io -o yaml

# Ver métricas
kubectl top nodes
# NAME       CPU(cores)   CPU%   MEMORY(Mi)   MEMORY%
# worker-1   500m         25%    1024Mi       50%

kubectl top pods -n production
# NAME          CPU(cores)   MEMORY(Mi)
# app-pod       100m         256Mi
# db-pod        200m         512Mi

# Ver métricas de un namespace específico
kubectl top pods -n kube-system --sort-by=memory
----
]

==== 9.1.2 Fuentes de Métricas

**Kubelet** (en cada Node) expone métricas:

[source,bash]
----
# Ver endpoint de métricas (puerto 10250)
curl -k --cert /etc/kubernetes/pki/apiserver.crt \
     --key /etc/kubernetes/pki/apiserver.key \
     https://10.0.0.10:10250/metrics

# Métricas expuestas:
# - container_cpu_usage_seconds_total
# - container_memory_usage_bytes
# - container_network_transmit_bytes_total
# - pod_network_receive_bytes_total
# - etc.
----
]

==== 9.1.3 Scrapers de Métricas Personalizadas

Exponer métricas desde tu aplicación:

[source,yaml]
----
# Deployment con actuator para Prometheus
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-app
  namespace: production
spec:
  replicas: 1
  selector:
    matchLabels:
      app: metrics-app
  template:
    metadata:
      labels:
        app: metrics-app
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: app
        image: my-app:1.0
        
        ports:
        - containerPort: 8080
          name: metrics
        
        env:
        - name: MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE
          value: "health,metrics,prometheus"
        
        livenessProbe:
          httpGet:
            path: /actuator/health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10

---
# Service con Service Monitor (para Prometheus)
apiVersion: v1
kind: Service
metadata:
  name: metrics-app
  namespace: production
  labels:
    app: metrics-app
spec:
  type: ClusterIP
  selector:
    app: metrics-app
  ports:
  - port: 8080
    targetPort: 8080
    name: metrics
----
]

[source,bash]
----
# Verificar que la aplicación expone métricas
kubectl port-forward -n production svc/metrics-app 8080:8080
curl http://localhost:8080/metrics | head -20

# OUTPUT:
# # HELP jvm_memory_used_bytes The amount of used memory
# # TYPE jvm_memory_used_bytes gauge
# jvm_memory_used_bytes{area="heap"} 256000000
# ...
----
]

---

=== 9.2 Prometheus

==== 9.2.1 Arquitectura de Prometheus

```
┌────────────────────────────────────────┐
│   Prometheus Server                    │
│  ┌──────────────────────────────────┐  │
│  │ Time Series Database (TSDB)      │  │
│  │ Almacena: métrica, labels, valor │  │
│  │ Ejemplo: cpu_usage{pod=app}=50%  │  │
│  └──────────────────────────────────┘  │
│  ┌──────────────────────────────────┐  │
│  │ Alert Manager                    │  │
│  │ Envia alertas (email, Slack, etc)│  │
│  └──────────────────────────────────┘  │
└────────────────────────────────────────┘
         ↑                    ↓
    PULL Metrics         Send Alerts
         ↑                    ↓
   ┌──────────┐         ┌──────────┐
   │ Kubelet  │         │ AlertMgr │
   │Exporters │         │ Routes   │
   │ Apps     │         │ Webhooks │
   └──────────┘         └──────────┘
```

==== 9.2.2 Instalación de Prometheus

[source,bash]
----
# Opción 1: Con Helm (recomendado)
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring --create-namespace \
  --set prometheus.prometheusSpec.retention=7d

# Verificar instalación
kubectl get pods -n monitoring
# prometheus-kube-prom-prometheus-0
# prometheus-grafana-xxxxx
# prometheus-kube-prom-operator-xxxxx

# Acceder a Prometheus (port-forward)
kubectl port-forward -n monitoring svc/prometheus-kube-prom-prometheus 9090:9090
# http://localhost:9090
----
]

==== 9.2.3 Configuración de Prometheus

[source,yaml]
----
# ConfigMap con scrape config
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s

    # AlertManager config
    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager:9093

    rule_files:
    - '/etc/prometheus/rules/*.yml'

    scrape_configs:
    
    # Kubernetes API Server
    - job_name: 'kubernetes-apiservers'
      kubernetes_sd_configs:
      - role: endpoints
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https

    # Kubernetes Nodes
    - job_name: 'kubernetes-nodes'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)

    # Kubernetes Pods (con anotaciones)
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name

    # aplicaciones custom
    - job_name: 'custom-app'
      scrape_interval: 30s
      static_configs:
      - targets: ['app-metrics:8080']
        labels:
          app: 'my-app'
----
]

==== 9.2.4 PromQL (Prometheus Query Language)

Querías básicas:

[source,bash]
----
# En Prometheus UI: http://localhost:9090

# 1. Métrica simple
container_cpu_usage_seconds_total

# 2. Con filtro de label
container_cpu_usage_seconds_total{pod="app-pod"}

# 3. Multiplos labels
container_cpu_usage_seconds_total{pod="app-pod", namespace="production"}

# 4. Rate (cambio por segundo, últimos 5 minutos)
rate(container_cpu_usage_seconds_total[5m])

# 5. Aumento (incremento total)
increase(http_requests_total[5m])

# 6. Promedio
avg(container_memory_usage_bytes)

# 7. Percentil
histogram_quantile(0.95, http_request_duration_seconds_bucket)

# 8. Sum by labels
sum(rate(http_requests_total[5m])) by (status_code)

# 9. Top 5 pods por CPU
topk(5, container_cpu_usage_seconds_total)

# 10. Predicción (trending)
predict_linear(disk_free_bytes[1h], 3600)  # Predicción 1 hora
----
]

==== 9.2.5 Alert Rules

[source,yaml]
----
# PrometheusRule
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: app-alerts
  namespace: monitoring
spec:
  groups:
  
  # Grupo 1: CPU
  - name: cpu.rules
    interval: 30s
    rules:
    - alert: HighCPUUsage
      expr: rate(container_cpu_usage_seconds_total[5m]) > 0.8
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage: {{ $labels.pod }}"
        description: "Pod {{ $labels.pod }} CPU: {{ $value | humanizePercentage }}"
    
    - alert: CriticalCPUUsage
      expr: rate(container_cpu_usage_seconds_total[5m]) > 0.95
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Critical CPU: {{ $labels.pod }}"
        description: "Pod CPU: {{ $value | humanizePercentage }}"

  # Grupo 2: Memoria
  - name: memory.rules
    interval: 30s
    rules:
    - alert: HighMemoryUsage
      expr: container_memory_usage_bytes / container_spec_memory_limit_bytes > 0.8
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High memory: {{ $labels.pod }}"
        description: "Memory {{ $value | humanizePercentage }}"
    
    - alert: OOMKilled
      expr: increase(container_last_seen[1m]) == 0 and container_memory_usage_bytes > 0
      for: 1m
      labels:
        severity: critical

  # Grupo 3: Disponibilidad
  - name: availability.rules
    interval: 30s
    rules:
    - alert: PodNotReady
      expr: kube_pod_status_ready{condition="false"} == 1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Pod not ready: {{ $labels.pod }}"
    
    - alert: DeploymentReplicasMismatch
      expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
      for: 5m
      labels:
        severity: warning
----
]

==== 9.2.6 AlertManager

Enrutar alertas a diferentes canales:

[source,yaml]
----
# AlertManager config
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

    route:
      # Receptor por defecto
      receiver: 'default'
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h

      # Sub-rutas
      routes:
      
      # Alertas críticas → oncall team
      - match:
          severity: critical
        receiver: 'oncall-pagerduty'
        continue: true
        group_wait: 0s
      
      # Warnings → engineering channel
      - match:
          severity: warning
        receiver: 'slack-engineering'
        group_wait: 30s
      
      # Database alerts → DBA
      - match:
          component: database
        receiver: 'email-dba'

    receivers:
    
    # Default: Slack
    - name: 'default'
      slack_configs:
      - channel: '#alerts'
        title: 'Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    
    # PagerDuty (oncall)
    - name: 'oncall-pagerduty'
      pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_KEY'
        description: '{{ .GroupLabels.alertname }}'
    
    # Engineering channel
    - name: 'slack-engineering'
      slack_configs:
      - channel: '#engineering'
        title: 'Warning: {{ .GroupLabels.alertname }}'
    
    # Email DBA
    - name: 'email-dba'
      email_configs:
      - to: 'dba@company.com'
        from: 'alertmanager@company.com'
        smarthost: 'smtp.company.com:587'
        auth_username: 'alertmanager'
        auth_password: 'password'
        headers:
          Subject: 'Database Alert: {{ .GroupLabels.alertname }}'

    inhibit_rules:
    # Si hay alerta crítica, suprimir warning del mismo pod
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['pod', 'namespace']
----
]

---

=== 9.3 Grafana

==== 9.3.1 Instalación de Grafana

[source,bash]
----
# Ya incluido en el Helm chart de Prometheus
# O instalarlo específicamente:

helm repo add grafana https://grafana.github.io/helm-charts
helm install grafana grafana/grafana \
  --namespace monitoring \
  --set adminPassword=admin \
  --set persistence.enabled=true

# Port-forward
kubectl port-forward -n monitoring svc/grafana 3000:80
# http://localhost:3000
# Admin / admin

# Obtener contraseña
kubectl get secret grafana -n monitoring -o jsonpath="{.data.admin-password}" | base64 -d
----
]

==== 9.3.2 Conectar Prometheus como Data Source

[source,yaml]
----
# Datasource de Prometheus
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasource-prometheus
  namespace: monitoring
data:
  prometheus.yaml: |
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      access: proxy
      url: http://prometheus-kube-prom-prometheus:9090
      isDefault: true
      jsonData:
        timeInterval: 30s
      editable: true
----
]

==== 9.3.3 Dashboard Personalizado (JSON)

[source,yaml]
----
# ConfigMap con Dashboard JSON
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-app
  namespace: monitoring
data:
  app-dashboard.json: |
    {
      "dashboard": {
        "title": "Application Performance",
        "timezone": "UTC",
        "panels": [
          {
            "id": 1,
            "title": "CPU Usage",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(container_cpu_usage_seconds_total{pod=~\"$pod\"}[5m])",
                "refId": "A"
              }
            ],
            "yaxes": [
              {
                "format": "short",
                "label": "CPU (cores)"
              }
            ]
          },
          {
            "id": 2,
            "title": "Memory Usage",
            "type": "graph",
            "targets": [
              {
                "expr": "container_memory_usage_bytes{pod=~\"$pod\"} / 1024 / 1024",
                "refId": "A"
              }
            ],
            "yaxes": [
              {
                "format": "short",
                "label": "Memory (MB)"
              }
            ]
          },
          {
            "id": 3,
            "title": "Network In",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(container_network_receive_bytes_total{pod=~\"$pod\"}[5m])",
                "refId": "A"
              }
            ]
          },
          {
            "id": 4,
            "title": "Network Out",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(container_network_transmit_bytes_total{pod=~\"$pod\"}[5m])",
                "refId": "A"
              }
            ]
          },
          {
            "id": 5,
            "title": "HTTP Requests",
            "type": "stat",
            "targets": [
              {
                "expr": "sum(rate(http_requests_total{pod=~\"$pod\"}[5m])) by (status_code)",
                "refId": "A"
              }
            ]
          },
          {
            "id": 6,
            "title": "Request Duration (p95)",
            "type": "graph",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, http_request_duration_seconds_bucket{pod=~\"$pod\"})",
                "refId": "A"
              }
            ]
          }
        ],
        "templating": {
          "list": [
            {
              "name": "pod",
              "type": "query",
              "datasource": "Prometheus",
              "query": "label_values(container_cpu_usage_seconds_total, pod)",
              "includeAll": true,
              "multi": true,
              "allValue": ".*"
            }
          ]
        }
      }
    }
----
]

==== 9.3.4 Alertas en Grafana

[source,yaml]
----
# Notification Channel
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-notification-slack
  namespace: monitoring
data:
  slack-notification.yaml: |
    apiVersion: 1
    notificationChannels:
    - name: Slack
      type: slack
      uid: slack-channel
      isDefault: false
      settings:
        url: https://hooks.slack.com/services/YOUR/WEBHOOK
        channel: "#alerts"
        username: "Grafana Bot"
        mentionGroups: "@oncall"

---
# Alert Rule en Grafana
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-alert-rule
  namespace: monitoring
data:
  alert-rule.yaml: |
    uid: app-cpu-alert
    title: High CPU Alert
    condition: B
    data:
    - refId: A
      queryType: ""
      relativeTimeRange:
        from: 10m
        to: now
      datasourceUid: prometheus-uid
      model:
        expr: 'rate(container_cpu_usage_seconds_total[5m]) > 0.8'
    - refId: B
      queryType: ""
      datasourceUid: "-100"
      model:
        conditions:
        - evaluator:
            params:
            - 0
            type: gt
          operator:
            type: and
          query:
            params:
            - A
          reducer:
            params: []
            type: last
          type: query
        datasourceUid: "-100"
        expression: A
    noDataState: NoData
    execErrState: Alerting
    for: 5m
    annotations:
      description: Pod CPU usage is > 80%
      summary: High CPU Alert
    labels:
      severity: warning
----
]

---

=== 9.4 ELK Stack: Elasticsearch, Logstash, Kibana

==== 9.4.1 Arquitectura de Logging

```
Pod Logs → Fluentd → Elasticsearch → Kibana
                            ↑
                         (Search)
```

==== 9.4.2 Instalación de ELK Stack

[source,bash]
----
# Helm chart para ECK (Elasticsearch Cloud on Kubernetes)
helm repo add elastic https://helm.elastic.co
helm repo update

# Instalar operator
helm install elastic-operator elastic/eck-operator \
  --namespace elastic-system --create-namespace

# Crear Elasticsearch cluster
cat > elasticsearch.yaml << EOF
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: quickstart
  namespace: elastic-system
spec:
  version: 8.10.0
  nodeSets:
  - name: default
    count: 1
    config:
      node.store.allow_mmap: false
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            requests:
              memory: 512Mi
              cpu: 250m
            limits:
              memory: 2Gi
              cpu: 1000m
EOF

kubectl apply -f elasticsearch.yaml

# Ver Elasticsearch
kubectl get elasticsearch -n elastic-system
kubectl port-forward -n elastic-system svc/quickstart-es-http 9200:9200
----
]

==== 9.4.3 Fluentd para Recolectar Logs

[source,yaml]
----
# ConfigMap para Fluentd
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: logging
data:
  fluent.conf: |
    <source>
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    <filter kubernetes.**>
      @type kubernetes_metadata
      kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://' + ENV.fetch('KUBERNETES_SERVICE_HOST') + ':' + ENV.fetch('KUBERNETES_SERVICE_PORT') + '/api'}"
      verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
      ca_file "#{ENV['KUBERNETES_CA_FILE']}"
    </filter>

    <match **>
      @type elasticsearch
      @id output_elasticsearch
      @log_level info
      include_tag_key true
      host elasticsearch.elastic-system.svc.cluster.local
      port 9200
      path /
      logstash_format true
      logstash_prefix kubernetes
      <buffer>
        @type file
        path /var/log/fluentd-buffers/kubernetes.system.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_interval 5s
        retry_forever false
        retry_max_interval 30
        chunk_limit_size "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE'] || '8M'}"
      </buffer>
    </match>

---
# DaemonSet para Fluentd
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      serviceAccountName: fluentd
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch.elastic-system.svc.cluster.local"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        - name: FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX
          value: "kubernetes"
        
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: config
          mountPath: /fluentd/etc/fluent.conf
          subPath: fluent.conf
      
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config
        configMap:
          name: fluentd-config

---
# ServiceAccount y RBAC para Fluentd
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: logging

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - namespaces
  verbs:
  - get
  - list
  - watch

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluentd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: fluentd
subjects:
- kind: ServiceAccount
  name: fluentd
  namespace: logging
----
]

==== 9.4.4 Kibana

[source,bash]
----
# Crear Kibana
cat > kibana.yaml << EOF
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: quickstart
  namespace: elastic-system
spec:
  version: 8.10.0
  count: 1
  elasticsearchRef:
    name: quickstart
  podTemplate:
    spec:
      containers:
      - name: kibana
        resources:
          requests:
            memory: 512Mi
            cpu: 250m
          limits:
            memory: 1Gi
            cpu: 500m
EOF

kubectl apply -f kibana.yaml

# Ver Kibana
kubectl get kibana -n elastic-system

# Port-forward
kubectl port-forward -n elastic-system svc/quickstart-kb-http 5601:5601
# http://localhost:5601
----
]

==== 9.4.5 Búsqueda en Kibana (KQL)

[source,bash]
----
# Kibana Query Language (KQL) - búsquedas comunes

# 1. Pod específico
kubernetes.pod_name : "app-pod"

# 2. Namespace específico
kubernetes.namespace_name : "production"

# 3. Nivel de log
level : "ERROR"

# 4. Rango de tiempo
@timestamp >= now-1h

# 5. Combinaciones
kubernetes.pod_name : "app-*" AND level : "ERROR"

# 6. Mensajes que contengan
message : "*timeout*"

# 7. Excluir
NOT kubernetes.pod_name : "debug-pod"

# 8. Status codes
http.status_code : 500

# 9. CPU alto
system.cpu.system.pct > 0.8

# 10. Análisis por Pod
kibana.alert.rule.uuid : "*" | stats count() by kubernetes.pod_name
----
]

---

=== 9.5 Distributed Tracing con Jaeger

==== 9.5.1 Concepto

Tracer el camino de una request a través del cluster:

```
Cliente → API Gateway → Microservicio A → Microservicio B → Base de datos
          [Span 1]         [Span 2]         [Span 3]         [Span 4]
```

==== 9.5.2 Instalación de Jaeger

[source,bash]
----
# Helm chart para Jaeger
helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
helm repo update

helm install jaeger jaegertracing/jaeger \
  --namespace observability --create-namespace \
  --set collector.service.type=ClusterIP

# Verificar instalación
kubectl get pods -n observability | grep jaeger

# Port-forward
kubectl port-forward -n observability svc/jaeger 6831:6831/udp
kubectl port-forward -n observability svc/jaeger-query 16686:16686
# http://localhost:16686
----
]

==== 9.5.3 Instrumentar Aplicación (OpenTelemetry)

[source,yaml]
----
# Pod con OpenTelemetry agent
apiVersion: apps/v1
kind: Deployment
metadata:
  name: traced-app
  namespace: production
spec:
  replicas: 1
  selector:
    matchLabels:
      app: traced-app
  template:
    metadata:
      labels:
        app: traced-app
    spec:
      containers:
      - name: app
        image: my-traced-app:1.0
        
        ports:
        - containerPort: 8080
        
        # Variables de entorno para Jaeger
        env:
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "http://jaeger-collector:4318"
        - name: OTEL_EXPORTER_OTLP_HEADERS
          value: "authorization=Bearer token123"
        - name: OTEL_SERVICE_NAME
          value: "traced-app"
        - name: OTEL_TRACES_SAMPLER
          value: "always_on"
        - name: OTEL_TRACES_SAMPLER_ARG
          value: "0.1"  # 10% sampling
        - name: OTEL_RESOURCE_ATTRIBUTES
          value: "service.namespace=production,service.version=1.0"

---
# Service para tracing
apiVersion: v1
kind: Service
metadata:
  name: traced-app
  namespace: production
spec:
  type: ClusterIP
  selector:
    app: traced-app
  ports:
  - port: 8080
    targetPort: 8080
----
]

==== 9.5.4 Análisis en Jaeger

[source,bash]
----
# En Jaeger UI: http://localhost:16686

# 1. Buscar por service
Service: traced-app

# 2. Ver traces de operación
Operation: GET /api/users

# 3. Filter por tags
tags: http.status_code=200

# 4. Análisis de latencia
duration > 100ms

# 5. Error traces
tags: error=true

# 6. Ver spans
Click en trace → expandir spans → ver duración, tags, logs
----
]

---

=== 9.6 Observabilidad Completa: Stack Integrado

[source,yaml]
----
# Namespace para observabilidad
apiVersion: v1
kind: Namespace
metadata:
  name: observability
  labels:
    pod-security.kubernetes.io/enforce: baseline

---
# ServiceAccount para apps
apiVersion: v1
kind: ServiceAccount
metadata:
  name: app-observability
  namespace: observability

---
# Deployment con todas las características
apiVersion: apps/v1
kind: Deployment
metadata:
  name: observable-app
  namespace: observability
spec:
  replicas: 2
  selector:
    matchLabels:
      app: observable-app
  template:
    metadata:
      labels:
        app: observable-app
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: app-observability
      
      containers:
      - name: app
        image: observable-app:1.0
        imagePullPolicy: Always
        
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 8081
          name: metrics
        
        env:
        # Prometheus metrics
        - name: METRICS_PORT
          value: "8081"
        
        # Jaeger tracing
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "http://jaeger-collector.observability:4318"
        - name: OTEL_SERVICE_NAME
          value: "observable-app"
        - name: OTEL_TRACES_SAMPLER
          value: "parentbased_traceidratio"
        - name: OTEL_TRACES_SAMPLER_ARG
          value: "0.1"
        
        # Logging structured
        - name: LOG_LEVEL
          value: "INFO"
        - name: LOG_FORMAT
          value: "json"
        
        # Resource metadata
        - name: OTEL_RESOURCE_ATTRIBUTES
          value: "service.namespace=observability,service.instance.id=$(POD_NAME)"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
        
        volumeMounts:
        - name: logs
          mountPath: /var/log
      
      volumes:
      - name: logs
        emptyDir: {}

---
# Service
apiVersion: v1
kind: Service
metadata:
  name: observable-app
  namespace: observability
  labels:
    app: observable-app
spec:
  type: ClusterIP
  selector:
    app: observable-app
  ports:
  - port: 8080
    targetPort: 8080
    name: http
  - port: 8081
    targetPort: 8081
    name: metrics

---
# ServiceMonitor para Prometheus
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: observable-app
  namespace: observability
spec:
  selector:
    matchLabels:
      app: observable-app
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics

---
# PrometheusRule para alertas
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: observable-app-alerts
  namespace: observability
spec:
  groups:
  - name: observable-app
    interval: 30s
    rules:
    - alert: AppHighLatency
      expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.5
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "App latency is high"
        description: "p95 latency: {{ $value }}s"
    
    - alert: AppHighErrorRate
      expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.01
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "App error rate is high"
        description: "Error rate: {{ $value | humanizePercentage }}"
----
]

---

=== 9.7 Best Practices

**1. Retention Policies**
```yaml
❌ Guardar todo indefinidamente (caro)
✅ Prometheus: 7-30 días de retención
✅ Elasticsearch: rotación por índices (1 día)
✅ Archiving a S3/GCS para análisis histórico
```

**2. Sampling**
```yaml
❌ Tracer el 100% de requests (sobrecarga)
✅ Sampling inteligente:
   - 100% para errores
   - 10% para requests normales
   - 1% para background tasks
```

**3. Labels Importantes**
```yaml
✅ Pod name, namespace, cluster
✅ Service, environment, version
✅ Http status, error type
❌ Datos sensibles (passwords, tokens)
```

**4. Alertas Significativas**
```yaml
❌ Alerta por cada Pod caído
✅ Alerta por multiple Pod failures
✅ Alerta por error rate > threshold
```

**5. Dashboards**
```yaml
✅ Una métrica por panel (no abarrotado)
✅ Usar color rojo para warnings
✅ Incluir contexto (avg, p95, max)
```

---

=== 9.8 Resumen del Módulo 9

En este módulo aprendiste:

1. **Metrics Server**: Recolección de métricas de CPU/memoria
   - Metrics Server architecture
   - Kubectl top pods/nodes
   - Custom metrics exporters

2. **Prometheus**: Sistema de alerting basado en métricas
   - TSDB (Time Series Database)
   - PromQL queries
   - Alert rules
   - AlertManager routing

3. **Grafana**: Visualización de datos
   - Dashboards personalizados
   - Datasources
   - Alertas en Grafana

4. **ELK Stack**: Agregación de logs
   - Elasticsearch (almacenamiento)
   - Kibana (búsqueda)
   - Fluentd (recolección)
   - KQL queries

5. **Jaeger**: Distributed tracing
   - OpenTelemetry instrumentation
   - Análisis de latencia
   - Debugging de microservicios

6. **Observabilidad Completa**: Integración de todos los pillares

Con estos conocimientos, estás listo para aprender sobre **GitOps y CI/CD** en el Módulo 10.

---

== MÓDULO 10: GitOps y CI/CD

**GitOps** es el conjunto de prácticas para gestionar infraestructura y aplicaciones usando Git como fuente única de verdad.

**Diferencia clave:**
```
Imperativo (Tradicional)          Declarativo (GitOps)
                                  
kubectl apply ...                 git push
kubectl scale ...        VS       git merge
kubectl set image ...             (Reconciliación automática)
kubectl delete ...        
```

**Flujo GitOps:**
```
┌─────────────┐
│  Git Repo   │
│ (Source of  │
│   Truth)    │
└─────────────┘
       ↑
       │ (Push o Pull)
       │
┌──────────────────────────────────┐
│ GitOps Operator (Flux/ArgoCD)   │
│ - Monitorea cambios en Git       │
│ - Aplica cambios automáticamente │
│ - Sincroniza estado real vs Git  │
└──────────────────────────────────┘
       ↓
┌─────────────────────┐
│ Kubernetes Cluster  │
│ (Estado deseado)    │
└─────────────────────┘
```

---

=== 10.1 Conceptos Fundamentales

==== 10.1.1 Git como Fuente de Verdad

Todo debe estar en Git:

```yaml
❌ Manejar cluster manualmente
❌ kubectl apply sin guardar en Git
❌ Secrets hardcodeados en YAML

✅ Todo versionado en Git
✅ Auditoría completa de cambios
✅ Secrets encriptados (Sealed Secrets, SOPS)
✅ Rollback a cualquier commit
```

==== 10.1.2 Declarativo vs Imperativo

**Imperativo** (orden qué hacer):
```bash
kubectl set image deployment/app app=myapp:v2
kubectl scale deployment app --replicas=3
```

**Declarativo** (describir estado deseado):
```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: app
        image: myapp:v2
```

GitOps usa **declarativo** con repositorios Git.

==== 10.1.3 Push vs Pull

**Push Model (CI/CD tradicional):**
```
Git Push → Jenkins → Build → Push to Registry → kubectl apply
(problema: si Jenkins falla, cluster desincronizado)
```

**Pull Model (GitOps, recomendado):**
```
Git Push → Operator en cluster (Flux/ArgoCD)
                ↓
           Monitorea cambios
                ↓
           kubectl apply automático
(ventaja: operador siempre sincroniza)
```

---

=== 10.2 Flux CD

==== 10.2.1 Instalación de Flux

[source,bash]
----
# Instalar Flux CLI
curl -s https://fluxcd.io/install.sh | sudo bash

# Verificar instalación
flux --version

# Instalar Flux en cluster
flux bootstrap github \
  --owner=<GITHUB_USER> \
  --repo=<REPO_NAME> \
  --path=clusters/production \
  --personal \
  --private=false

# Flux crea:
# - Namespace flux-system
# - Controllers (source-controller, kustomize-controller, helm-controller)
# - Secret con credenciales Git

# Verificar
kubectl get pods -n flux-system
----
]

==== 10.2.2 GitRepository (Monitorear cambios)

[source,yaml]
----
# GitRepository: Monitorear repositorio Git
apiVersion: source.toolkit.fluxcd.io/v1
kind: GitRepository
metadata:
  name: app-repo
  namespace: flux-system
spec:
  interval: 30s  # Comprobar cambios cada 30s
  url: https://github.com/my-company/app-config
  ref:
    branch: main
  
  # Usar token para repos privados
  secretRef:
    name: github-credentials
  
  # Ignorar archivos
  ignore: |
    # Flux ignora estos archivos
    /.*
    /**/*.md
    **/charts/**

---
# Secret con credenciales (GitHub token)
apiVersion: v1
kind: Secret
metadata:
  name: github-credentials
  namespace: flux-system
stringData:
  username: git
  password: ghp_xxxxxxxxxxxxxxxx  # GitHub Personal Access Token
----
]

==== 10.2.3 Kustomization (Aplicar configuración)

[source,yaml]
----
# Kustomization: Aplicar configuración del Git repo
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: app-deployment
  namespace: flux-system
spec:
  interval: 10m
  
  # Repositorio fuente
  sourceRef:
    kind: GitRepository
    name: app-repo
  
  # Ruta dentro del repo
  path: ./deploy/production
  
  # Prioridad (si hay múltiples Kustomizations)
  prune: true
  wait: true
  
  # Validación
  validation: client
  
  # Salud
  healthChecks:
  - apiVersion: apps/v1
    kind: Deployment
    name: app
    namespace: production
  - apiVersion: v1
    kind: Service
    name: app
    namespace: production
  
  # Hacer antes de aplicar
  patchStrategicMergePatches:
  - target:
      kind: Deployment
      name: app
    patch: |
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: app
      spec:
        replicas: 3

---
# ConfigMap actualizado automáticamente
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: production
data:
  config.yaml: |
    environment: production
    replicas: 3
    version: "1.0"
----
]

==== 10.2.4 HelmRelease (Gestionar Helm Charts)

[source,yaml]
----
# HelmRelease: Instalar/actualizar Helm charts con Flux
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: postgres
  namespace: production
spec:
  interval: 5m
  
  # Fuente del chart
  chart:
    spec:
      chart: postgresql
      sourceRef:
        kind: HelmRepository
        name: bitnami
        namespace: flux-system
      version: "12.x"
  
  # Valores personalizados
  values:
    global:
      postgresql:
        auth:
          username: app_user
          password: secure_password
          database: app_db
    
    primary:
      persistence:
        enabled: true
        size: 10Gi
        storageClassName: fast-ssd
    
    metrics:
      enabled: true
      serviceMonitor:
        enabled: true
  
  # Estrategia de actualización
  upgrade:
    remediation:
      retries: 3
  
  # Salud
  healthChecks:
  - apiVersion: apps/v1
    kind: Deployment
    name: postgres
    namespace: production

---
# HelmRepository: Fuente de Helm charts
apiVersion: source.toolkit.fluxcd.io/v1beta2
kind: HelmRepository
metadata:
  name: bitnami
  namespace: flux-system
spec:
  interval: 10m
  url: https://charts.bitnami.com/bitnami
----
]

==== 10.2.5 Automatización de Actualizaciones

[source,yaml]
----
# ImageRepository: Monitorear nuevas imágenes
apiVersion: image.toolkit.fluxcd.io/v1beta2
kind: ImageRepository
metadata:
  name: app-images
  namespace: flux-system
spec:
  image: myregistry/app
  interval: 5m
  
  # Credenciales (si es privado)
  secretRef:
    name: registry-credentials

---
# ImagePolicy: Seleccionar qué versión usar
apiVersion: image.toolkit.fluxcd.io/v1beta2
kind: ImagePolicy
metadata:
  name: app-latest
  namespace: flux-system
spec:
  imageRepositoryRef:
    name: app-images
  
  # Estrategia: semver, alphabetical, timestamp
  policy:
    semver:
      range: '1.x'  # v1.0, v1.1, v1.2, etc

---
# ImageUpdateAutomation: Actualizar Git cuando hay nueva imagen
apiVersion: image.toolkit.fluxcd.io/v1beta1
kind: ImageUpdateAutomation
metadata:
  name: app-update
  namespace: flux-system
spec:
  interval: 5m
  
  sourceRef:
    kind: GitRepository
    name: app-repo
  
  # Qué actualizar
  update:
    strategy: Setters
    path: ./deploy
  
  # Git commit
  git:
    commit:
      author:
        email: flux@company.com
        name: Flux Automation
      messageTemplate: |
        Automated image update
        
        {{range .Updated.Images -}}
        {{println .}}
        {{- end}}

---
# En deploy/app-deployment.yaml, marcar para auto-update:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
  namespace: production
spec:
  template:
    spec:
      containers:
      - name: app
        image: myregistry/app:v1.2.3  # {image-update}
----
]

==== 10.2.6 Notificaciones

[source,yaml]
----
# Alert: Monitorear cambios en repositorio
apiVersion: notification.toolkit.fluxcd.io/v1beta3
kind: Alert
metadata:
  name: repo-changes
  namespace: flux-system
spec:
  providerRef:
    name: slack
  eventSeverity: info
  eventSources:
  - kind: GitRepository
    name: '*'
  - kind: Kustomization
    name: '*'
  - kind: HelmRelease
    name: '*'
  suspend: false

---
# Provider: Destino de notificaciones
apiVersion: notification.toolkit.fluxcd.io/v1beta3
kind: Provider
metadata:
  name: slack
  namespace: flux-system
spec:
  type: slack
  address: https://hooks.slack.com/services/YOUR/WEBHOOK/URL

---
# Receptor: Webhook para alertas
apiVersion: notification.toolkit.fluxcd.io/v1beta3
kind: Receiver
metadata:
  name: github-webhooks
  namespace: flux-system
spec:
  type: github
  secretRef:
    name: webhook-token
  resources:
  - kind: GitRepository
    name: app-repo
----
]

---

=== 10.3 ArgoCD

==== 10.3.1 Instalación de ArgoCD

[source,bash]
----
# Crear namespace
kubectl create namespace argocd

# Instalar ArgoCD
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# Verificar instalación
kubectl get pods -n argocd

# Port-forward para acceder UI
kubectl port-forward svc/argocd-server -n argocd 8080:443

# Obtener contraseña inicial
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d

# URL: https://localhost:8080
# User: admin
# Password: (del comando anterior)

# CLI
curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64
chmod +x /usr/local/bin/argocd

argocd login localhost:8080 --insecure
----
]

==== 10.3.2 Application (Declarar aplicación)

[source,yaml]
----
# ArgoCD Application
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: myapp
  namespace: argocd
spec:
  # Proyecto (namespace lógico)
  project: default
  
  # Fuente: donde está el código
  source:
    repoURL: https://github.com/my-company/app-config
    targetRevision: main
    path: deploy/production
    
    # Si usa Helm
    helm:
      releaseName: myapp
      values: |
        replicas: 3
        image:
          tag: v1.2.3
      
      # O usar file
      # valuesObject:
      #   replicas: 3
    
    # Si usa Kustomize
    # kustomize:
    #   commonLabels:
    #     version: v1
    #   replicas:
    #   - name: app
    #     count: 3
    
    # Si usa plugin
    # plugin:
    #   name: my-plugin
    #   env:
    #   - name: CONFIG
    #     value: /path/to/config
  
  # Destino: dónde deployar
  destination:
    server: https://kubernetes.default.svc  # Cluster actual
    namespace: production
  
  # Sincronización automática
  syncPolicy:
    automated:
      prune: true      # Eliminar recursos no en Git
      selfHeal: true   # Resincronizar si alguien cambia manualmente
      allowEmpty: false # No permitir borrar todos los recursos
    
    syncOptions:
    - CreateNamespace=true
    
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
  
  # Health (obligatorio que estos Pods estén ready)
  ignoreDifferences:
  - group: apps
    kind: Deployment
    jsonPointers:
    - /spec/replicas

---
# Otro ejemplo: multi-fuente
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: complex-app
  namespace: argocd
spec:
  project: default
  
  # Múltiples fuentes
  sources:
  
  # Fuente 1: Base de datos (Helm)
  - repoURL: https://charts.bitnami.com/bitnami
    targetRevision: 12.x
    chart: postgresql
    helm:
      releaseName: postgres
      values: |
        primary:
          persistence:
            size: 20Gi
  
  # Fuente 2: Aplicación (Git)
  - repoURL: https://github.com/my-company/app
    targetRevision: main
    path: deploy
  
  destination:
    server: https://kubernetes.default.svc
    namespace: production
  
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
----
]

==== 10.3.3 Sincronización

[source,bash]
----
# Ver aplicaciones
argocd app list
# NAME           CLUSTER                         STATUS      SYNC STATUS
# myapp          https://kubernetes.default.svc  Healthy     Synced

# Ver detalles
argocd app get myapp
# Name:               myapp
# Status:             Healthy
# Sync Status:        Synced
# Repo:               https://github.com/my-company/app
# Target Branch:      main
# Sync Policy:        Automated
# Last Sync:          2024-01-15 10:30:00

# Sincronizar manualmente
argocd app sync myapp

# Forzar resincronización
argocd app sync myapp --force

# Ver historial
argocd app history myapp
# VERSION   DEPLOYED AT             REVISION
# 0         2024-01-15 10:30:00     abc123
# 1         2024-01-16 14:20:00     def456

# Revertir a versión anterior
argocd app rollback myapp 0

# Actualizar valores
argocd app set myapp -p image.tag=v2.0.0
----
]

==== 10.3.4 AppProject (RBAC)

[source,yaml]
----
# AppProject: Control de acceso
apiVersion: argoproj.io/v1alpha1
kind: AppProject
metadata:
  name: team-a
  namespace: argocd
spec:
  # Descripción
  description: Team A project

  # Fuentes permitidas
  sourceRepos:
  - 'https://github.com/team-a/*'
  - 'https://charts.bitnami.com/bitnami'
  
  # Destinos permitidos
  destinations:
  - namespace: 'team-a-*'
    server: 'https://kubernetes.default.svc'
  - namespace: 'shared'
    server: 'https://kubernetes.default.svc'
  
  # Roles RBAC
  roles:
  - name: developers
    description: Developers can deploy to staging
    policies:
    - p, proj:team-a:developers, applications, get, team-a/*, allow
    - p, proj:team-a:developers, applications, sync, team-a/*, allow
  
  - name: ops
    description: Ops can manage everything
    policies:
    - p, proj:team-a:ops, applications, *, team-a/*, allow
    - p, proj:team-a:ops, repositories, *, *, allow

---
# Asociar usuario a role
apiVersion: v1
kind: ConfigMap
metadata:
  name: argocd-rbac-cm
  namespace: argocd
data:
  policy.default: |
    g, admins, role:admin
    g, team-a-developers, proj:team-a:developers
    g, team-a-ops, proj:team-a:ops
    
  policy.csv: |
    # Roles (qué puede hacer)
    p, role:developer, applications, get, team-a/*, allow
    p, role:developer, applications, sync, team-a/*, allow
    
    p, role:ops, applications, *, *, allow
    p, role:ops, repositories, *, *, allow
    
    # User mappings (OIDC/LDAP)
    g, alice@company.com, role:developer
    g, bob@company.com, role:ops
----
]

==== 10.3.5 Notifications

[source,yaml]
----
# ArgoCD Notification
apiVersion: v1
kind: ConfigMap
metadata:
  name: argocd-notifications-cm
  namespace: argocd
data:
  trigger.on-deployed: |
    - when: app.status.operationState.finishedAt != '' and app.status.operationState.phase in ['Succeeded']
      oncePer: app.status.operationState.finishedAt
      send: [app-deployed]
  
  trigger.on-health-degraded: |
    - when: app.status.health.status == 'Degraded'
      send: [app-health-degraded]
  
  template.app-deployed: |
    message: |
      {{if eq .app.status.operationState.phase "Succeeded"}}✅{{else}}❌{{end}}
      Application {{.app.metadata.name}} {{.app.status.operationState.phase}}
      {{if eq .app.status.sync.status "Synced"}}(Synced){{else}}(OutOfSync){{end}}
      {{- if .commitMessage}}
      
      {{.commitMessage}}{{end}}
  
  service.slack: |
    token: $slack-token
  
  service.email: |
    host: smtp.gmail.com
    port: 587
    from: argocd@company.com
    username: $email-username
    password: $email-password

---
# Subscription (Enviar notificaciones a Slack)
apiVersion: v1
kind: Secret
metadata:
  name: argocd-notifications-secret
  namespace: argocd
stringData:
  slack-token: xoxb-xxxxxxxxxxxxx

---
# Trigger para aplicación
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: myapp
  namespace: argocd
  annotations:
    notifications.argoproj.io/subscribe.on-deployed.slack: "#deployments"
    notifications.argoproj.io/subscribe.on-health-degraded.slack: "#alerts"
spec:
  # ... resto de config
----
]

---

=== 10.4 CI/CD Pipelines

==== 10.4.1 GitHub Actions

[source,yaml]
----
# .github/workflows/build-and-deploy.yaml
name: Build and Deploy

on:
  push:
    branches:
      - main
    paths:
      - 'src/**'
      - 'Dockerfile'
      - '.github/workflows/build-and-deploy.yaml'

jobs:
  build:
    runs-on: ubuntu-latest
    
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
      with:
        fetch-depth: 0  # Para obtener historial de commits
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Login to Docker Hub
      uses: docker/login-action@v2
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}
    
    - name: Docker meta
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ secrets.DOCKER_USERNAME }}/myapp
        tags: |
          type=ref,event=branch
          type=semver,pattern={{version}}
          type=sha,prefix=sha-
          type=raw,value=latest,enable={{is_default_branch}}
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=registry,ref=${{ secrets.DOCKER_USERNAME }}/myapp:buildcache
        cache-to: type=registry,ref=${{ secrets.DOCKER_USERNAME }}/myapp:buildcache,mode=max

  test:
    runs-on: ubuntu-latest
    needs: build
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Run tests
      run: |
        docker run --rm ${{ needs.build.outputs.image-tag }} pytest
    
    - name: Run security scan
      run: |
        docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
          aquasec/trivy image ${{ needs.build.outputs.image-tag }}

  deploy:
    runs-on: ubuntu-latest
    needs: [build, test]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout config repo
      uses: actions/checkout@v3
      with:
        repository: my-company/app-config
        token: ${{ secrets.CONFIG_REPO_TOKEN }}
        path: config
    
    - name: Update image tag in config
      working-directory: config
      run: |
        sed -i "s|image: .*|image: ${{ needs.build.outputs.image-tag }}|" deploy/production/deployment.yaml
    
    - name: Commit and push
      working-directory: config
      run: |
        git config user.email "ci@company.com"
        git config user.name "CI/CD Bot"
        git add deploy/production/deployment.yaml
        git commit -m "Update image: ${{ needs.build.outputs.image-tag }}"
        git push origin main
    
    - name: Verify deployment (opcional)
      run: |
        echo "GitOps tool (Flux/ArgoCD) detectará el cambio y desplegará"
----
]

==== 10.4.2 Tekton Pipelines

[source,yaml]
----
# Tekton: CI/CD nativo de Kubernetes
apiVersion: v1
kind: Namespace
metadata:
  name: tekton-pipelines

---
# Task: Clonar repositorio
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: git-clone
  namespace: tekton-pipelines
spec:
  params:
  - name: url
    description: Git repository URL
  - name: revision
    description: Git revision (branch, tag, commit)
    default: main
  
  results:
  - name: commit
    description: Commit SHA
  
  steps:
  - name: clone
    image: alpine/git:latest
    script: |
      #!/bin/sh
      git clone --depth 1 --branch $(params.revision) $(params.url) /workspace/repo
      cd /workspace/repo
      git rev-parse HEAD > $(results.commit.path)

---
# Task: Build Docker image
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: build-image
  namespace: tekton-pipelines
spec:
  params:
  - name: image
    description: Image name
  - name: dockerfile
    default: Dockerfile
  
  steps:
  - name: build-and-push
    image: gcr.io/kaniko-project/executor:latest
    args:
    - --destination=$(params.image)
    - --dockerfile=/workspace/repo/$(params.dockerfile)
    - --context=/workspace/repo

---
# Task: Run tests
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: run-tests
  namespace: tekton-pipelines
spec:
  steps:
  - name: test
    image: python:3.11
    workingDir: /workspace/repo
    script: |
      #!/bin/bash
      pip install -r requirements.txt
      pytest tests/

---
# Pipeline: Orquestar tasks
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: build-and-deploy
  namespace: tekton-pipelines
spec:
  params:
  - name: git-url
  - name: git-revision
  - name: image-name
  
  tasks:
  
  # Task 1: Clone
  - name: clone
    taskRef:
      name: git-clone
    params:
    - name: url
      value: $(params.git-url)
    - name: revision
      value: $(params.git-revision)
  
  # Task 2: Test (dependencia en clone)
  - name: test
    taskRef:
      name: run-tests
    runAfter:
    - clone
  
  # Task 3: Build (dependencia en test)
  - name: build
    taskRef:
      name: build-image
    runAfter:
    - test
    params:
    - name: image
      value: $(params.image-name)

---
# PipelineRun: Ejecutar pipeline
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  name: build-app-run
  namespace: tekton-pipelines
spec:
  pipelineRef:
    name: build-and-deploy
  
  params:
  - name: git-url
    value: https://github.com/my-company/app
  - name: git-revision
    value: main
  - name: image-name
    value: myregistry/app:v1.2.3
  
  workspaces:
  - name: shared-workspace
    volumeClaimTemplate:
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: 1Gi
----
]

==== 10.4.3 Webhook para CI/CD automático

[source,yaml]
----
# EventListener: Recibir webhooks de Git
apiVersion: triggers.tekton.dev/v1beta1
kind: EventListener
metadata:
  name: github-push
  namespace: tekton-pipelines
spec:
  serviceAccountName: tekton-trigger-sa
  
  triggers:
  - name: github-trigger
    interceptors:
    - ref:
        name: "github"
      params:
      - name: "secretRef"
        value:
          secretName: github-webhook-secret
          secretKey: token
    
    bindings:
    - ref: github-binding
    
    template:
      ref: pipeline-template

---
# TriggerBinding: Extraer datos del webhook
apiVersion: triggers.tekton.dev/v1beta1
kind: TriggerBinding
metadata:
  name: github-binding
  namespace: tekton-pipelines
spec:
  params:
  - name: git-url
    value: $(body.repository.clone_url)
  - name: git-revision
    value: $(body.ref)
  - name: pr-number
    value: $(body.number)

---
# TriggerTemplate: Crear PipelineRun
apiVersion: triggers.tekton.dev/v1beta1
kind: TriggerTemplate
metadata:
  name: pipeline-template
  namespace: tekton-pipelines
spec:
  params:
  - name: git-url
  - name: git-revision
  
  resourcetemplates:
  - apiVersion: tekton.dev/v1
    kind: PipelineRun
    metadata:
      generateName: build-app-
      namespace: tekton-pipelines
    spec:
      pipelineRef:
        name: build-and-deploy
      
      params:
      - name: git-url
        value: $(tt.params.git-url)
      - name: git-revision
        value: $(tt.params.git-revision)
      - name: image-name
        value: myregistry/app:latest

---
# Secret para webhook
apiVersion: v1
kind: Secret
metadata:
  name: github-webhook-secret
  namespace: tekton-pipelines
stringData:
  token: ghp_xxxxxxxxxxxxxxxx

---
# RBAC para Tekton triggers
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tekton-trigger-sa
  namespace: tekton-pipelines

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: tekton-triggers-role
rules:
- apiGroups: ["tekton.dev"]
  resources: ["pipelineruns", "taskruns"]
  verbs: ["create", "get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tekton-triggers-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: tekton-triggers-role
subjects:
- kind: ServiceAccount
  name: tekton-trigger-sa
  namespace: tekton-pipelines
----
]

---

=== 10.5 Versionado y Releases

==== 10.5.1 Semantic Versioning

```
MAJOR.MINOR.PATCH
  ↓     ↓      ↓
  2.    1.     3

MAJOR: Breaking changes (1.0 → 2.0)
MINOR: New features, backwards compatible (2.0 → 2.1)
PATCH: Bug fixes (2.1 → 2.1.1)

Ejemplos:
1.0.0  → 1.0.1  (bug fix)
1.0.1  → 1.1.0  (new feature)
1.1.0  → 2.0.0  (breaking change)
```

==== 10.5.2 Changelog Automático

[source,bash]
----
# Usar conventional commits
# Format: <type>(<scope>): <subject>
#
# type: feat, fix, docs, style, refactor, test, chore
# scope: optional, e.g., "api", "web", "db"
# subject: brief description

# Ejemplos:
# feat(api): add user endpoint
# fix(web): resolve navbar styling issue
# docs(readme): update installation steps

# Tool: standard-version (genera changelog)
npm install -g standard-version

# Crear versión (actualiza package.json, CHANGELOG.md, git tag)
standard-version --release-as minor

# Ver cambios
git log --oneline | head -10

# Publicar
git push origin main
git push origin v1.1.0
----
]

==== 10.5.3 GitHub Releases

[source,yaml]
----
# .github/workflows/release.yaml
name: Release

on:
  push:
    tags:
      - 'v*'

jobs:
  release:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout
      uses: actions/checkout@v3
      with:
        fetch-depth: 0
    
    - name: Get changelog
      id: changelog
      run: |
        CHANGELOG=$(git log --pretty=format:"%h - %s" $(git describe --tags --abbrev=0 HEAD~1)..HEAD)
        echo "CHANGELOG=$CHANGELOG" >> $GITHUB_OUTPUT
    
    - name: Create Release
      uses: actions/create-release@v1
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        tag_name: ${{ github.ref }}
        release_name: Release ${{ github.ref }}
        body: ${{ steps.changelog.outputs.CHANGELOG }}
        draft: false
        prerelease: false
    
    - name: Build and push image
      run: |
        docker build -t myregistry/app:${{ github.ref }} .
        docker push myregistry/app:${{ github.ref }}
    
    - name: Update deployment config
      run: |
        # Actualizar imagen en config repo
        sed -i "s|image: .*|image: myregistry/app:${{ github.ref }}|" deploy/deployment.yaml
        git config user.email "ci@company.com"
        git config user.name "CI/CD Bot"
        git add deploy/deployment.yaml
        git commit -m "Update image for release: ${{ github.ref }}"
        git push https://github.com/my-company/app-config main
----
]

---

=== 10.6 Ejemplo Completo: GitOps Workflow

[source,yaml]
----
# Estructura Git repo:
# app-config/
# ├── clusters/
# │   ├── production/
# │   │   ├── kustomization.yaml
# │   │   └── apps/
# │   │       ├── app-deployment.yaml
# │   │       ├── app-service.yaml
# │   │       └── app-configmap.yaml
# │   └── staging/
# └── apps/
#     └── app/
#         ├── kustomization.yaml
#         └── base/
#             ├── deployment.yaml
#             ├── service.yaml
#             └── configmap.yaml

# 1. Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: production

---
# 2. Aplicación (Flux)
apiVersion: source.toolkit.fluxcd.io/v1
kind: GitRepository
metadata:
  name: app-config
  namespace: flux-system
spec:
  interval: 30s
  url: https://github.com/my-company/app-config
  ref:
    branch: main

---
# 3. Sincronizar con Kustomization
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: app-production
  namespace: flux-system
spec:
  interval: 10m
  sourceRef:
    kind: GitRepository
    name: app-config
  path: ./clusters/production
  prune: true
  wait: true

---
# 4. Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: app
  template:
    metadata:
      labels:
        app: app
    spec:
      containers:
      - name: app
        image: myregistry/app:v1.2.3
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi

---
# 5. Service
apiVersion: v1
kind: Service
metadata:
  name: app
  namespace: production
spec:
  type: LoadBalancer
  selector:
    app: app
  ports:
  - port: 8080
    targetPort: 8080

---
# 6. ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: production
data:
  app.yaml: |
    environment: production
    replicas: 3
    log_level: info

---
# Flujo:
# 1. Cambio en código → Push a main
# 2. GitHub Actions build imagen + tests
# 3. Imagen pushed a registry
# 4. Actions actualiza image tag en app-config repo
# 5. Flux detecta cambio en Git (cada 30s)
# 6. Flux aplica cambios automáticamente
# 7. ArgoCD sincroniza (si está configurado)
# 8. Cluster en estado deseado
----
]

---

=== 10.7 Best Practices

**1. Git como única fuente de verdad**
```yaml
✅ Todo versionado en Git
✅ Commits atómicos con mensajes descriptivos
✅ PRs para cambios (no push directo a main)
❌ Cambios manuales con kubectl
```

**2. Separar CI y CD**
```yaml
CI Pipeline:
  - Clone repo
  - Build
  - Test
  - Scan (seguridad)
  - Push image

CD Pipeline (GitOps):
  - Flux/ArgoCD detecta Git change
  - Aplica cambios automáticamente
```

**3. Secrets management**
```yaml
❌ Secrets en repositorio
✅ Sealed Secrets / SOPS
✅ Vault para secrets
✅ CI secret management (GitHub Secrets, etc)
```

**4. Deployments sin downtime**
```yaml
apiVersion: apps/v1
kind: Deployment
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    spec:
      terminationGracePeriodSeconds: 30
```

**5. Health checks y rollback automático**
```yaml
✅ Liveness probes
✅ Readiness probes
✅ Kustomization healthChecks
✅ ArgoCD auto-rollback si falla
```

---

=== 10.8 Resumen del Módulo 10

En este módulo aprendiste:

1. **GitOps Fundamentals**:
   - Git como fuente única de verdad
   - Declarativo vs Imperativo
   - Push vs Pull model

2. **Flux CD**:
   - GitRepository para monitorear cambios
   - Kustomization para aplicar configuración
   - HelmRelease para gestionar Helm charts
   - Automatización de actualizaciones de imagen
   - Notificaciones (Slack, Email, etc)

3. **ArgoCD**:
   - Instalación y configuración
   - Applications y sincronización
   - AppProject para RBAC
   - Health checks y rollback

4. **CI/CD Pipelines**:
   - GitHub Actions (build, test, push, deploy)
   - Tekton Pipelines (CI/CD nativo de Kubernetes)
   - Webhooks para automatización

5. **Versionado y Releases**:
   - Semantic Versioning
   - Conventional commits
   - Changelog automático
   - GitHub Releases

6. **GitOps Workflow Completo**: Integración end-to-end

Con estos conocimientos, estás listo para aprender sobre **Helm** en el Módulo 11.

---

== MÓDULO 11: Helm Package Manager

**Helm** es el package manager de Kubernetes. Como apt para Linux, npm para Node.js, pip para Python.

```
┌─────────────────────────────────────────┐
│  Helm: Package Manager para Kubernetes  │
├─────────────────────────────────────────┤
│                                         │
│ Chart (package)  →  Release (instancia)│
│  nginx.tgz                 nginx-prod  │
│  postgres.tgz              postgres-dev│
│  prometheus.tgz            prom-1      │
│                                         │
│ Como: .deb → instalación en Linux      │
│       .rar → Release en github          │
└─────────────────────────────────────────┘
```

---

=== 11.1 Helm Basics

==== 11.1.1 Instalación

[source,bash]
----
# Descargar e instalar Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# Verificar instalación
helm version
# version.BuildInfo{Version:"v3.13.0", GitCommit:"..."}

# Completar shell
helm completion bash | sudo tee /etc/bash_completion.d/helm

# Agregar repositorios
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add elastic https://helm.elastic.co

# Listar repos
helm repo list
# NAME                    URL
# bitnami                 https://charts.bitnami.com/bitnami
# prometheus-community    https://prometheus-community.github.io/helm-charts

# Actualizar catálogo
helm repo update

# Buscar charts
helm search repo nginx
helm search repo postgres
helm search repo --versions prometheus
----
]

==== 11.1.2 Chart: Qué es

Un **Chart** es un paquete que contiene:

```
my-app-chart/
├── Chart.yaml              (metadatos del chart)
├── values.yaml             (valores por defecto)
├── values-production.yaml  (valores override)
├── charts/                 (sub-charts dependientes)
├── templates/              (YAML templates)
│   ├── deployment.yaml
│   ├── service.yaml
│   ├── configmap.yaml
│   ├── NOTES.txt           (instrucciones post-install)
│   └── _helpers.tpl        (funciones reutilizables)
├── README.md
└── LICENSE
```

==== 11.1.3 Release: Instalación de Chart

[source,bash]
----
# Instalar chart (crear release)
helm install my-nginx bitnami/nginx

# Ver releases
helm list
# NAME      NAMESPACE   STATUS    APP VERSION
# my-nginx  default     deployed  1.25.0

# Ver detalles de release
helm get values my-nginx
helm get manifest my-nginx
helm get notes my-nginx

# Instalar con valores personalizados
helm install my-app bitnami/nginx \
  --set replicaCount=3 \
  --set image.tag=1.24.0 \
  --namespace production \
  --create-namespace

# Instalar desde archivo de valores
helm install my-app my-app-chart \
  -f values-production.yaml \
  -f values-secrets.yaml

# Dry-run (simular sin aplicar)
helm install my-app bitnami/nginx --dry-run --debug

# Ver qué se aplicaría
helm get manifest my-nginx
----
]

==== 11.1.4 Actualizar y Rollback

[source,bash]
----
# Upgrade: actualizar release
helm upgrade my-nginx bitnami/nginx --set replicaCount=5

# Upgrade + install (si no existe)
helm upgrade --install my-nginx bitnami/nginx --set replicaCount=3

# Ver historial
helm history my-nginx
# REVISION   UPDATED                     STATUS       CHART          DESCRIPTION
# 1          2024-01-15 10:30:00 +0000   superseded   nginx-15.1.1   Install complete
# 2          2024-01-16 14:20:00 +0000   deployed     nginx-15.2.0   Upgrade complete

# Rollback a revisión anterior
helm rollback my-nginx 1

# Rollback a revisión anterior (penúltima)
helm rollback my-nginx

# Desinstalar release
helm uninstall my-nginx
# release "my-nginx" uninstalled

# Desinstalar pero mantener ConfigMaps (para datos)
helm uninstall my-app --keep-history
----
]

---

=== 11.2 Estructura de Chart

==== 11.2.1 Chart.yaml

Metadatos del chart:

[source,yaml]
----
apiVersion: v2
name: myapp
description: A Helm chart for deploying MyApp on Kubernetes
type: application

# Versión del chart (puede cambiar sin afectar app)
version: 1.2.3

# Versión de la aplicación
appVersion: "2.0.1"

# Información del mantenedor
maintainers:
- name: John Doe
  email: john@company.com
  url: https://github.com/johndoe

# Dependencias (otros charts)
dependencies:
- name: postgresql
  version: 12.x
  repository: "https://charts.bitnami.com/bitnami"
  condition: postgresql.enabled
  alias: postgres  # Nombre alternativo
- name: redis
  version: 17.x
  repository: "https://charts.bitnami.com/bitnami"

# Home page
home: https://github.com/my-company/myapp

# Repositorio del código
sources:
- https://github.com/my-company/myapp

# Palabras clave
keywords:
- app
- kubernetes
- helm

# Licencia
license: Apache-2.0

# Icono
icon: https://raw.githubusercontent.com/my-company/myapp/main/logo.png
----
]

==== 11.2.2 values.yaml

Valores por defecto:

[source,yaml]
----
# Global values (accesibles a todos los templates)
global:
  environment: production
  domain: example.com

# Aplicación
replicaCount: 3

image:
  repository: myregistry/myapp
  tag: "1.0.0"
  pullPolicy: IfNotPresent

# Imagen para init containers
initImage:
  repository: busybox
  tag: "1.35"

# Pull secrets para imágenes privadas
imagePullSecrets: []
# - name: myregistrykey

# Service Account
serviceAccount:
  create: true
  annotations: {}
  name: ""

# Pod annotations
podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8080"
  prometheus.io/path: "/metrics"

# Pod security context
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

# Container security context
securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: true
  runAsNonRoot: true

# Service
service:
  type: ClusterIP
  port: 8080
  targetPort: 8080
  annotations: {}

# Ingress
ingress:
  enabled: true
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
  hosts:
  - host: app.example.com
    paths:
    - path: /
      pathType: Prefix
  tls:
  - secretName: app-tls
    hosts:
    - app.example.com

# Resources
resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 100m
    memory: 128Mi

# Autoscaling
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# Node selector (afinidad)
nodeSelector: {}
  # kubernetes.io/os: linux

# Tolerations
tolerations: []

# Affinity
affinity: {}
  # nodeAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:

# Health checks
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 10

readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5

# Environment variables
env:
- name: ENV
  value: "production"
- name: LOG_LEVEL
  value: "info"

# ConfigMap data
configMap:
  enabled: true
  data:
    app.yaml: |
      environment: production
      debug: false

# Secrets
secrets:
  enabled: true
  # En producción, usar Sealed Secrets o SOPS
  database:
    username: app_user
    password: "change-me"

# Persistence
persistence:
  enabled: true
  storageClassName: "standard"
  accessMode: ReadWriteOnce
  size: 10Gi
  # mountPath: /data

# Database subchart
postgresql:
  enabled: true
  auth:
    username: app_user
    password: "secure-password"
    database: app_db

# Redis subchart
redis:
  enabled: false
  replica:
    replicaCount: 2
----
]

==== 11.2.3 templates/

Plantillas de Kubernetes:

[source,yaml]
----
# templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "myapp.fullname" . }}
  labels:
    {{- include "myapp.labels" . | nindent 4 }}
  annotations:
    {{- toYaml .Values.deployment.annotations | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "myapp.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "myapp.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "myapp.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
      - name: {{ .Chart.Name }}
        securityContext:
          {{- toYaml .Values.securityContext | nindent 12 }}
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        ports:
        - name: http
          containerPort: {{ .Values.service.targetPort }}
          protocol: TCP
        livenessProbe:
          {{- toYaml .Values.livenessProbe | nindent 12 }}
        readinessProbe:
          {{- toYaml .Values.readinessProbe | nindent 12 }}
        resources:
          {{- toYaml .Values.resources | nindent 12 }}
        env:
        {{- toYaml .Values.env | nindent 12 }}
        {{- if .Values.configMap.enabled }}
        volumeMounts:
        - name: config
          mountPath: /etc/config
          readOnly: true
        {{- end }}
        {{- if .Values.persistence.enabled }}
        - name: data
          mountPath: {{ .Values.persistence.mountPath | default "/data" }}
        {{- end }}
      {{- if .Values.configMap.enabled }}
      volumes:
      - name: config
        configMap:
          name: {{ include "myapp.fullname" . }}-config
      {{- end }}
      {{- if .Values.persistence.enabled }}
      - name: data
        persistentVolumeClaim:
          claimName: {{ include "myapp.fullname" . }}-pvc
      {{- end }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}

---
# templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "myapp.fullname" . }}
  labels:
    {{- include "myapp.labels" . | nindent 4 }}
  {{- with .Values.service.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
spec:
  type: {{ .Values.service.type }}
  ports:
  - port: {{ .Values.service.port }}
    targetPort: http
    protocol: TCP
    name: http
  selector:
    {{- include "myapp.selectorLabels" . | nindent 4 }}

---
# templates/configmap.yaml
{{- if .Values.configMap.enabled }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "myapp.fullname" . }}-config
  labels:
    {{- include "myapp.labels" . | nindent 4 }}
data:
  {{- range $key, $value := .Values.configMap.data }}
  {{ $key }}: |
    {{ $value | nindent 4 }}
  {{- end }}
{{- end }}

---
# templates/_helpers.tpl (funciones reutilizables)
{{/*
Expand the name of the chart.
*/}}
{{- define "myapp.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Create a default fully qualified app name.
*/}}
{{- define "myapp.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}
{{- end }}

{{/*
Create chart name and version as used by the chart label.
*/}}
{{- define "myapp.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Common labels
*/}}
{{- define "myapp.labels" -}}
helm.sh/chart: {{ include "myapp.chart" . }}
{{ include "myapp.selectorLabels" . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{/*
Selector labels
*/}}
{{- define "myapp.selectorLabels" -}}
app.kubernetes.io/name: {{ include "myapp.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

{{/*
Create the name of the service account to use
*/}}
{{- define "myapp.serviceAccountName" -}}
{{- if .Values.serviceAccount.create }}
{{- default (include "myapp.fullname" .) .Values.serviceAccount.name }}
{{- else }}
{{- default "default" .Values.serviceAccount.name }}
{{- end }}
{{- end }}
----
]

---

=== 11.3 Templating con Helm

==== 11.3.1 Funciones Básicas

[source,yaml]
----
# Variables y funciones básicas

# 1. Variables
{{ .Chart.Name }}           # Nombre del chart
{{ .Chart.Version }}        # Versión del chart
{{ .Chart.AppVersion }}     # Versión de la app
{{ .Release.Name }}         # Nombre del release
{{ .Release.Namespace }}    # Namespace
{{ .Values.replicaCount }}  # Valor de values.yaml

# 2. Default value (si no existe)
{{ .Values.replicaCount | default 3 }}

# 3. Condiciones
{{ if .Values.ingress.enabled }}
  # Crear Ingress
{{ else }}
  # No crear Ingress
{{ end }}

# 4. Con negación
{{ if not .Values.postgresql.enabled }}
  # PostgreSQL deshabilitado
{{ end }}

# 5. Loops
{{ range .Values.replicas }}
  - {{ . }}
{{ end }}

# 6. Dictionary (objeto)
{{ range $key, $value := .Values.env }}
- name: {{ $key }}
  value: {{ $value | quote }}
{{ end }}

# 7. Transformación de tipos
{{ .Values.image.tag | quote }}           # Entre comillas
{{ .Values.port | int }}                  # Convertir a entero
{{ .Values.replicaCount | default 3 }}    # Valor por defecto
{{ .Values.name | upper }}                # Mayúsculas
{{ .Values.name | lower }}                # Minúsculas
{{ .Values.email | contains "@" }}        # Contiene

# 8. Pipeline (encadenamiento)
{{ .Values.name | upper | quote }}        # NOMBRE entre comillas

# 9. Indentación
{{ .Values.config | toYaml | nindent 4 }} # Indentar YAML
{{ .Values.labels | toJson }}              # Convertir a JSON

# 10. Include (reutilizar template)
{{ include "myapp.labels" . }}
----
]

==== 11.3.2 Condicionales Avanzados

[source,yaml]
----
# templates/deployment.yaml - Sección condicional compleja

spec:
  {{- if not .Values.autoscaling.enabled }}
  # Si autoscaling está deshabilitado, fijar replicas
  replicas: {{ .Values.replicaCount }}
  {{- end }}

  {{- if .Values.persistence.enabled }}
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: {{ .Values.persistence.storageClassName }}
      resources:
        requests:
          storage: {{ .Values.persistence.size }}
  {{- end }}

  {{- with .Values.securityContext }}
  securityContext:
    {{- toYaml . | nindent 4 }}
  {{- end }}

  {{- if eq .Values.environment "production" }}
  # Solo en producción
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - myapp
        topologyKey: kubernetes.io/hostname
  {{- else if eq .Values.environment "staging" }}
  # En staging
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
  {{- end }}
----
]

==== 11.3.3 Bucles

[source,yaml]
----
# templates/deployment.yaml - Loops

containers:
- name: app
  env:
  # Loop sobre variables
  {{- range $key, $value := .Values.env }}
  - name: {{ $key | upper }}
    value: {{ $value | quote }}
  {{- end }}

  volumeMounts:
  # Loop sobre montajes
  {{- range .Values.volumeMounts }}
  - name: {{ .name }}
    mountPath: {{ .mountPath }}
    readOnly: {{ .readOnly | default false }}
  {{- end }}

volumes:
# Loop sobre volúmenes
{{- range .Values.volumes }}
- name: {{ .name }}
  {{- if .configMap }}
  configMap:
    name: {{ .configMap }}
  {{- end }}
  {{- if .secret }}
  secret:
    secretName: {{ .secret }}
  {{- end }}
{{- end }}

# Counter loop
{{- range $i, $e := until (int .Values.replicaCount) }}
- name: instance-{{ $i }}
  value: "{{ add $i 1 }}"
{{- end }}

# Condicional en loop
{{- range .Values.mounts }}
{{- if .enabled }}
- name: {{ .name }}
  mountPath: {{ .path }}
{{- end }}
{{- end }}
----
]

---

=== 11.4 Crear Chart Personalizado

==== 11.4.1 Crear Chart

[source,bash]
----
# Crear estructura base
helm create myapp

# Esto genera:
myapp/
├── Chart.yaml
├── values.yaml
├── charts/
├── templates/
│   ├── deployment.yaml
│   ├── service.yaml
│   ├── ingress.yaml
│   ├── hpa.yaml
│   ├── pdb.yaml
│   ├── serviceaccount.yaml
│   ├── configmap.yaml
│   ├── secret.yaml
│   ├── NOTES.txt
│   ├── _helpers.tpl
│   └── tests/
└── .helmignore

# Editar Chart.yaml
vi myapp/Chart.yaml

# Editar values.yaml
vi myapp/values.yaml

# Crear templates personalizados
vi myapp/templates/custom-resource.yaml

# Linting (validar sintaxis)
helm lint myapp

# Dry-run para verificar rendering
helm template myapp ./myapp

# Con valores custom
helm template myapp ./myapp -f values-production.yaml
----
]

==== 11.4.2 Dependencias

[source,bash]
----
# Chart.yaml con dependencias
cat > myapp/Chart.yaml << EOF
apiVersion: v2
name: myapp
version: 1.0.0
appVersion: "1.0"
dependencies:
- name: postgresql
  version: "12.x"
  repository: "https://charts.bitnami.com/bitnami"
  condition: postgresql.enabled
- name: redis
  version: "17.x"
  repository: "https://charts.bitnami.com/bitnami"
  condition: redis.enabled
EOF

# Descargar dependencias
helm dependency update myapp

# Esto descarga charts en myapp/charts/

# Ver dependencias
helm dependency list myapp

# Valores para dependencias en values.yaml
postgresql:
  enabled: true
  auth:
    username: app
    password: mypassword
    database: myapp

redis:
  enabled: false
----
]

==== 11.4.3 Testing de Charts

[source,yaml]
----
# templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "{{ include "myapp.fullname" . }}-test-connection"
  labels:
    {{- include "myapp.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": test
spec:
  containers:
  - name: wget
    image: busybox
    command: ['wget']
    args: ['{{ include "myapp.fullname" . }}:{{ .Values.service.port }}']
  restartPolicy: Never

---
# templates/tests/test-health.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "{{ include "myapp.fullname" . }}-test-health"
  labels:
    {{- include "myapp.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
spec:
  containers:
  - name: test
    image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
    command:
    - /app/health-check.sh
  restartPolicy: Never
----
]

[source,bash]
----
# Ejecutar tests
helm test myapp

# Ver resultados
kubectl get pods | grep test

# Ver logs del test
kubectl logs -l app=myapp-test
----
]

==== 11.4.4 Hooks (Pre-install, Post-install, etc.)

[source,yaml]
----
# templates/pre-install-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: "{{ .Release.Name }}-pre-install-job"
  labels:
    {{- include "myapp.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": pre-install
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
spec:
  template:
    spec:
      serviceAccountName: {{ include "myapp.serviceAccountName" . }}
      containers:
      - name: setup
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        command:
        - /app/setup-database.sh
      restartPolicy: Never

---
# templates/post-install-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: "{{ .Release.Name }}-post-install-job"
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-weight": "0"
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
spec:
  template:
    spec:
      containers:
      - name: verify
        image: busybox
        command:
        - /bin/sh
        - -c
        - echo "Installation completed successfully"
      restartPolicy: Never

---
# templates/pre-upgrade-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: "{{ .Release.Name }}-pre-upgrade-job"
  annotations:
    "helm.sh/hook": pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
spec:
  template:
    spec:
      containers:
      - name: backup
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        command:
        - /app/backup-database.sh
      restartPolicy: Never
----
]

Tipos de hooks:
- `pre-install`: Antes de instalar
- `post-install`: Después de instalar
- `pre-upgrade`: Antes de actualizar
- `post-upgrade`: Después de actualizar
- `pre-delete`: Antes de desinstalar
- `post-delete`: Después de desinstalar
- `pre-rollback`: Antes de hacer rollback
- `post-rollback`: Después de hacer rollback
- `test`: Pruebas

---

=== 11.5 Gestión de Releases

==== 11.5.1 Instalación Avanzada

[source,bash]
----
# Instalar con valores múltiples
helm install myapp ./myapp-chart \
  -f values.yaml \
  -f values-production.yaml \
  --set replicaCount=5 \
  --set image.tag=v2.0.0 \
  --set postgresql.enabled=true \
  --namespace production \
  --create-namespace

# Instalar con archivo de valores JSON
helm install myapp ./myapp-chart \
  --values values.json

# Instalar con valores literales (strings)
helm install myapp ./myapp-chart \
  --set-string image.tag=v1.2.3 \
  --set-string env.DATABASE_URL="postgresql://..."

# Validar chart antes de instalar
helm lint myapp-chart
helm template myapp myapp-chart --debug

# Instalar con espera a que esté listo
helm install myapp myapp-chart \
  --wait \
  --timeout 5m \
  --atomic  # Rollback si falla
----
]

==== 11.5.2 Upgrades

[source,bash]
----
# Upgrade simple
helm upgrade myapp ./myapp-chart

# Upgrade con valores nuevos
helm upgrade myapp ./myapp-chart \
  --set replicaCount=10 \
  --set image.tag=v2.0.0

# Upgrade con merge de valores anteriores
helm upgrade myapp ./myapp-chart \
  --reuse-values \
  --set replicaCount=5

# Upgrade + install (si no existe)
helm upgrade --install myapp ./myapp-chart

# Upgrade con estrategia
helm upgrade myapp ./myapp-chart \
  --set strategy.type=RollingUpdate \
  --set strategy.rollingUpdate.maxSurge=1 \
  --set strategy.rollingUpdate.maxUnavailable=0

# Upgrade atomático (rollback si falla)
helm upgrade myapp ./myapp-chart --atomic --timeout 5m

# Upgrade con fuerza
helm upgrade myapp ./myapp-chart --force

# Ver cambios antes de aplicar
helm diff upgrade myapp ./myapp-chart
----
]

==== 11.5.3 Rollback

[source,bash]
----
# Ver historial de upgrades
helm history myapp
# REVISION   UPDATED                     STATUS       CHART           DESCRIPTION
# 1          Mon Jan 15 10:30:00 2024    superseded   myapp-1.0.0     Install complete
# 2          Tue Jan 16 14:20:00 2024    superseded   myapp-1.1.0     Upgrade complete
# 3          Wed Jan 17 09:15:00 2024    deployed     myapp-1.2.0     Upgrade complete

# Rollback a revisión anterior
helm rollback myapp 2

# Rollback a penúltima revisión
helm rollback myapp

# Rollback y crear nueva revisión
helm rollback myapp 2 --cleanup-on-fail

# Ver status después de rollback
helm status myapp
----
]

==== 11.5.4 Valores Override

[source,bash]
----
# Precedencia (de menor a mayor)
# 1. Default en values.yaml
# 2. -f values-custom.yaml (archivo de valores)
# 3. --set key=value (línea de comandos)
# 4. --set-string key="value" (string literal)

# Archivo de valores
cat > values-prod.yaml << EOF
replicaCount: 5
image:
  tag: v2.0.0
postgresql:
  enabled: true
  auth:
    password: prod-password
EOF

# Instalar con múltiples override
helm install myapp ./myapp-chart \
  -f values-prod.yaml \
  --set nodeSelector."disktype"="ssd" \
  --set persistence.size=50Gi \
  --set-json resources='{"limits":{"cpu":"1","memory":"1Gi"}}'

# Ver valores finales
helm get values myapp

# Ver valores con defaults
helm get values myapp --all
----
]

---

=== 11.6 Repositorios Helm

==== 11.6.1 Crear y Publicar Chart

[source,bash]
----
# Packagizar chart
helm package myapp-chart

# Esto genera: myapp-chart-1.2.3.tgz

# Crear índice de repositorio
helm repo index . --url https://charts.my-company.com

# Esto genera: index.yaml con hash y URL de charts

# Subir a servidor (S3, GitHub Pages, etc)
aws s3 cp myapp-chart-1.2.3.tgz s3://my-helm-charts/
aws s3 cp index.yaml s3://my-helm-charts/

# O con GitHub Pages
git add myapp-chart-1.2.3.tgz index.yaml
git commit -m "Release myapp chart v1.2.3"
git push origin main

# Agregar repo personalizad
helm repo add myrepo https://s3.amazonaws.com/my-helm-charts
helm repo add myrepo https://raw.githubusercontent.com/my-company/helm-charts/main

# Actualizar repo
helm repo update myrepo

# Usar chart del repo
helm install myapp myrepo/myapp-chart

# Ver disponibles
helm search repo myrepo
----
]

==== 11.6.2 Repositorio Privado

[source,bash]
----
# Crear secret para repo privado
kubectl create secret docker-registry my-repo-secret \
  --docker-username=user \
  --docker-password=pass \
  --docker-server=private-repo.company.com

# Usar en Helm (con pull secret en imagePullSecrets)
helm install myapp myrepo/myapp \
  --set imagePullSecrets[0].name=my-repo-secret

# O con credenciales en Helm
helm repo add myrepo https://user:pass@private-repo.company.com/helm
helm repo update myrepo
helm search repo myrepo
----
]

---

=== 11.7 Ejemplo Completo: WordPress con Helm

[source,yaml]
----
# values-wordpress.yaml
replicaCount: 2

image:
  repository: wordpress
  tag: "6.0"
  pullPolicy: IfNotPresent

service:
  type: LoadBalancer
  port: 80

ingress:
  enabled: true
  className: nginx
  hosts:
  - host: wordpress.example.com
    paths:
    - path: /
      pathType: Prefix
  tls:
  - secretName: wordpress-tls
    hosts:
    - wordpress.example.com

persistence:
  enabled: true
  size: 20Gi
  storageClassName: fast-ssd

resources:
  requests:
    cpu: 250m
    memory: 256Mi
  limits:
    cpu: 500m
    memory: 512Mi

# MySQL con Bitnami chart
mysql:
  enabled: true
  auth:
    rootPassword: root-password
    database: wordpress
    username: wordpress
    password: wp-password
  primary:
    persistence:
      size: 20Gi

# Variables de WordPress
env:
- name: WORDPRESS_DB_HOST
  value: "myapp-mysql:3306"
- name: WORDPRESS_DB_USER
  valueFrom:
    secretKeyRef:
      name: mysql-credentials
      key: username
- name: WORDPRESS_DB_PASSWORD
  valueFrom:
    secretKeyRef:
      name: mysql-credentials
      key: password
- name: WORDPRESS_DB_NAME
  value: wordpress

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 5
  targetCPUUtilizationPercentage: 70
----

[source,bash]
----
# Instalar
helm install wordpress ./wordpress-chart \
  -f values-wordpress.yaml \
  --namespace wordpress \
  --create-namespace

# Verificar instalación
helm list -n wordpress
kubectl get pods -n wordpress
kubectl get svc -n wordpress

# Ver notas
helm get notes wordpress -n wordpress

# Upgradee a nueva versión
helm upgrade wordpress ./wordpress-chart \
  --set image.tag=6.1 \
  -n wordpress

# Rollback si hay problemas
helm rollback wordpress -n wordpress
----
]

---

=== 11.8 Best Practices

**1. Versionado**
```yaml
✅ Usar semantic versioning
✅ Documentar cambios en CHANGELOG
❌ Reutilizar versiones
```

**2. Templates**
```yaml
✅ Usar helpers (_helpers.tpl)
✅ Documentar valores complejos
✅ Validar con helm lint
❌ Hardcodear valores
```

**3. Valores**
```yaml
✅ Valores por defecto sensatos
✅ Valores override en archivos
✅ Secrets separados
❌ Secrets en values.yaml
```

**4. Testing**
```yaml
✅ Usar helm test
✅ Pre-install/post-install hooks
✅ Dry-run antes de aplicar
❌ Aplicar sin validar
```

**5. Documentación**
```yaml
✅ README.md con ejemplos
✅ NOTES.txt con instrucciones
✅ Comentarios en templates
❌ Charts sin documentación
```

---

=== 11.9 Resumen del Módulo 11

En este módulo aprendiste:

1. **Helm Basics**:
   - Instalación y repositorios
   - Charts y releases
   - Upgrade, rollback, uninstall

2. **Estructura de Chart**:
   - Chart.yaml (metadatos)
   - values.yaml (valores por defecto)
   - templates/ (YAML templates)
   - Subcharts y dependencias

3. **Templating**:
   - Variables, funciones, condicionales
   - Bucles y pipeline
   - Indentación y transformación

4. **Crear Charts**:
   - helm create
   - Validación con helm lint
   - Testing de charts
   - Hooks (pre-install, post-install, etc.)

5. **Gestión de Releases**:
   - Instalación avanzada
   - Upgrades con estrategias
   - Rollbacks
   - Override de valores

6. **Repositorios**:
   - Crear y publicar charts
   - Repositorios privados
   - Búsqueda y actualización

7. **Ejemplo Práctico**: WordPress con MySQL

Con estos conocimientos, estás listo para aprender sobre **Service Mesh** en el Módulo 12.

---

== MÓDULO 12: Service Mesh con Istio

**Service Mesh** es una capa de infraestructura dedicada a gestionar la comunicación entre servicios (service-to-service communication) en aplicaciones distribuidas.

```
ANTES (sin Service Mesh):
┌─────────┐         ┌─────────┐
│ Service │────────▶│ Service │
│    A    │         │    B    │
└─────────┘         └─────────┘
(Lógica: reintentos, timeouts, circuit breaker, tracing, etc.)

DESPUÉS (con Service Mesh):
┌─────────┐ ┌─────────┐ ┌─────────┐
│ Service │─│ Envoy   │─│ Service │
│    A    │ │ Sidecar │ │    B    │
└─────────┘ └─────────┘ └─────────┘
             ↓ (Infraestructura maneja: reintentos, timeouts,
               circuit breaker, tracing, encryption, etc.)
```

---

=== 12.1 Conceptos de Service Mesh

==== 12.1.1 ¿Qué es un Service Mesh?

Un **Service Mesh** es una capa de infraestructura que maneja la comunicación entre servicios:

[cols="1,1"]
|===
| Sin Service Mesh | Con Service Mesh
| Lógica en cada microservicio | Lógica centralizada en malla
| Código repetido en N servicios | Una sola configuración
| Difícil de actualizar | Fácil de evolucionar
| Difícil de observar | Observabilidad completa
|===

**Problema que resuelve:**
- **Reintentos**: ¿Si falla, cuántas veces reintentar?
- **Timeouts**: ¿Cuánto esperar antes de fallar?
- **Circuit Breaker**: ¿Cómo evitar cascadas de fallos?
- **Load Balancing**: ¿Cómo distribuir tráfico?
- **Encryption**: ¿Cómo cifrar comunicación?
- **Rate Limiting**: ¿Cómo controlar tráfico?
- **Tracing**: ¿Cómo debuggear peticiones?

==== 12.1.2 Sidecar Proxies

Cada pod obtiene un proxy adicional (sidecar):

```yaml
Pod:
┌─────────────────────────┐
│ Container: myapp:8080   │
├─────────────────────────┤
│ Container: envoy-proxy  │
│ (intercepta tráfico)    │
└─────────────────────────┘
     ↓ (intercepta)
Todas las peticiones HTTP/TCP
pasan por Envoy
```

[source,bash]
----
# Ver sidecars inyectados
kubectl describe pod myapp-pod-xyz
kubectl get pods -o jsonpath='{.items[*].spec.containers[*].name}'

# Ver recursos del sidecar
kubectl top pods -c istio-proxy

# Ver logs del sidecar
kubectl logs myapp-pod-xyz -c istio-proxy
----
]

==== 12.1.3 Control Plane vs Data Plane

[cols="1,1"]
|===
| Control Plane (Istiod) | Data Plane (Envoy Proxies)
| Gestiona configuración | Ejecuta tráfico
| Valida policies | Aplica policies
| Distribución de certificados | Maneja encryption
| Ofrece APIs | Intercepta peticiones
| 1 por cluster | 1 por pod
|===

```
┌──────────────────────────────────┐
│   Control Plane (Istiod)         │
│  ┌─────────────────────────────┐ │
│  │ API: VirtualService         │ │
│  │ API: DestinationRule        │ │
│  │ API: Gateway                │ │
│  │ API: PeerAuthentication     │ │
│  │ Certificate Manager         │ │
│  └─────────────────────────────┘ │
│            ↓ (configura)          │
└──────────────────────────────────┘
         ↓
┌──────────────────────────────────┐
│   Data Plane (Envoy Proxies)     │
│  ┌────────────┐ ┌────────────┐   │
│  │ Pod 1      │ │ Pod 2      │   │
│  │  +Envoy    │ │  +Envoy    │   │
│  └────────────┘ └────────────┘   │
│  ┌────────────┐ ┌────────────┐   │
│  │ Pod 3      │ │ Pod 4      │   │
│  │  +Envoy    │ │  +Envoy    │   │
│  └────────────┘ └────────────┘   │
└──────────────────────────────────┘
```

==== 12.1.4 Casos de Uso

1. **Gestión de Tráfico**:
   - Canary deployments
   - A/B testing
   - Blue-green deployments
   - Load balancing inteligente

2. **Seguridad**:
   - mTLS automático
   - Autorización entre servicios
   - Encriptación end-to-end

3. **Observabilidad**:
   - Tracing distribuido
   - Métricas de tráfico
   - Visualización en Kiali

4. **Resiliencia**:
   - Circuit breaking
   - Retry policies
   - Timeouts
   - Bulkhead pattern

---

=== 12.2 Istio: Instalación y Arquitectura

==== 12.2.1 Instalación de Istio

[source,bash]
----
# Descargar Istio
curl -L https://istio.io/downloadIstio | sh -
cd istio-1.19.0

# Agregar a PATH
export PATH=$PWD/bin:$PATH

# Verificar instalación
istioctl version

# Verificar requisitos del cluster
istioctl analyze

# Instalar Istio en default mode
istioctl install --set profile=demo -y

# Opciones de profile:
# - default: Producción
# - demo: Desarrollo/laboratorio
# - minimal: Solo istiod
# - remote: Para clusters remotos

# Verificar instalación
kubectl get pods -n istio-system
# istio-ingressgateway-xxxx
# istio-egressgateway-xxxx
# istiod-xxxx

# Inyectar sidecars automáticamente en namespace
kubectl label namespace default istio-injection=enabled

# Verificar label
kubectl get namespace default --show-labels

# Inyectar en pods existentes (redeploy)
kubectl rollout restart deployment/myapp -n default

# Verificar que tengan sidecar
kubectl get pods -o jsonpath='{.items[*].spec.containers[*].name}'
----
]

==== 12.2.2 Arquitectura de Istio

[source,yaml]
----
# Componentes de Istio

# 1. Istiod (Control Plane)
# - Proporciona APIs: VirtualService, DestinationRule, etc.
# - Gestiona certificados mTLS
# - Configura proxies

# 2. Envoy (Data Plane)
# - Proxy en cada pod
# - Intercepta tráfico
# - Aplica políticas

# 3. Ingress Gateway
# - Punto de entrada al cluster
# - Reemplaza Ingress de K8s
# - Routing avanzado

# 4. Egress Gateway
# - Punto de salida del cluster
# - Tráfico a servicios externos
# - Filtrado y control

# Estructura de Istio CRDs:
# - VirtualService: Routing (A dónde va tráfico)
# - DestinationRule: Cómo llega (circuit break, lb strategy)
# - Gateway: Punto de entrada/salida
# - ServiceEntry: Servicios externos
# - PeerAuthentication: mTLS
# - AuthorizationPolicy: Quién puede acceder
# - RequestAuthentication: JWT validation
# - Telemetry: Métricas y tracing
----
]

[source,bash]
----
# Ver recursos de Istio
kubectl get virtualservices
kubectl get destinationrules
kubectl get gateways
kubectl get peerauthentications
kubectl get authorizationpolicies

# Ver con namespace
kubectl get vs -n production
kubectl get dr -n production

# Describe de recurso
kubectl describe vs myapp-vs

# Editar
kubectl edit vs myapp-vs

# Borrar
kubectl delete vs myapp-vs
----
]

==== 12.2.3 Envoy Proxies

Cada sidecar Envoy:

[source,yaml]
----
# Ver configuración de Envoy
istioctl analyze

# Debug de proxy
istioctl proxy-config cluster my-pod-xyz -n default

# Ver rutas
istioctl proxy-config routes my-pod-xyz -n default

# Ver listeners (puertos que escucha)
istioctl proxy-config listeners my-pod-xyz -n default

# Ver secretos (certificados mTLS)
istioctl proxy-config secret my-pod-xyz -n default

# Logs del proxy
kubectl logs my-pod-xyz -c istio-proxy
kubectl logs my-pod-xyz -c istio-proxy -f  # realtime

# Stats del proxy
istioctl dashboard envoy my-pod-xyz
# Abre http://localhost:15000 con stats detalladas
----
]

==== 12.2.4 Validación de Configuración

[source,bash]
----
# Validar configuración global
istioctl analyze

# Validar en namespace específico
istioctl analyze -n production

# Validar archivo YAML antes de aplicar
istioctl analyze --ignore-unknown-crds my-config.yaml

# Ver configuración que se genera
istioctl analyze -o json

# Validar y reparar automáticamente
istioctl analyze --recursive=/path/to/files

# Debug de un pod
istioctl describe pod my-pod-xyz -n default

# Ver si está inyectado correctamente
kubectl get pods -o jsonpath='{.items[0].spec.containers[*].name}'
----
]

---

=== 12.3 Traffic Management

==== 12.3.1 VirtualService: Routing

**VirtualService** define A DÓNDE va el tráfico.

[source,yaml]
----
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp-vs
spec:
  # Host: servicio K8s que queremos configurar
  hosts:
  - myapp

  # HTTP: reglas de routing
  http:
  # Regla 1: Rutas que matchean /admin van a version v2
  - match:
    - uri:
        prefix: "/admin"
    route:
    - destination:
        host: myapp
        subset: v2
      weight: 100

  # Regla 2: Headers específicos van a version v3
  - match:
    - headers:
        user-type:
          exact: "premium"
    route:
    - destination:
        host: myapp
        subset: v3
      weight: 100

  # Regla 3: Default (todas las demás)
  - route:
    - destination:
        host: myapp
        subset: v1
      weight: 60
    - destination:
        host: myapp
        subset: v2
      weight: 40
    timeout: 10s
    retries:
      attempts: 3
      perTryTimeout: 2s
----
]

**Conceptos:**

- `hosts`: Servicio K8s que configuras
- `match`: Condiciones (path, header, method, etc.)
- `route`: Destinos (subsets del servicio)
- `weight`: Porcentaje de tráfico (canary)
- `timeout`: Máximo tiempo de espera
- `retries`: Reintentos automáticos

==== 12.3.2 DestinationRule: Cómo conectar

**DestinationRule** define CÓMO conectar al servicio.

[source,yaml]
----
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: myapp-dr
spec:
  host: myapp

  # TLS: mTLS para este servicio
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL

    # Connection pool: límites de conexiones
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 100
        http2MaxRequests: 100
        maxRequestsPerConnection: 2

    # Outlier detection: circuit breaker
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
      splitExternalLocalOriginErrors: true

    # Load balancing
    loadBalancer:
      simple: LEAST_REQUEST
      # Opciones: ROUND_ROBIN, LEAST_REQUEST, RANDOM, PASSTHROUGH

  # Subsets: versiones del servicio
  subsets:
  - name: v1
    labels:
      version: v1
    trafficPolicy:
      connectionPool:
        http:
          http1MaxPendingRequests: 50

  - name: v2
    labels:
      version: v2
    trafficPolicy:
      connectionPool:
        http:
          http1MaxPendingRequests: 100

  - name: v3
    labels:
      version: v3
    trafficPolicy:
      tls:
        mode: DISABLE  # Disable mTLS para v3
----
]

**Conceptos:**

- `host`: Servicio destino
- `trafficPolicy`: Políticas globales
- `connectionPool`: Límites de conexiones
- `outlierDetection`: Circuit breaker
- `loadBalancer`: Estrategia de balanceo
- `subsets`: Versiones del servicio

==== 12.3.3 Gateway: Punto de Entrada

[source,yaml]
----
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: myapp-gateway
spec:
  selector:
    istio: ingressgateway  # Usar ingress gateway

  servers:
  # HTTPS
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: myapp-tls
    hosts:
    - "myapp.example.com"
    - "app.example.com"

  # HTTP
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "myapp.example.com"
    - "app.example.com"

---
# VirtualService vinculado a Gateway
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp-routes
spec:
  hosts:
  - "myapp.example.com"
  gateways:
  - myapp-gateway  # Usar el gateway

  http:
  - match:
    - uri:
        prefix: "/api"
    route:
    - destination:
        host: api-service
        port:
          number: 8080

  - route:
    - destination:
        host: web-service
        port:
          number: 80
----
]

[source,bash]
----
# Ver gateways
kubectl get gateways

# Ver qué están escuchando
kubectl get svc -n istio-system istio-ingressgateway

# Obtener IP/hostname del gateway
kubectl get svc istio-ingressgateway -n istio-system -o jsonpath='{.status.loadBalancer.ingress[0].ip}'

# Testar acceso
curl -H "Host: myapp.example.com" http://<gateway-ip>/api/v1/users
----
]

==== 12.3.4 Canary Deployments

Desplegar gradualmente nueva versión:

[source,yaml]
----
# Deployments: v1 y v2 corriendo simultáneamente
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-v1
spec:
  replicas: 9  # 90%
  selector:
    matchLabels:
      app: myapp
      version: v1
  template:
    metadata:
      labels:
        app: myapp
        version: v1
    spec:
      containers:
      - name: app
        image: myapp:1.0.0

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-v2
spec:
  replicas: 1  # 10% (canary)
  selector:
    matchLabels:
      app: myapp
      version: v2
  template:
    metadata:
      labels:
        app: myapp
        version: v2
    spec:
      containers:
      - name: app
        image: myapp:2.0.0

---
# VirtualService: envía 90% a v1, 10% a v2
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp-canary
spec:
  hosts:
  - myapp
  http:
  - route:
    - destination:
        host: myapp
        subset: v1
      weight: 90
    - destination:
        host: myapp
        subset: v2
      weight: 10  # Canary: solo 10% a v2

---
# DestinationRule: define subsets
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: myapp-canary-dr
spec:
  host: myapp
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
----

Proceso de canary:
```
Día 1: v1 90% + v2 10%
Día 2: v1 75% + v2 25%
Día 3: v1 50% + v2 50%
Día 4: v1 25% + v2 75%
Día 5: v1 0% + v2 100%
Si hay errores → Rollback inmediato
```
]

==== 12.3.5 A/B Testing

Enviar usuarios específicos a versión A o B:

[source,yaml]
----
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp-ab-test
spec:
  hosts:
  - myapp
  http:
  # Usuarios logueados → Nueva UI (v2)
  - match:
    - sourceLabels:
        user-type: "premium"
    route:
    - destination:
        host: myapp
        subset: v2

  # Usuarios anónimos → UI actual (v1)
  - match:
    - sourceLabels:
        user-type: "free"
    route:
    - destination:
        host: myapp
        subset: v1

  # Por header específico
  - match:
    - headers:
        x-test-version:
          exact: "new-ui"
    route:
    - destination:
        host: myapp
        subset: v2

  # Default
  - route:
    - destination:
        host: myapp
        subset: v1
      weight: 50
    - destination:
        host: myapp
        subset: v2
      weight: 50
----
]

---

=== 12.4 Seguridad en Istio

==== 12.4.1 mTLS: Mutual TLS

Encriptación automática entre servicios:

[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
spec:
  # Modo STRICT: mTLS es obligatorio
  mtls:
    mode: STRICT

---
# Alternative: PERMISSIVE (permite también HTTP)
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: allow-http
spec:
  mtls:
    mode: PERMISSIVE

---
# Seleccionar namespaces específicos
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: production-strict
spec:
  selector:
    matchLabels:
      security: high
  mtls:
    mode: STRICT

---
# Port-level mTLS (diferente por puerto)
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: mixed-mtls
spec:
  selector:
    matchLabels:
      app: myapp
  mtls:
    mode: PERMISSIVE
  portLevelMtls:
    8080:
      mode: DISABLE    # Puerto 8080: sin mTLS
    8443:
      mode: STRICT     # Puerto 8443: mTLS obligatorio
----
]

[source,bash]
----
# Verificar certificados mTLS
istioctl proxy-config secret my-pod-xyz -n default

# Ver si mTLS está activo
kubectl logs pod-name -c istio-proxy | grep -i "tls"

# Listar PeerAuthentication
kubectl get peerauthentication

# Describir configuración
kubectl describe pa default
----
]

==== 12.4.2 AuthorizationPolicy: Control de Acceso

Definir quién puede acceder a qué:

[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: allow-frontend
spec:
  selector:
    matchLabels:
      app: backend
  rules:
  # Permitir peticiones desde frontend
  - from:
    - source:
        principals: ["cluster.local/ns/default/sa/frontend"]
    to:
    - operation:
        methods: ["GET"]
        paths: ["/api/v1/users"]

---
# Deny all (default deny)
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: deny-all
spec:
  selector:
    matchLabels:
      app: database
  rules: []  # Vacío = deny all

---
# Allow all
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: allow-all
spec:
  selector:
    matchLabels:
      app: public-api
  rules:
  - {}  # Vacío = allow all

---
# Autorización basada en JWT
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: jwt-authz
spec:
  selector:
    matchLabels:
      app: api
  rules:
  - from:
    - source:
        requestPrincipals: ["issuer@company.com/*"]
    to:
    - operation:
        methods: ["POST"]
        paths: ["/api/v1/admin/*"]

---
# Autorización por namespace
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: cross-namespace-allow
spec:
  selector:
    matchLabels:
      app: service-a
  rules:
  - from:
    - source:
        namespaces: ["production", "staging"]
    to:
    - operation:
        methods: ["GET"]
----
]

==== 12.4.3 RequestAuthentication: JWT Validation

Validar JWT tokens:

[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: jwt-auth
spec:
  selector:
    matchLabels:
      app: api
  jwtRules:
  - issuer: "https://auth.company.com"
    jwksUri: "https://auth.company.com/.well-known/jwks.json"
    audiences: "my-app"
    forwardOriginalToken: true

---
# Con múltiples issuers
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: multi-auth
spec:
  jwtRules:
  - issuer: "https://auth1.company.com"
    jwksUri: "https://auth1.company.com/.well-known/jwks.json"
  - issuer: "https://auth2.company.com"
    jwksUri: "https://auth2.company.com/.well-known/jwks.json"
----

[source,bash]
----
# Testar JWT
TOKEN="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
curl -H "Authorization: Bearer $TOKEN" http://api.example.com/endpoint
----
]

---

=== 12.5 Observabilidad

==== 12.5.1 Distributed Tracing

Rastrear peticiones entre servicios:

[source,yaml]
----
# Configuración de tracing en Istio
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: enable-tracing
spec:
  tracing:
  - providers:
    - name: "jaeger"
    randomSamplingPercentage: 100  # 100% de peticiones

---
# Configuración de Jaeger (storage backend)
apiVersion: v1
kind: ConfigMap
metadata:
  name: jaeger-config
  namespace: istio-system
data:
  sampling.json: |
    {
      "default_strategy": {
        "type": "probabilistic",
        "param": 1
      }
    }
----

[source,bash]
----
# Acceder a Jaeger UI
istioctl dashboard jaeger

# Ver trazas
# http://localhost:16686

# Traces muestran:
# - Latencia de cada span
# - Errores en servicios
# - Dependencias entre servicios

# Ejemplo de trace:
# GET /api/users (100ms)
# ├─ frontend → backend (50ms)
# │  └─ query users table (40ms)
# ├─ backend → database (30ms)
# │  └─ SQL query (25ms)
# └─ database → cache (5ms)
----
]

==== 12.5.2 Metrics: Prometheus

Istio exporta métricas a Prometheus:

[source,yaml]
----
# Métricas disponibles automáticamente:

# request_total: Número de peticiones
# - metric: istio_requests_total
# - labels: source_app, destination_app, response_code

# request_duration: Latencia
# - metric: istio_request_duration_milliseconds
# - labels: source_app, destination_app, le (bucket)

# request_bytes: Bytes enviados
# - metric: istio_request_bytes

# response_bytes: Bytes recibidos
# - metric: istio_response_bytes

# tcp_connections_opened: Conexiones TCP
# - metric: istio_tcp_connections_opened_total

# tcp_connections_closed: Conexiones TCP cerradas
# - metric: istio_tcp_connections_closed_total
----

[source,bash]
----
# Dashboard de Prometheus
istioctl dashboard prometheus

# Ejemplos de queries PromQL:

# Tasa de error
sum(rate(istio_requests_total{response_code=~"5.."}[5m])) by (destination_app)

# P95 latencia
histogram_quantile(0.95, 
  sum(rate(istio_request_duration_milliseconds_bucket[5m])) by (destination_app, le)
)

# Tráfico por servicio
sum(rate(istio_requests_total[5m])) by (destination_app)

# Versiones en canary deployment
sum(rate(istio_requests_total[5m])) by (destination_app, destination_version)
----
]

==== 12.5.3 Kiali: Visualización

Panel de visualización de la malla:

[source,bash]
----
# Abrir Kiali
istioctl dashboard kiali

# Usuario: admin
# Password: admin

# Características:
# - Graph: Visualiza servicios y tráfico
# - Workloads: Estado de pods
# - Applications: Agrupación lógica
# - Services: Servicios K8s
# - Distributed Tracing: Integración con Jaeger
# - Metrics: Gráficas de Prometheus

# Cambiar namespace
En Kiali UI: dropdown arriba a la izquierda

# Ver tráfico en tiempo real
Graph → Hover sobre conexión

# Ver errores
Services → Click en servicio → Logs
----
]

[source,yaml]
----
# Configurar Kiali dashboard
apiVersion: v1
kind: ConfigMap
metadata:
  name: kiali
  namespace: istio-system
data:
  kiali.yaml: |
    auth:
      strategy: anonymous
    service:
      type: LoadBalancer
    external_services:
      jaeger:
        url: http://jaeger:16686
      prometheus:
        url: http://prometheus:9090
----
]

---

=== 12.6 Ejemplo Completo: Aplicación con Istio

Desplegar aplicación con gestión de tráfico segura:

[source,yaml]
----
# 1. Deployment v1
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: v1
  template:
    metadata:
      labels:
        app: myapp
        version: v1
    spec:
      containers:
      - name: app
        image: myapp:1.0.0
        ports:
        - containerPort: 8080

---
# 2. Deployment v2 (canary)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-v2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
      version: v2
  template:
    metadata:
      labels:
        app: myapp
        version: v2
    spec:
      containers:
      - name: app
        image: myapp:2.0.0
        ports:
        - containerPort: 8080

---
# 3. Service K8s
apiVersion: v1
kind: Service
metadata:
  name: myapp
spec:
  selector:
    app: myapp
  ports:
  - port: 8080
    targetPort: 8080
    name: http

---
# 4. DestinationRule (circuit breaker + subsets)
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: myapp
spec:
  host: myapp
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
    connectionPool:
      http:
        http1MaxPendingRequests: 100
        maxRequestsPerConnection: 2
    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s

  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2

---
# 5. VirtualService (90/10 canary)
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp
spec:
  hosts:
  - myapp
  http:
  - route:
    - destination:
        host: myapp
        subset: v1
      weight: 90
    - destination:
        host: myapp
        subset: v2
      weight: 10
    timeout: 5s
    retries:
      attempts: 3
      perTryTimeout: 1s

---
# 6. Gateway (ingreso)
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: myapp-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "myapp.example.com"

---
# 7. VirtualService para Gateway
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: myapp-gateway-routes
spec:
  hosts:
  - "myapp.example.com"
  gateways:
  - myapp-gateway
  http:
  - route:
    - destination:
        host: myapp
        port:
          number: 8080

---
# 8. PeerAuthentication (mTLS)
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: myapp-mtls
spec:
  selector:
    matchLabels:
      app: myapp
  mtls:
    mode: STRICT

---
# 9. AuthorizationPolicy
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: myapp-authz
spec:
  selector:
    matchLabels:
      app: myapp
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/default/sa/frontend"]
    to:
    - operation:
        methods: ["GET", "POST"]
        paths: ["/api/*"]
----

[source,bash]
----
# Desplegar todo
kubectl apply -f myapp-istio.yaml

# Verificar recursos
kubectl get deployments,svc,vs,dr,gateway,pa,authz

# Acceder a aplicación
kubectl port-forward svc/istio-ingressgateway 8080:80 -n istio-system
curl http://localhost:8080 -H "Host: myapp.example.com"

# Ver Kiali
istioctl dashboard kiali

# Generar tráfico para ver métricas
while true; do
  curl http://localhost:8080 -H "Host: myapp.example.com"
  sleep 1
done

# Gradualmente aumentar tráfico a v2
kubectl patch vs myapp --type merge -p '
{
  "spec": {
    "http": [{
      "route": [
        {"destination": {"host": "myapp", "subset": "v1"}, "weight": 50},
        {"destination": {"host": "myapp", "subset": "v2"}, "weight": 50}
      ]
    }]
  }
}'

# Si hay errores, rollback
kubectl patch vs myapp --type merge -p '
{
  "spec": {
    "http": [{
      "route": [
        {"destination": {"host": "myapp", "subset": "v1"}, "weight": 100},
        {"destination": {"host": "myapp", "subset": "v2"}, "weight": 0}
      ]
    }]
  }
}'
----
]

---

=== 12.7 Best Practices con Istio

**1. Seguridad**
```yaml
✅ mTLS siempre habilitado
✅ Deny-all + allow específicos
✅ JWT validation
✅ Certificados actualizados
❌ HTTP sin encripción
❌ Allow-all policies
```

**2. Performance**
```yaml
✅ Circuit breaker configurado
✅ Timeouts apropiados
✅ Connection pools limitados
✅ Outlier detection
❌ Limites sin controlar
❌ Infinitas reintentos
```

**3. Observabilidad**
```yaml
✅ Tracing habilitado
✅ Métricas en Prometheus
✅ Alertas en Kiali
✅ Logs centralizados
❌ Sin visibilidad
❌ Sin tracing
```

**4. Gradualidad**
```yaml
✅ Canary deployments
✅ Blue-green sin downtime
✅ A/B testing en producción
❌ Cambios abruptos
❌ Todos afectados simultáneamente
```

---

=== 12.8 Resumen del Módulo 12

En este módulo aprendiste:

1. **Conceptos de Service Mesh**:
   - Qué es y por qué usarlo
   - Sidecar proxies (Envoy)
   - Control Plane vs Data Plane

2. **Istio: Instalación y Arquitectura**:
   - Instalación con istioctl
   - Inyección automática de sidecars
   - Componentes (Istiod, Envoy, Gateways)

3. **Traffic Management**:
   - VirtualService: dónde va el tráfico
   - DestinationRule: cómo conectar
   - Gateway: punto de entrada
   - Canary y A/B testing

4. **Seguridad**:
   - mTLS automático entre servicios
   - AuthorizationPolicy: control de acceso
   - RequestAuthentication: JWT validation

5. **Observabilidad**:
   - Distributed tracing con Jaeger
   - Métricas en Prometheus
   - Visualización en Kiali

6. **Ejemplo Completo**: Aplicación con canary deployment

7. **Best Practices**: Seguridad, performance, observabilidad, gradualidad

Con estos conocimientos, estás listo para aprender sobre **Desarrollo Avanzado** en el Módulo 13.


Con estos conocimientos, estás listo para aprender sobre **Desarrollo Avanzado** en el Módulo 13.


== Apéndices

---

=== A. Comandos de kubectl: Referencia Completa

==== A.1 Comandos Básicos

[source,bash]
----
# Información del cluster
kubectl version
kubectl cluster-info
kubectl api-versions
kubectl api-resources

# Namespaces
kubectl get namespaces
kubectl create namespace production
kubectl delete namespace staging
kubectl config set-context --current --namespace=production

# Contextos
kubectl config view
kubectl config get-contexts
kubectl config use-context docker-desktop
kubectl config set-context my-context --cluster=minikube --namespace=default

# Crear recursos
kubectl create deployment nginx --image=nginx
kubectl create service clusterip myapp --tcp=80:8080
kubectl create configmap app-config --from-file=config.yaml
kubectl create secret generic db-secret --from-literal=password=secret123

# Aplicar manifests
kubectl apply -f deployment.yaml
kubectl apply -f *.yaml
kubectl apply -f directory/
kubectl apply -k kustomization/

# Get: listar recursos
kubectl get pods
kubectl get pods -n production
kubectl get pods -o wide
kubectl get pods -o yaml
kubectl get pods -o json
kubectl get pods --all-namespaces
kubectl get pods -A

# Get con labels
kubectl get pods -l app=nginx
kubectl get pods -l "app=nginx,tier=frontend"
kubectl get pods --show-labels

# Get con selección de columnas
kubectl get pods -o custom-columns=NAME:.metadata.name,STATUS:.status.phase
kubectl get pods -o jsonpath='{.items[*].metadata.name}'

# Describe: detalles de un recurso
kubectl describe pod my-pod
kubectl describe node worker-1
kubectl describe svc myapp

# Delete: borrar recursos
kubectl delete pod my-pod
kubectl delete deployment nginx
kubectl delete -f deployment.yaml
kubectl delete pods --all
kubectl delete pods -l app=nginx

# Edit: editar recursos en vivo
kubectl edit deployment nginx
kubectl edit svc myapp

# Patch: cambios menores
kubectl patch deployment nginx -p '{"spec":{"replicas":5}}'
kubectl set image deployment/nginx nginx=nginx:1.20

# Exposer: crear servicios
kubectl expose pod my-pod --port=8080 --type=ClusterIP
kubectl expose deployment nginx --port=80 --type=LoadBalancer

# Port-forward: acceso local
kubectl port-forward pod/my-pod 8080:8080
kubectl port-forward svc/myapp 8080:80
kubectl port-forward deployment/nginx 3000:80 --address 0.0.0.0
----
]

==== A.2 Debugging

[source,bash]
----
# Logs
kubectl logs pod-name
kubectl logs pod-name -c container-name
kubectl logs pod-name --follow
kubectl logs pod-name --tail=50
kubectl logs pod-name --previous  # Si el pod se reincició
kubectl logs deployment/nginx --all-containers=true

# Exec: entrar en pod
kubectl exec -it pod-name -- /bin/bash
kubectl exec -it pod-name -c container-name -- /bin/sh
kubectl exec pod-name -- ls -la
kubectl exec pod-name -- env

# Top: recursos
kubectl top nodes
kubectl top pods
kubectl top pods -n production

# Describe: detalles completos
kubectl describe pod pod-name
kubectl describe node worker-1
# Muestra: status, eventos, recursos, etc.

# Events: eventos del cluster
kubectl get events
kubectl get events -n production
kubectl get events --sort-by='.lastTimestamp'

# Status del pod
kubectl get pod pod-name -o yaml | grep -A 20 status

# Debug pod con alpine
kubectl run debug --image=alpine --restart=Never -- sleep 1000
kubectl exec -it debug -- /bin/sh

# Verificar DNS
kubectl run --rm -it debug --image=alpine --restart=Never -- nslookup kubernetes.default
kubectl run --rm -it debug --image=alpine --restart=Never -- nslookup myapp.default.svc.cluster.local

# Conectar a servicio
kubectl run --rm -it debug --image=alpine --restart=Never -- wget -qO- http://myapp:8080/health

# Ver configuración de pod
kubectl get pod pod-name -o yaml
kubectl get pod pod-name -o json

# Ver diferencias entre manifest y cluster
kubectl diff -f deployment.yaml

# Eventos de deployment
kubectl describe deployment myapp
kubectl rollout status deployment/myapp
kubectl rollout history deployment/myapp
----
]

==== A.3 Flags Comunes

[source,bash]
----
# Namespace
-n, --namespace STRING
kubectl get pods -n production

# Output
-o, --output FORMAT (json, yaml, wide, custom-columns, jsonpath, etc.)
kubectl get pods -o json
kubectl get pods -o jsonpath='{.items[*].metadata.name}'

# Selector
-l, --selector SELECTOR
kubectl get pods -l app=nginx,tier=frontend

# All namespaces
-A, --all-namespaces
kubectl get pods -A

# Watch: monitorear cambios
-w, --watch
kubectl get pods -w

# Recurse
-R, --recursive
kubectl apply -f . -R

# Dry-run: simular sin aplicar
--dry-run=client
kubectl apply -f deployment.yaml --dry-run=client

# Sort-by: ordenar
--sort-by=FIELD
kubectl get pods --sort-by=.metadata.creationTimestamp

# Show-labels: mostrar labels
--show-labels
kubectl get pods --show-labels

# Field-selector: filtrar por campos
--field-selector STATUS.phase=Running
kubectl get pods --field-selector=status.phase=Running

# Context
--context CONTEXT
kubectl get pods --context=production-cluster

# Validate: validar YAML
--validate=true
kubectl apply -f deployment.yaml --validate=true

# Record: guardar cambios
--record
kubectl set image deployment/nginx nginx=nginx:1.20 --record
----
]

==== A.4 Aliases Útiles

[source,bash]
----
# Agregar al ~/.bashrc o ~/.zshrc
alias k='kubectl'
alias kgp='kubectl get pods'
alias kgd='kubectl get deployment'
alias kgs='kubectl get svc'
alias kgc='kubectl get configmap'
alias kgn='kubectl get nodes'
alias kdp='kubectl describe pod'
alias kdd='kubectl describe deployment'
alias kds='kubectl describe svc'
alias kdel='kubectl delete'
alias kl='kubectl logs'
alias klf='kubectl logs -f'
alias kex='kubectl exec -it'
alias kpf='kubectl port-forward'
alias kctx='kubectl config use-context'
alias kge='kubectl get events --sort-by='\''.lastTimestamp'\'

# Funciones útiles
function kns() {
  kubectl config set-context --current --namespace=$1
}

function kdesc() {
  kubectl describe $1 $2 -n ${3:-default}
}

# Uso:
kns production     # Cambiar namespace
kdesc pod myapp    # Describir pod
kgp -n production  # Get pods con namespace
----
]

---

=== B. Recursos YAML: Referencia Completa

==== B.1 Estructura General

[source,yaml]
----
# Estructura base de cualquier recurso Kubernetes
apiVersion: v1                    # Versión de API
kind: Pod                         # Tipo de recurso
metadata:                         # Metadatos
  name: my-pod                    # Nombre único
  namespace: default              # Namespace (default si omitido)
  labels:                         # Etiquetas (key-value)
    app: myapp
    tier: frontend
  annotations:                    # Anotaciones (descripción, info)
    description: "My application"
    managed-by: "helm"
  ownerReferences:                # Propietario (si es owned)
  - apiVersion: apps/v1
    kind: Deployment
    name: myapp
spec:                             # Especificación
  containers:                     # Contenedores
  - name: app
    image: myapp:1.0.0
    ports:
    - containerPort: 8080
  restartPolicy: Always           # Always, Never, OnFailure
status:                           # Estado (solo lectura)
  phase: Running
  conditions: []
----
]

==== B.2 API Versions Comunes

[source,yaml]
----
# v1 - Recursos básicos
apiVersion: v1
kind: Pod, Service, ConfigMap, Secret, PersistentVolume, etc.

# apps/v1 - Controladores
apiVersion: apps/v1
kind: Deployment, StatefulSet, DaemonSet, ReplicaSet, etc.

# batch/v1 - Trabajos
apiVersion: batch/v1
kind: Job, CronJob

# networking.k8s.io/v1 - Networking
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy, Ingress

# storage.k8s.io/v1 - Storage
apiVersion: storage.k8s.io/v1
kind: StorageClass, VolumeAttachment

# rbac.authorization.k8s.io/v1 - RBAC
apiVersion: rbac.authorization.k8s.io/v1
kind: Role, RoleBinding, ClusterRole, ClusterRoleBinding

# policy/v1 - Policies
apiVersion: policy/v1
kind: PodDisruptionBudget

# autoscaling/v2 - Autoscaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler

# cert-manager.io/v1 - Certificados
apiVersion: cert-manager.io/v1
kind: Certificate, Issuer, ClusterIssuer

# networking.istio.io/v1beta1 - Istio
apiVersion: networking.istio.io/v1beta1
kind: VirtualService, DestinationRule, Gateway

# Ver todas las versiones disponibles
kubectl api-versions

# Ver recursos de una versión
kubectl api-resources --api-group=apps
----
]

==== B.3 Tipos de Recursos (Kinds)

[source,yaml]
----
# Workload Resources (correr aplicaciones)
Pod, Deployment, StatefulSet, DaemonSet, Job, CronJob

# Service Resources (networking)
Service, Ingress, NetworkPolicy, ServiceEntry

# Storage Resources
PersistentVolume, PersistentVolumeClaim, StorageClass, VolumeSnapshot

# Configuration Resources
ConfigMap, Secret

# RBAC Resources
ServiceAccount, Role, RoleBinding, ClusterRole, ClusterRoleBinding

# Policies
PodSecurityPolicy, NetworkPolicy, ResourceQuota, LimitRange

# Cluster Resources
Node, Namespace, Cluster

# Custom Resources (CRD)
CustomResourceDefinition, [cualquier recurso custom]

# Metadata Resources
HorizontalPodAutoscaler, VerticalPodAutoscaler, PodDisruptionBudget

# Categorías
kubectl get pods            # Workloads
kubectl get svc             # Services
kubectl get pvc             # Storage
kubectl get configmap       # Configuration
kubectl get sa              # RBAC
----
]

==== B.4 Campos Comunes

[source,yaml]
----
# Presente en casi todos los recursos

metadata:
  name: my-resource           # Nombre (único en namespace)
  namespace: default          # Namespace
  uid: "550e8400-e29b-41d4"  # ID único (generado)
  resourceVersion: "123456"   # Versión (para optimistic locking)
  generation: 1               # Generación
  creationTimestamp: "2024-01-15T10:30:00Z"
  deletionTimestamp: null     # Timestamp de eliminación
  labels: {}                  # Etiquetas para seleccionar
  annotations: {}             # Anotaciones (info adicional)
  ownerReferences: []         # Propietarios
  finalizers: []              # Limpieza antes de borrar
  managedFields: []           # Información de cambios

spec:                         # Especificación deseada
  # Varía según el kind

status:                       # Estado actual (solo lectura)
  # Varía según el kind
  phase: Running              # Estado general
  conditions: []              # Condiciones detalladas
  observedGeneration: 1       # Última generación observada

# Etiquetas comunes
labels:
  app: myapp                  # Nombre de la app
  version: v1                 # Versión
  tier: frontend              # Tier (frontend, backend, database)
  environment: production     # Entorno (production, staging, dev)
  component: api              # Componente
  team: platform              # Equipo responsable

# Anotaciones comunes
annotations:
  description: "My application"
  managed-by: "helm"
  prometheus.io/scrape: "true"
  prometheus.io/port: "8080"
  external-dns.alpha.kubernetes.io/hostname: "app.example.com"
----
]

==== B.5 Selectores y Queries

[source,yaml]
----
# Label Selector: seleccionar recursos por labels
selector:
  matchLabels:
    app: nginx              # Exacto
    tier: frontend
  matchExpressions:
- key: environment
  operator: In              # In, NotIn, Exists, DoesNotExist
  values:
  - production
  - staging
- key: tier
  operator: NotIn
  values:
  - database

# Ejemplo: Deployment que selecciona Pods
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx          # DEBE coincidir con selector
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx

# Field Selector: seleccionar por campos
kubectl get pods --field-selector=status.phase=Running
kubectl get pods --field-selector=spec.nodeName=worker-1

# JSONPath: queries complejas
kubectl get pods -o jsonpath='{.items[*].metadata.name}'
kubectl get pods -o jsonpath='{.items[?(@.status.phase=="Running")].metadata.name}'
kubectl get pods -o jsonpath='{.items[*].spec.containers[*].image}'
----
]

---

=== C. Glosario: Términos Clave de Kubernetes

[cols="1,2"]
|===
| Término | Definición

| **API Server**
| Servidor central que gestiona toda la comunicación en el cluster. Expone la API de Kubernetes.

| **kubelet**
| Agente en cada nodo que asegura que los pods corren como se especificó.

| **kube-proxy**
| Proxy de red en cada nodo que gestiona las reglas de networking.

| **etcd**
| Base de datos distribuida que almacena todo el estado del cluster.

| **Scheduler**
| Componente que asigna pods a nodos basado en recursos y políticas.

| **Controller Manager**
| Ejecuta controllers que regulan el estado del cluster.

| **Pod**
| Unidad más pequeña desplegable. Puede contener 1+ contenedores.

| **Deployment**
| Controller que gestiona pods (replicas, updates, rollbacks).

| **StatefulSet**
| Controller para apps que requieren identidad y storage persistente.

| **DaemonSet**
| Asegura que todos los nodos ejecuten un pod específico.

| **Service**
| Abstracción que expone pods como servicio (IP, DNS, puerto).

| **Ingress**
| Router HTTP/HTTPS que expone servicios al exterior.

| **ConfigMap**
| Objeto que almacena configuración en pares clave-valor.

| **Secret**
| Objeto que almacena datos sensibles (passwords, tokens, etc.).

| **PersistentVolume**
| Storage independiente del ciclo de vida del pod.

| **PersistentVolumeClaim**
| Solicitud de storage por parte de un pod.

| **StorageClass**
| Define tipos de storage disponibles (fast SSD, standard, etc).

| **Namespace**
| Separación lógica de recursos dentro del cluster.

| **Label**
| Par clave-valor para identificar y organizar recursos.

| **Annotation**
| Información arbitraria attached a recursos (no se usa para seleccionar).

| **Selector**
| Mecanismo para seleccionar recursos por labels.

| **RBAC**
| Role-Based Access Control. Control de permisos basado en roles.

| **Role**
| Conjunto de permisos en un namespace específico.

| **RoleBinding**
| Vincula un Role a usuarios/serviceaccounts en un namespace.

| **ClusterRole**
| Conjunto de permisos a nivel de cluster.

| **ClusterRoleBinding**
| Vincula un ClusterRole a usuarios/serviceaccounts globalmente.

| **ServiceAccount**
| Identidad para que los pods accedan la API de K8s.

| **Network Policy**
| Define reglas de tráfico de entrada/salida para pods.

| **Container Runtime**
| Software que ejecuta contenedores (Docker, containerd, CRI-O).

| **Node**
| Máquina (física o virtual) que ejecuta pods.

| **Cluster**
| Conjunto de nodos gestionados por Kubernetes.

| **Control Plane**
| Componentes que gestionan el cluster (API Server, Scheduler, etc.).

| **Worker Node**
| Nodo que ejecuta cargas de trabajo (pods).

| **Taints & Tolerations**
| Mecanismo para repeler pods de ciertos nodos.

| **Affinity**
| Preferencias de scheduling de pods a nodos específicos.

| **Quality of Service**
| Garantías de recursos: Guaranteed, Burstable, BestEffort.

| **Resource Request**
| Cantidad mínima de recursos que un contenedor necesita.

| **Resource Limit**
| Cantidad máxima de recursos que un contenedor puede usar.

| **Probe**
| Verificación de salud: Liveness, Readiness, Startup.

| **Finalizer**
| Hook que se ejecuta antes de que un objeto sea borrado.

| **Operator**
| Controller que maneja aplicaciones complejas automáticamente.

| **Custom Resource Definition**
| Define nuevos tipos de recursos personalizados.

| **Webhook**
| Callback que se ejecuta durante operaciones de API.

| **Admission Controller**
| Plugin que intercepta y valida solicitudes de API.

| **CRI**
| Container Runtime Interface. Interface para runtimes de contenedores.

| **CNI**
| Container Network Interface. Interface para plugins de red.

|===

---

=== D. Referencias y Recursos

==== D.1 Documentación Oficial

[cols="1,2"]
|===
| Recurso | URL

| Documentación Oficial
| https://kubernetes.io/docs/

| Referencia de API
| https://kubernetes.io/docs/reference/

| Tutoriales Interactivos
| https://kubernetes.io/docs/tutorials/

| Conceptos
| https://kubernetes.io/docs/concepts/

| Tareas Comunes
| https://kubernetes.io/docs/tasks/

| Troubleshooting
| https://kubernetes.io/docs/tasks/debug-application-cluster/

| kubectl Cheatsheet
| https://kubernetes.io/docs/reference/kubectl/cheatsheet/

| API Reference
| https://kubernetes.io/docs/reference/generated/kubernetes-api/

|===

==== D.2 Libros Recomendados

[cols="1,2,1"]
|===
| Libro | Descripción | Nivel

| Kubernetes in Action
| Guía práctica con ejemplos reales. Excelente para principiantes.
| Principiante

| The Kubernetes Book
| Cobertura completa de Kubernetes. Muy recomendado.
| Intermedio

| Kubernetes Patterns
| Patrones y mejores prácticas para diseñar aplicaciones.
| Avanzado

| Cloud Native DevOps with Kubernetes
| DevOps con Kubernetes, GitOps, CI/CD.
| Intermedio-Avanzado

| Programming Kubernetes
| Extend Kubernetes con código. Operators y controllers.
| Avanzado

| Production Kubernetes
| Manejo de Kubernetes en producción a escala.
| Avanzado

|===

==== D.3 Cursos Online

[cols="1,2"]
|===
| Plataforma | Curso

| Kubernetes.io
| Kubernetes by Example (gratis)

| Linux Academy / A Cloud Guru
| Certified Kubernetes Administrator (CKA)

| Udemy
| Kubernetes Complete Guide to DevOps

| Pluralsight
| Kubernetes Path

| KodeKloud
| Kubernetes for Developers

| O'Reilly
| Varios cursos en demanda

|===

==== D.4 Comunidades

[cols="1,2"]
|===
| Comunidad | Descripción

| Kubernetes Slack
| Comunidad oficial con canales por tema

| Stack Overflow
| Preguntas y respuestas (tag: kubernetes)

| Kubernetes Forums
| Foros oficiales de discusión

| Reddit: r/kubernetes
| Comunidad de Reddit dedicada a Kubernetes

| GitHub Discussions
| Discusiones en repositorio oficial

| CNCF Community
| Cloud Native Computing Foundation (mantiene Kubernetes)

|===

---

=== E. Laboratorios Prácticos

==== E.1 Setup del Entorno Local

[source,bash]
----
# Opción 1: Docker Desktop (recomendado)
# Descargar desde: https://www.docker.com/products/docker-desktop
# Habilitar Kubernetes en Settings → Kubernetes → Enable

# Opción 2: Minikube
curl -LO https://github.com/kubernetes/minikube/releases/download/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube
minikube start
minikube status
minikube stop

# Opción 3: Kind (Kubernetes in Docker)
go install sigs.k8s.io/kind@latest
kind create cluster --name my-cluster
kind delete cluster --name my-cluster

# Opción 4: k3s (Kubernetes ligero)
curl -sfL https://get.k3s.io | sh -
sudo systemctl start k3s
sudo systemctl stop k3s

# Instalar kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

# Instalar otros tools
# Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# kubectx (cambiar contexto rápidamente)
git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx

# Verificar instalación
kubectl version
kubectl cluster-info
kubectl get nodes
----
]

==== E.2 Laboratorio 1: Deploy Simple

[source,yaml]
----
# Ejercicio: Desplegar nginx, exponerlo y acceder

# 1. Crear deployment
cat > nginx-deploy.yaml << EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
EOF

kubectl apply -f nginx-deploy.yaml

# 2. Crear servicio
kubectl expose deployment nginx --type=LoadBalancer --port=80

# 3. Verificar
kubectl get deployments
kubectl get pods
kubectl get svc

# 4. Acceder (esperar IP o usar port-forward)
kubectl port-forward svc/nginx 8080:80

# 5. En otra terminal
curl http://localhost:8080

# Solución esperada: página de bienvenida de nginx
----
]

==== E.3 Laboratorio 2: ConfigMap y Secrets

[source,yaml]
----
# Ejercicio: Crear app con configuración y secrets

# 1. Crear ConfigMap
kubectl create configmap app-config \
  --from-literal=log_level=info \
  --from-literal=env=production

# 2. Crear Secret
kubectl create secret generic db-secret \
  --from-literal=username=admin \
  --from-literal=password=secret123

# 3. Crear pod que use ambos
cat > app-with-config.yaml << EOF
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: myapp:1.0
    env:
    - name: LOG_LEVEL
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: log_level
    - name: DB_USERNAME
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: username
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: password
EOF

kubectl apply -f app-with-config.yaml

# 4. Verificar que el pod tiene las variables
kubectl exec -it app -- env | grep -E "LOG_LEVEL|DB_"

# Solución: Ver variables en el pod
----
]

==== E.4 Laboratorio 3: Scaling y Autoescaling

[source,yaml]
----
# Ejercicio: Escalar pods manualmente y luego automáticamente

# 1. Ver replicas
kubectl get deployment nginx

# 2. Escalar manualmente
kubectl scale deployment nginx --replicas=5

# 3. Ver que aumentó
kubectl get pods

# 4. Crear HPA
cat > hpa.yaml << EOF
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
EOF

kubectl apply -f hpa.yaml

# 5. Generar carga para ver scaling
kubectl run load-gen --image=busybox --restart=Never -- /bin/sh -c "while true; do wget -q -O- http://nginx; done"

# 6. Ver HPA en acción
kubectl get hpa -w
kubectl get pods -w

# Solución: Ver pods escalando arriba/abajo según carga
----
]

==== E.5 Laboratorio 4: Networking

[source,yaml]
----
# Ejercicio: Crear servicios y testar conectividad

# 1. Crear dos deployments
kubectl create deployment web --image=nginx
kubectl create deployment api --image=node:alpine

# 2. Exponer con ClusterIP
kubectl expose deployment web --port=80 --type=ClusterIP
kubectl expose deployment api --port=3000 --type=ClusterIP

# 3. Desde un pod, conectar a otro servicio
kubectl run client --image=alpine --restart=Never -it -- sh

# Dentro del pod:
# nslookup web  # Ver IP
# wget -qO- http://web  # Acceder

# 4. Crear Ingress
cat > ingress.yaml << EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: web.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web
            port:
              number: 80
EOF

kubectl apply -f ingress.yaml

# 5. Acceder via ingress (requiere configured ingress controller)
# En /etc/hosts:
# 127.0.0.1 web.local

# Solución: Navegar a http://web.local
----
]

==== E.6 Laboratorio 5: Persistencia

[source,yaml]
----
# Ejercicio: Crear PVC y usarlo en pod

# 1. Ver StorageClasses disponibles
kubectl get storageclasses

# 2. Crear PersistentVolumeClaim
cat > pvc.yaml << EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: standard
  resources:
    requests:
      storage: 5Gi
EOF

kubectl apply -f pvc.yaml

# 3. Crear pod que use PVC
cat > pod-with-pvc.yaml << EOF
apiVersion: v1
kind: Pod
metadata:
  name: data-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: data-pvc
EOF

kubectl apply -f pod-with-pvc.yaml

# 4. Escribir datos
kubectl exec -it data-pod -- sh -c "echo 'hello' > /data/test.txt"

# 5. Eliminar pod y recrear
kubectl delete pod data-pod
kubectl apply -f pod-with-pvc.yaml

# 6. Verificar que datos persisten
kubectl exec -it data-pod -- cat /data/test.txt

# Solución: Ver "hello" en la salida
----
]

---

=== F. Examen de Certificación

==== F.1 CKA: Certified Kubernetes Administrator

**Contenido del examen:**
- Cluster Architecture, Installation & Configuration (25%)
- Workloads & Scheduling (15%)
- Services & Networking (20%)
- Storage (10%)
- Troubleshooting (30%)

[cols="1,2,1"]
|===
| Tema | Descripción | Importancia

| Cluster Setup
| kubeadm, versiones, upgrades
| Alta

| RBAC
| Roles, RoleBindings, ServiceAccounts
| Alta

| Networking
| Services, Ingress, Network Policies
| Alta

| Storage
| PVC, PV, StorageClass
| Media

| Troubleshooting
| Debugging pods, nodes, networking
| Alta

| Security
| Pod Security, Secrets management
| Media

|===

**Recursos de estudio:**
- Documentación oficial: https://kubernetes.io/docs/
- Hands-on labs: https://kodekloud.com/courses/kubernetes-challenge/
- Simulacros: https://killer.sh/

**Tips para el examen:**
```
✅ Practica mucho: 70% del examen es práctico
✅ Domina kubectl: Es la herramienta principal
✅ Conoce los flags comunes: --dry-run, -o yaml, etc.
✅ Crea aliases útiles: alias k=kubectl
✅ Practica bajo presión: El examen tiene tiempo limitado
✅ Lee cuidadosamente: Las preguntas pueden ser tricky
❌ No intentes memorizar: Enfócate en entender conceptos
```

==== F.2 CKAD: Certified Kubernetes Application Developer

**Contenido del examen:**
- Application Design and Build (20%)
- Application Deployment (20%)
- Application Observability and Maintenance (15%)
- Application Environment, Config and Security (25%)
- Services and Networking (20%)

[cols="1,2,1"]
|===
| Tema | Descripción | Importancia

| Deployments
| Crear, actualizar, debuggear
| Alta

| Networking
| Services, Ingress, DNS
| Alta

| Configuration
| ConfigMaps, Secrets, env vars
| Alta

| Storage
| Volumes, PVC
| Media

| Observability
| Logs, metrics, debugging
| Media

| Security
| RBAC, Pod Security, Secrets
| Media

| Resource Limits
| Requests, limits, QoS
| Media

|===

**Recursos de estudio:**
- Documentación oficial
- KodeKloud CKAD course: https://kodekloud.com/courses/kubernetes-for-developers-ckad/
- Simulacros: https://killer.sh/

**Tips para el examen:**
```
✅ Conoce imperative commands: kubectl run, create, expose
✅ Familiarízate con Deployments: Updates, rollbacks
✅ Entiende ConfigMaps y Secrets: Cómo usarlos
✅ Debugging: logs, exec, describe
✅ Networking: Services, DNS, Ingress
✅ Autoscaling: HPA, manual scaling
```

==== F.3 CKS: Certified Kubernetes Security Specialist

**Contenido del examen:**
- Cluster Setup (10%)
- Cluster Hardening (15%)
- System Hardening (15%)
- Minimize Microservice Vulnerabilities (20%)
- Supply Chain Security (20%)
- Monitoring, Logging and Runtime Security (20%)

[cols="1,2,1"]
|===
| Tema | Descripción | Importancia

| Pod Security
| Pod Security Policies, Security Contexts
| Alta

| RBAC
| Roles, bindings, service accounts
| Alta

| Network Policies
| Control de tráfico entre pods
| Alta

| Secrets Management
| Encryption at rest, in transit
| Alta

| Image Security
| Image scanning, registry security
| Media

| Runtime Security
| Container runtime, monitoring
| Media

| Compliance
| Best practices, hardening
| Media

|===

**Recursos de estudio:**
- Documentación oficial + security guide
- KodeKloud CKS course: https://kodekloud.com/courses/certified-kubernetes-security-specialist-cks/
- Simulacros: https://killer.sh/

**Tips para el examen:**
```
✅ Entiende RBAC profundamente: RBAC está en muchas preguntas
✅ Network Policies: Cómo bloquear/permitir tráfico
✅ Pod Security: Security Contexts, policies
✅ Encryption: etcd encryption, TLS
✅ Secrets management: No guardes en plain text
✅ Image security: Sighing, scanning, registry
```

==== F.4 Estrategia General de Preparación

[source,bash]
----
# Fase 1: Aprender (2-3 semanas)
# - Completar todos los módulos del curso
# - Hacer ejercicios prácticos
# - Leer documentación oficial

# Fase 2: Practicar (2-3 semanas)
# - Laboratorios prácticos intensivos
# - Simulacros de examen
# - Aumentar velocidad y precisión

# Fase 3: Intensivo (1 semana)
# - Simulacros bajo tiempo
# - Practica en entorno similiar al examen
# - Revisa temas débiles

# Fase 4: Descanso (3 días)
# - Descansa antes del examen
# - No intentes aprender cosas nuevas
# - Duerme bien

# Día del examen:
# - Llega 15 minutos antes
# - Verifica conexión de internet
# - Lee todas las preguntas primero
# - Maneja bien el tiempo
# - Evita errores de typo (copia-pega)

# Comando útil para practicar rápido:
alias kdry='kubectl --dry-run=client -o yaml'
alias kex='kubectl exec -it'
alias kl='kubectl logs'

# Practica imperativa:
kubectl run pod-name --image=image-name --dry-run=client -o yaml
kubectl create deployment name --image=image --dry-run=client -o yaml
kubectl expose pod pod-name --port=8080 --dry-run=client -o yaml
----
]

---

== FIN DEL CURSO: KUBERNETES COMPLETO

**Felicitaciones por completar este curso comprehensivo de Kubernetes.**

Durante 12 módulos + Apéndices, cubriste:

✅ **Módulo 1:** Introducción a Kubernetes
✅ **Módulo 2:** Pods y Contenedores
✅ **Módulo 3:** Controladores (Deployment, StatefulSet, DaemonSet, Jobs)
✅ **Módulo 4:** Servicios y Networking (Service, Ingress, DNS)
✅ **Módulo 5:** Configuración y Secretos (ConfigMap, Secret)
✅ **Módulo 6:** Storage (PV, PVC, StorageClass, StatefulSet)
✅ **Módulo 7:** Scaling y Autoscaling (HPA, VPA, manual scaling)
✅ **Módulo 8:** Seguridad (RBAC, Pod Security, Network Policies, mTLS)
✅ **Módulo 9:** Monitoreo y Logging (Prometheus, Grafana, ELK, Jaeger)
✅ **Módulo 10:** GitOps y CI/CD (Flux, ArgoCD, GitHub Actions, Tekton)
✅ **Módulo 11:** Helm Package Manager (Charts, Templating, Releases)
✅ **Módulo 12:** Service Mesh con Istio (mTLS, Routing, Observability)

**Apéndices:**
✅ **A:** Comandos kubectl Referencia Completa
✅ **B:** Recursos YAML y Estructura
✅ **C:** Glosario de Términos
✅ **D:** Referencias y Recursos
✅ **E:** Laboratorios Prácticos
✅ **F:** Examen de Certificación (CKA, CKAD, CKS)

---

**Próximos Pasos:**

1. **Practica constantemente**: La mejor forma de aprender K8s es usándolo
2. **Construye proyectos**: Deploya aplicaciones reales en K8s
3. **Explora ecosistema**: Operator, CRDs, Plugins, Custom controllers
4. **Certifícate**: CKA, CKAD o CKS según tu especialidad
5. **Contribuye**: Kubernetes es open-source, contribuye a la comunidad

---

*Curso desarrollado como material de referencia comprehensive para Kubernetes. Todos los ejemplos son funcionales y listos para producción.*

*Última actualización: 2024*
*Versión del curso: 2.0*
